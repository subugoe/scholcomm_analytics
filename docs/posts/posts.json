[
  {
    "path": "posts/openbib_ta_release/",
    "title": "Introducing Open Metadata about Transformative Agreements",
    "description": "This post presents a new dataset that combines open metadata from the cOAlition S Journal Checker Tool and OpenAlex to analyse transformative agreements. Data on these much-discussed agreements are scattered across different sources and are only partially available. To address this, we preserved and combined open metadata from the cOAlition S Journal Checker Tool and OpenAlex, resulting in a unified dataset for large-scale bibliometric studies.",
    "author": [
      {
        "name": "Najko Jahn",
        "url": {}
      }
    ],
    "date": "2025-05-09",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nSince their initial proposal (Schimmer, Geschuhn, and Vogler 2015), transformative agreements have become a predominant model to finance open access in scholarly journals (Dér 2025).\nMeasuring their impact, however, remains challenging as data about these agreements are scattered across different sources (Kramer 2024).\nThe cOAlition S Public Transformative Agreement Data dump, which powers the Journal Checker Tool, is an important step towards transparency.\nThis resource is based on transformative agreements recorded in the ESAC Registry.\nHowever, the Journal Checker Tool only presents current agreements and removes data on expired agreements.\nAnother shortcoming in the analysis of transformative agreements is that bibliometric databases have not integrated data on transformative agreements such as those provided by ESAC or the COAlition S, making it difficult to identify articles published under these agreements (Bakker, Langham-Putrow, and Riegelman 2024).\nOpen access monitoring services also lack comprehensive coverage of this data point.\nTo close this gap, this blog post introduces an open data release about transformative agreements developed as part of the initial OPENBIB data release of the German Competence Network for Bibliometrics(Haupka et al. 2025) at the SUB Göttingen.\nThis dataset, licensed under CC0, combines cOAlition S data with OpenAlex to improve transparency and enable estimates of articles published under these agreements.\nThe dataset comprises:\n1,043 transformative agreements covering 17,199 eligible journals and 5,355 participating institutions\n1,362,221 eligible articles published by first authors affiliated with participating institutions\nPreliminary versions of this dataset were used in the SUB Göttingen’s Hybrid Open Access Dashboard, a comprehensive monitoring effort based on 13,000 hybrid journals in transformative agreements (Achterberg and Jahn 2023), and studies on the impact of transformation agreements on open access in hybrid journals (Jahn 2025b). The data were also used to compare findings when applied to open metadata and proprietary bibliometric databases Scopus and Web of Science (Jahn 2025a).\nUsing Dutch Research Council NWO funded papers, de Jonge, Kramer, and Sondervan (2025) validated an open method based on transformative agreement data and OpenAlex and were able to accuratly identify the majority of articles under these agreements.\nThis blog post will present methods used to compile the dataset and will present a use case based on Google BigQuery to help with the first steps using this new open data source.\nMethods\nData retrieval and curation\nA dedicated bot has preserved weekly snapshots of the cOAlition S Public Transformative Agreement Data dump since December 2022.\nThese snapshots, available on GitHub, were merged using a custom script that retains only the most recent data for each agreement.\nThe original data links agreements to journals through names and ISSNs.\nAfter mapping to linking ISSN (ISSN-L), journals were associated with publishers using the ESAC Registry.\nTo improve institutional coverage, the data were enriched with ROR-IDs from OpenAlex’s institution data.\nBecause OpenAlex does not fully support corresponding authors, articles enabled by transformative agreements were estimated by matching first author affiliations with participating institutions, considering agreement durations from the ESAC Registry, as described in Jahn (2025b).\nProcessing was performed on Google BigQuery, with dataset compilation for the initial version completed in April 2025.\nData files are available from Zenodo and programmatically via the Open Scholarly Data warehouse (dataset openbib).\nData files\nThe dataset comprises four main files:\nHistoric cOAlition S Transformative Agreement Data\njct_journals links journals to transformative agreements\njct_institutions links participating institutions to agreements\nESAC snapshot\njct_esac - Metadata about agreements including publisher name and duration\nArticles under Transformative Agreements\njct_articles links OpenAlex articles to agreements through journals, institutions, and duration\nFull documentation of data files is available in the data documentation.\nUse case\nIn the following, a use case based on Google BigQuery is presented.\nAnyone can view and query this data with a Google Cloud Computing account, with standard usage fees applying for querying the data.\nThe dataset is also available on Zenodo.\nHow many articles were enabled by transformative agreements?\nThis query retrieves annual counts for articles enabled by transformative agreements, focusing on articles and reviews as classified by OpenAlex:\n\nSELECT\n  publication_year,\n  esac.publisher,\n  COUNT(DISTINCT(jct.id)) AS n,\nFROM\n  `subugoe-collaborative.openbib.jct_articles` AS jct\nINNER JOIN\n  `subugoe-collaborative.openalex.works` AS oalex\nON\n  oalex.doi = jct.doi\nINNER JOIN\n  `subugoe-collaborative.openbib.jct_esac` AS esac\nON\n  esac.id = jct.esac_id\nWHERE oalex.type IN ('article', 'review') AND is_paratext = FALSE\nGROUP BY\n  publication_year,\n  publisher\nORDER BY\n  publication_year DESC,\n  n DESC\n\n\n\nbq_df\n\n#> # A tibble: 292 × 3\n#>    publication_year publisher                      n\n#>               <int> <chr>                      <int>\n#>  1             2025 Wiley                      13741\n#>  2             2025 Elsevier                   11194\n#>  3             2025 Springer Nature             7930\n#>  4             2025 Taylor & Francis            6035\n#>  5             2025 Sage                        3194\n#>  6             2025 Oxford University Press     2700\n#>  7             2025 American Chemical Society   1202\n#>  8             2025 Royal Society of Chemistry   697\n#>  9             2025 American Physical Society    581\n#> 10             2025 Cambridge University Press   548\n#> # ℹ 282 more rows\n\n\n\n\nFigure 1: Growth of articles enabled by transformative agreements between 2020 and 2024, showing the dominance of the five largest commercial publishers in the scholarly publishing market.\n\n\n\nFigure 1 shows the growth of articles enabled by transformative agreements between 2020 and 2024, highlighting the dominance of five major commercial publishers, with Elsevier, Springer Nature and Wiley leading.\nHow many articles were made open access by transformative agreements?\nTransformative agreements vary in structure and implementation.\nJournal bundles may include open access journals, hybrid journals, and subscription journals, with varying document types allowed and potential limitations on open access article numbers.\nThe following query examines the open access status of articles enabled by transformative agreements:\n\nSELECT\n  publication_year,\n  esac.publisher,\n  oalex.open_access.oa_status,\n  COUNT(DISTINCT(jct.id)) AS n,\nFROM\n  `subugoe-collaborative.openbib.jct_articles` AS jct\nINNER JOIN\n  `subugoe-collaborative.openalex.works` AS oalex\nON\n  oalex.doi = jct.doi\nINNER JOIN\n  `subugoe-collaborative.openbib.jct_esac` AS esac\nON\n  esac.id = jct.esac_id\nWHERE oalex.type IN ('article', 'review') AND is_paratext = FALSE\nGROUP BY\n  publication_year,\n  publisher, \n  oalex.open_access.oa_status\nORDER BY\n  publication_year DESC,\n  n DESC\n\n\n\nbq_oa_df\n\n#> # A tibble: 1,243 × 4\n#>    publication_year publisher        oa_status     n\n#>               <int> <chr>            <chr>     <int>\n#>  1             2025 Wiley            hybrid     7761\n#>  2             2025 Elsevier         hybrid     5417\n#>  3             2025 Springer Nature  hybrid     4682\n#>  4             2025 Elsevier         closed     4491\n#>  5             2025 Taylor & Francis hybrid     3597\n#>  6             2025 Wiley            closed     3326\n#>  7             2025 Sage             closed     2495\n#>  8             2025 Wiley            gold       2188\n#>  9             2025 Taylor & Francis closed     2110\n#> 10             2025 Springer Nature  gold       1562\n#> # ℹ 1,233 more rows\n\n\n\n\nFigure 2: Articles covered by transformative agreements by open access status\n\n\n\nFigure 2 shows open access by business model.\nThe majority of articles were made available in hybrid journals.\nThe notable number of closed articles may reflect matching limitations or complexities of transformative agreements regarding journal inclusion, article caps, and document type restrictions.\nIt may also signal issues with OpenAlex open access tagging.\nResponsible use\nWhile the data presented allow analysis of transformative agreements using bibliometric databases, some shortcomings must be acknowledged.\nThe public Transformative Agreement Data Dumps from the Journal Checker Tool and the ESAC Registry are voluntary, crowd-sourced efforts. The information is subject to change.\nDue to limited publicly available invoice data, the article dataset only represents estimates based on first author affiliations according to OpenAlex.\nAlthough transformative agreement guidelines typically refer to corresponding authors (data not fully available in OpenAlex), research has shown a strong correlation between first and corresponding authorship at the level of publishers and countries.\nFunding information\nThis work was supported by the Federal Ministry of Education and Research of Germany (BMBF) under grants 16WIK2301E / 16WIK2101F.\n\n\n\nAchterberg, Inke, and Najko Jahn. 2023. “Introducing the Hybrid Open Access Dashboard (HOAD).” cOAlition S. https://www.coalition-s.org/blog/introducing-the-hybrid-open-access-dashboard-hoad/.\n\n\nBakker, Caitlin, Allison Langham-Putrow, and Amy Riegelman. 2024. “Impact of Transformative Agreements on Publication Patterns: An Analysis Based on Agreements from the ESAC Registry.” International Journal of Librarianship 8 (4): 67–96. https://doi.org/10.23974/ijol.2024.vol8.4.341.\n\n\nde Jonge, Hans, Bianca Kramer, and Jeroen Sondervan. 2025. “Tracking Transformative Agreements Through Open Metadata: Method and Validation Using Dutch Research Council NWO Funded Papers.” MetaArXiv. https://doi.org/10.31222/osf.io/tz6be_v1.\n\n\nDér, Ádám. 2025. “What Gets Missed in the Discourse on Transformative Agreements.” Katina Magazine, February. https://doi.org/10.1146/katina-20250212-1.\n\n\nHaupka, Nick, Jack Culbert, Paul Donner, Najko Jahn, Christopher Lenke, Philipp Mayr, Andreas Meier, et al. 2025. “OPENBIB: Selected Curated Open Metadata Based on OpenAlex.” Kompetenznetzwerk Bibliometrie. https://doi.org/10.5281/zenodo.15308680.\n\n\nJahn, Najko. 2025a. “Estimating Transformative Agreement Impact on Hybrid Open Access: A Comparative Large-Scale Study Using Scopus, Web of Science and Open Metadata.” https://arxiv.org/abs/2504.15038.\n\n\n———. 2025b. “How Open Are Hybrid Journals Included in Transformative Agreements?” Quantitative Science Studies 6 (January): 242–62. https://doi.org/10.1162/qss_a_00348.\n\n\nKramer, Bianca. 2024. Study on Scientific Publishing in Europe – Development, Diversity, and Transparency of Costs. Publications Office of the European Union. https://doi.org/doi/10.2777/89349.\n\n\nSchimmer, Ralf, Kai Geschuhn, and Andreas Vogler. 2015. “Disrupting the subscription journals’business model for the necessary large-scale transformation to open access.” Max Planck Digital Library. https://doi.org/10.17617/1.3.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-06-11T10:51:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/scopus_oa_tagging_changes/",
    "title": "Changes in evidence for green open access in Scopus",
    "description": "In March 2024, Scopus announced changes to its open access tagging policy to better align with the Unpaywall definitions. In this blog post, I examine the impact of the policy change by comparing three Scopus snapshots, comprising around 20 million records. Although the overall share of open access did not change, the analysis found a decrease in the number of copies in repositories, affecting about 2 million items, that cannot be explained by Unpaywall changes.",
    "author": [
      {
        "name": "Sophia Dörner",
        "url": {}
      }
    ],
    "date": "2024-12-16",
    "categories": [],
    "contents": "\n\n\n\nOn 23 March 2024 Scopus announced changes to its open access (OA) tagging policy to better align with the definitions of Unpaywall, the OA evidence source that Scopus uses for its bibliometric database.\nAccording to the announcement, these changes affected approximately 2 million items relative to publisher-provide OA where the OA status has changed from bronze to hybrid or gold, or from hybrid to gold.\nDespite this, Scopus claims that no items previously tagged as OA lost any OA status tags, or that articles published in closed access wrongly received an OA tag.\nIn this blogpost, I investigate the extent of the OA tagging policy update using three different Scopus snapshots provided by the German Competence Network of Bibliometrics.\nUsing a shared corpus of more than 19 million items, I contrasted the situations before and after the policy changes came into effect.\nResults confirm shifts between tags relative to publisher-provided OA.\nHowever, a total of 1,879,531 items lost evidence about green open access tags.\nChecking a sample against the Unpaywall API suggests that these changes were only made on the Scopus side.\nData and Method\nTo analyse the effects of open access tagging policy changes in Scopus, I retrieved 19,519,565 items indexed in Scopus that were published between 2019 and 2023.\nData were obtained from the German Competence Network of Bibliometrics using the Scopus April 2024, July 2024 and October 2024 snapshots.\nThe three snapshots were matched by Scopus item ID to build a shared corpus.\nData preparation also involved labelling the Scopus provided open access tags to allow for comparision across the snapshots.\n\nScopus provided open access tag\nrenamed value\npublisherfullgold\ngold\npublisherhybridgold\nhybrid\npublisherfree2read\nbronze\nrepositoryvor\ngreen (vor)\nrepositoryam\ngreen (am)\nNULL\nnone\nIn some cases items did not have any open access tag assigned in one or several of the investigated snapshots.\nThe respective entries had NULL values in the open access status column.\nThose were renamed to none in order to keep the values during data transformation and analysis.\nThe differentiation between the two available green open access tags indicates the manuscript version published, where vor stands for version of record and am stands for accepted manuscript.\nResults\nOverall, Scopus recorded open access tags for approximately 46% of the publications it indexed between 2019 and 2023. The following table shows that the number and proportion of open access is consistent across the three snapshots.\nScopus records representing journal articles published between 2019 and 2023 with open access evidence across three different database snapshots.\nSnapshot\nRecords with OA tag\nShare (in %)\nApril 24\n8,975,035\n45.98\nJuly 24\n8,959,215\n45.90\nOctober 24\n8,943,508\n45.82\nThe barplot below illustrates the distribution of open access tags for each of the three snapshots.\nItem numbers with a bronze open access tag declined between the April and the July snapshots, while the number of items with a gold open access tag increased.\nThese changes are in accordance with the Scopus announcement.\nHowever, the number of items tagged as green open access dropped between the April and July snapshots without this being offset by any other OA type.\n\n\n\nTo explore possible shifts between OA categories, the sankey plot shows the flows between snapshots, i.e. the number of tags assigned between snapshots.\nLike Unpaywall, Scopus assigns multiple open access tags to a single item in case more than one open access location could be found.\nFor the 19,519,565 items analysed here, the number of open access tags per item varied between none and three; nodes titled Missing indicate items where this number of tags varied between snapshots for the same item.\n\n\n\nThere was a notable change relative to publisher-provided OA: 386,876 items with a bronze open access tag in the April snapshot were tagged with a gold open access tag in the July snapshot, which accounts for most of the bronze status changes.\nThis suggests an improved identification of full OA journals.\nBut also substantial changes regarding repository-provided OA can be observed: Comparing the April and July snapshots (see green highlighted flows on the left in the sankey plot), a total of 1,329,606 items with one of the green open access tags in the April snapshot lost this status in the July snapshot.\nThis number decreases to 549,925 when comparing the July and October snapshots (see green highlighted flows on the right in Figure sankey plot).\nAlthough items did not lost their open access status through the changes of the tagging policy, my analysis found a decrease of evidence for copies in repositories as indicate by the tags green (am) and green (vor).\nTo better understand this difference, I took two samples of 10,000 DOIs representing items loosing green open access status between the April and July or July and October snapshots and queried the Unpaywall API to retrieve open access status information for these DOIs.\nBetween the April and July snapshots, Unpaywall recorded a green open access version for 85%.\nFurthermore, Unpaywall assigned green as primary open access status to 3.4%. Comparing the July and October snapshots samples shows a similar result: Unpaywall found a repository copy for 85%.\nHere, Unpaywall assigned green as primary open access status to 2.5% of the investigated items.\nAlthough the number of articles for which Unpaywall identified a repository copy decreased slightly between the two comparisons, in most cases Unpaywall tracked a copy in a repository.\nThe analysis suggests that the underlying reason for the absence of green OA tags in Scopus cannot be explained by changes in the Unpaywall data.\nFurthermore, after manually checking the raw data used to populate the bibliometrics database of the German Competence Network of Bibliometrics and the Scopus online database, I was unable to detect any errors that could explain the decline in green open access evidence in Scopus.\nDiscussion\nRecent changes to the Scopus open access tagging policy are not fully consistent with Scopus documentation.\nExamining a shared corpus of around 20 million records representing journal articles between 2019 and 2023, the results suggest that green open access evidence was removed from Scopus after the policy changes were introduced, affecting around 2 million records.\nThis decline cannot be explained by changes to Unpaywall, the source of open access evidence used by Scopus.\nAlthough the overall share of open access remained constant over the three snapshots examined, analyses of open access that include green open access need to take these changes into account.\nCode availability\nThe code used for data preparation, analysis and visualisation is available on GitHub.\nUpdate January 2025\nOur enquiry with Elsevier has confirmed that there was a problem with the Unpaywall Data Matcher in Scopus. This has been resolved so that the number of green OA tags increased to the correct value in December 2024.\n\n\n\n",
    "preview": "posts/scopus_oa_tagging_changes/distill-preview.png",
    "last_modified": "2025-02-12T11:20:33+01:00",
    "input_file": {},
    "preview_width": 980,
    "preview_height": 760
  },
  {
    "path": "posts/oal_document_types_classifier/",
    "title": "Identifying journal article types in OpenAlex",
    "description": "Identifying suitable types of journal articles for bibliometric analyses is important. In this blog post, I present a document type classifier that helps to identify research contributions like original research articles using Crossref and OpenAlex. The classifier and classified OpenAlex records are openly available.",
    "author": [
      {
        "name": "Nick Haupka",
        "url": {}
      }
    ],
    "date": "2024-10-24",
    "categories": [],
    "contents": "\nJournals publish different types of articles.\nOriginal research articles and reviews are the most common and are most often used in bibliometric analyses.\nBut there are also other types, such as letters to the editor or book reviews, which are often not considered.\nBoth the vocabulary used to describe journal article types and the methods used to assign them vary between bibliometric databases.\nFor example, when analysing the classification in OpenAlex, Web of Science (WoS), Scopus, PubMed and Semantic Scholar, I found that OpenAlex tends to overestimate the assignment of the document type ‘article’. OpenAlex tagged 10% of items as articles, which were labelled as editorial material in Scopus and the Web of Science.\nIn this blog post, I will present a classifier designed to improve the identification of journal article types in open scholarly databases like OpenAlex.\nThe classifier uses metadata from Crossref and OpenAlex, including the number of references, citations and affiliations, to assess whether an item in a journal is a research contribution or not.\nTo train the classifier, I used open scholarly data from PubMed, OpenAlex and Crossref.\nAfter introducing the classifier, I will compare my approach with that of OpenAlex, Scopus and the methodology employed by the CWTS to identify core publications.\nBoth the source code of the classifier and the classified records are publicly accessible.\nData and Methods\nThe journal article type classifier was trained on approximately 9.5 million journal articles from PubMed, representing either research discourse or editorial discourse.\nPubMed was used because its classification of document types is similar to that of Scopus and Web of Science, with the advantage that PubMed data is openly accessible and reusable.\nThe training data was limited to the publication years 2012 to 2022.\nIn addition, articles from publishers with fewer than 5,000 publications were excluded from the training.\nSimilar to the CWTS approach to identify core publications, the classifier also takes into account metadata retrieved from Crossref and OpenAlex.\nThese metadata fields are:\nMetadata\nType\nRetrieved from\nhas abstract?\nBOOLEAN\nCrossref (October 2023 snapshot)\ntitle word count\nINT\nCrossref (October 2023 snapshot)\npage count\nINT\nCrossref (October 2023 snapshot)\nauthor count\nINT\nCrossref (October 2023 snapshot)\nhas license?\nBOOLEAN\nCrossref (October 2023 snapshot)\nnumber of citations\nINT\nCrossref (October 2023 snapshot)\nnumber of references\nINT\nCrossref (October 2023 snapshot)\nhas funding information?\nBOOLEAN\nCrossref (October 2023 snapshot)\nnumber of affiliations\nINT\nOpenAlex (April 2024 snapshot)\nhas OA url?\nBOOLEAN\nOpenAlex (April 2024 snapshot)\nThe dataset was then split into 75% training data and 25% test data.\nFor the classifier, I used the k-nearest neighbours algorithm.\nParameters for the algorithm were optimised using grid search.\n\nThe exact parameters that were used for the algorithm and also the programming code are available on GitHub.\nThe classifier is build on top of OpenAlex rule-based paratext recognition, which uses title heuristics.\nThis means that my classifier cannot classify pre-labelled editorial material from OpenAlex as research items.\nTo evaluate my model, I compared the results with OpenAlex, Scopus and the CWTS Leiden Ranking Open Edition.\nThe CWTS Leiden Ranking Open Edition is a university ranking based on a pre-computed subset of so called core publications indexed in OpenAlex to compare universities.\nA publication is considered a core publication if it has one or more authors, has at least one reference, is published in English and is also published in a core journal (Van Eck and Waltman 2024).\nA journal is considered a core journal if it has an international scope and also contains a high number of references. The CWTS approach is therefore more selective than my approach, because I did not exclude journals and non-English contributions.\nFor the comparison I used the September 2024 snapshot from OpenAlex, the July 2024 snapshot from Scopus and the data underlying the Leiden Ranking Open Edition 2024, which is available through Google BigQuery. Matching were carried out by DOI.\nResults\nOverall, the classifier categorised 12.647.946 out of 108.744.219 journal items, which were classified as articles and reviews in OpenAlex, as non-research items, representing a share of 11.6%. When restricted to the period 2012–2021, this figure drops to 3.778.393 out of 38.265.399 journal items (9.9%), presumably due to improved metadata coverage used to determine document types. Figure 1 compares the results of the classifier with OpenAlex and the CWTS approach, based on items in journals between 2012 and 2021. The left side of the figure displays all journals in OpenAlex, whereas the data from the figure on the right side is restricted to CWTS core journals. The grey line represents all articles and reviews in journals in OpenAlex. The green line indicates all articles and reviews in OpenAlex for which at least one reference and one citation were identified. The purple line below shows all items from journals in OpenAlex included as core publications in the CWTS Leiden Ranking Open Edition. The yellow line illustrates the proportion of items in OpenAlex that are recognised as research items by the classifier. About 48.3% of all journal items in OpenAlex from 2012 to 2021 were categorised as core publications by the CWTS when not restricting to core journals, demonstrating the effect of also using journal characteristics to define eligble publications. Overall, 27.72 of 203.545 journals (13.6%) in OpenAlex were considered as core journals by the CWTS in its latest Open Leiden Ranking. But also missing authors, affiliations and references had an impact on the CWTS classification. In contrast, my classifier determined about 84,5% of journal items as research items when not restricting to core journal. When restricted to core journals, it shows that the classifier is less sensitive to missing metadata or a low number of citations. In contrast, the proportion of CWTS core publications and the proportion of publications with at least one reference and one citation in OpenAlex are similar and achieve both about 71% on average.\n\n\n\nFigure 1: Comparison of my classifier with OpenAlex and the CWTS core classification.\n\n\n\nTo check for potentially discriminatory behaviour of my classifier, I compared my approach with the CWTS approach by journal topic (see Figure 2). Again, a common corpus based on DOI matching is used. Figure 2a and 2b show all journals in OpenAlex, while the data in Figure 2c and Figure 2d are restricted to the CWTS core journals. Figures 2a and 2b show that my classifier treats publications from different disciplines in a more balanced way, while the CWTS method excludes publications from the social sciences and humanities more often, probably due to the exclusion of non-core journals. In this respect, the CWTS points out that many social science journals were not considered core journals due to the lack of a suitable number of references. When using core journals (Figure 2c and Figure 2d) the behaviour changes. Here, my classifier and the CWTS classification show a similar pattern, but with health sciences publications being excluded more often. These publications may be case reports, which usually do not contain references.\n\n\n\nFigure 2: Comparison of the coverage of my classifier with the CWTS core classification using topics from OpenAlex.\n\n\n\nFigure 3 compares the proportion of articles and reviews in OpenAlex and Scopus in relation to the intersection of items in OpenAlex and Scopus in journals from 2012 to 2021 using a shared corpus based on DOI matching. The results of my classifier are represented by the yellow line.\nOpenAlex counts more items than Scopus when restricting to the document types articles and reviews (OpenAlex: 95.9% and Scopus: 87.6%).\nWith the help of the classifier, the retrieval of articles and reviews in OpenAlex can be improved (Classifier: 93.2%).\nNevertheless, Scopus counts were lower when comparing it to my method.\n\n\n\nFigure 3: Comparison of my classifier with OpenAlex and Scopus.\n\n\n\nDiscussion and Data Access\nThe comparison of my classifier with OpenAlex, Scopus and the CWTS has shown that it can help with document type assignments in open scholarly data sources such as Crossref and OpenAlex. My next step is to qualitatively check a larger sample. Meanwhile, OurResearch is also in the process of updating its classification of document types. Until then, my classifier can be used as a complementary tool for identifying suitable articles for bibliometric analysis using open scholarly data sources such as Crossref or OpenAlex.\nA major limitation of my classifier is that it often identifies clinical trials and case studies as non-research articles, as these often have no references or citations. However, this could be improved by adding open data from PubMed that classifies these document types.\nThe classified data is available via the publicly available Google BigQuery instance provided by the SUB Göttingen. Here you can also compare it with OpenAlex, Crossref and Semantic Scholar. To query it, you can use\n\nSELECT COUNT(DISTINCT(oal.doi)) AS n, type, label\nFROM 'subugoe-collaborative.openalex.works' AS oal\nJOIN 'subugoe-collaborative.resources.classification_article_reviews_september_2024' AS dt\n   ON oal.doi = dt.doi\nGROUP BY type, label\nORDER BY n DESC\n\nThe results will be constantly updated in line with the monthly release of OpenAlex and Crossref. I would be happy to get feedback about your experiences with the classifier!\nFunding\nThis work is funded by the Bundesministerium für Bildung und Forschung (BMBF) project KBOPENBIB (16WIK2301E). We acknowledge the support of the German Competence Center for Bibliometrics.\n\n\n\nVan Eck, Nees Jan, and Ludo Waltman. 2024. “A methodology for identifying core sources and core publications in OpenAlex.” Zenodo. https://doi.org/10.5281/zenodo.13879947.\n\n\n\n\n",
    "preview": "posts/oal_document_types_classifier/distill-preview.png",
    "last_modified": "2024-12-13T09:56:41+01:00",
    "input_file": {},
    "preview_width": 3896,
    "preview_height": 2213
  },
  {
    "path": "posts/openalex_document_types/",
    "title": "Recent Changes in Document type classification in OpenAlex compared to Web of Science and Scopus",
    "description": "In June 2024, we published a preprint on the classification of document types in OpenAlex and compared it with the scholarly databases Web of Science, Scopus, PubMed and Semantic Scholar. In this follow-up study, we want to investigate further developments in OpenAlex and compare the results with the proprietary databases Scopus and Web of Science.",
    "author": [
      {
        "name": "Nick Haupka",
        "url": {}
      },
      {
        "name": "Sophia Dörner",
        "url": {}
      },
      {
        "name": "Najko Jahn",
        "url": {}
      }
    ],
    "date": "2024-09-04",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\nIn June 2024, we submitted an analysis of publication and document types in OpenAlex in comparison with the proprietary databases Web of Science and Scopus and the open data sources Semantic Scholar and PubMed (Haupka et al. 2024).\nWe found substantial differences between these databases: While Web of Science and Scopus provided a comprehensive set of document types to describe works published in journals, OpenAlex supported only a comparably limited number of types.\nNotably, OpenAlex lacked a distinction between research articles and reviews, which can be crucial when calculating citation indicators.\nIn line with related studies (Alperin et al. 2024), we also observed discrepancies in the number of publications when restricting to certain document types.\nMeanwhile, in late May and late July 2024, OpenAlex introduced extended approaches to obtain publication and document types.\nAmong the four new categories were preprints and reviews. Using PubMed, OpenAlex identified approximately 4 million journal articles as editorials, erratum, letters, preprints, reviews, or retractions.\nOf course, we wanted to know how these improvements affect our findings.\nWe therefore re-applied our approach to the recent changes.\nUsing works published in journals between 2012 and 2022, we demonstrate that OpenAlex’s recent changes provide a more nuanced set of document types to refine scholarly works.\nHowever, the comparison with Web of Science and Scopus reveals that there remain considerable differences.\nData and Methods\nFollowing our preprint, we performed a pairwise comparison of journal publications indexed in OpenAlex with the Web of Science and Scopus published 2012 to 2022.\nTo investigate changes made in OpenAlex, we furthermore compared data from the OpenAlex July 2024 and August 2023 snapshots.\nScopus and Web of Science data were retrieved from the German Competence Network of Bibliometrics, using the April 2024 snapshots.\nWeb of Science data retrieval comprised the Core Collection.\nWe matched items between the databases by DOI after normalisation to lowercase.\nOverall, the intersection of OpenAlex and Scopus covered 24,704,172 and the intersection of OpenAlex and Web of Science covered 21,775,771 records.\nThen, we categorised works based on their document type information into two categories: research discourse and editorial discourse.\nThe research discourse category now also includes publications of type “preprint”, which was added to OpenAlex in May 2024.\nThe mapping tables used for reclassifying the document types can be found in the appendix of Haupka et al. (2024).\nFindings\nFigure 1 illustrates OpenAlex document type changes in comparison with Scopus.\nBefore the introduction of the more nuanced set of document types, OpenAlex tagged\n24,559,634 items (99.42%) as articles, which reduced to 22,132,347 (89.59%).\nScopus tagged 20,777,473 items (84.11%) as article.\nOpenAlex assigned the type review to 1,511,172 items (6.12%), whereas Scopus to 1,776,555 items (7.19%).\n\n\n\n\nFigure 1: Comparison of OpenAlex and Scopus for publication years 2012-2022\n\n\n\nFigure 2 illustrates the same for the comparison of OpenAlex with Web of Science.\nHere, OpenAlex tagged 21,673,833 items (99.53%) as articles before the introduction of the more nuanced set of document types and 19,500,710 (89.55%) after.\nIn Web of Science 17,266,997 items (79.29%) were tagged as articles.\nThe document type review is assigned to 1,362,290 items (6.26%) by OpenAlex, whereas Web of Science tagged 1,242,472 items (5.71%) as such.\n\n\n\n\nFigure 2: Comparison of OpenAlex and Web of Science for publication years 2012-2022\n\n\n\nOverall, Figures 1 and 2 demonstrate that even after the introduction of a more nuanced set of document types, OpenAlex still tags a higher proportion of items as articles than the commercial data sources.\nThe difference between the proportions of items tagged as articles is, however, slightly more pronounced in the comparison of OpenAlex with Web of Science.\nScopus tags a higher proportion of items as reviews and both Scopus and Web of Science still tag more items as editorial content than OpenAlex.\nIn sum, 340,998 (Scopus) and 656,366 (Web of Science) items are tagged as editorial/editorial material or letters in Scopus and Web of Science, respectively, while tagged as articles in OpenAlex.\nWhen grouping the document types into the two categories research discourse and editorial discourse, we found that even after the introduction of a more nuanced set of document types in OpenAlex, the proportion of items labelled as editorial discourse is still about 3% lower compared to Scopus and Web of Science, as shown in the tables below.\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion and Outlook\nOur updated analysis demonstrated a noticable improvement of the classification of document types in OpenAlex when comparing it to Scopus and Web of Science.\nCompared to data from 2023, the discrepancy in the classification of items has decreased slightly.\nThis indicates a convergence of the classification system in OpenAlex towards those from proprietary databases, with an enhanced coverage of reviews and editorial materials.\nIn addition, the rule-based string matching for recognising paratexts introduced and revised by OpenAlex resulted in more texts being categorised as editorial material than before.\nHowever, the results also show that the curation of document types has not yet been finalised.\nConclusively, we would like to point out that there is no correct classification system per se.\nRather different classification systems applied by the database operators can bring advantages and disadvantages.\nIn Semantic Scholar and PubMed, for example, publications are labelled as clinical studies and case reports, which in Scopus, Web of Science and OpenAlex are predominantly assigned to the document type article.\nA differentiation of these publications has the potential to increase the quality of bibliometric surveys in the analysed databases.\nAlso, the results from this analysis are only partially comparable with the results from our preprint, as in the preprint we worked with a more restrictive set that included publications from Semantic Scholar and PubMed.\nFunding\nThis work is funded by the Bundesministerium für Bildung und Forschung (BMBF) project KBOPENBIB (16WIK2301E). We acknowledge the support of the German Competence Center for Bibliometrics.\n\n\n\nAlperin, Juan Pablo, Jason Portenoy, Kyle Demes, Vincent Larivière, and Stefanie Haustein. 2024. “An Analysis of the Suitability of OpenAlex for Bibliometric Analyses.” arXiv. https://doi.org/10.48550/arXiv.2404.17663.\n\n\nHaupka, Nick, Jack H. Culbert, Alexander Schniedermann, Najko Jahn, and Philipp Mayr. 2024. “Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, PubMed and Semantic Scholar.” https://arxiv.org/abs/2406.15154.\n\n\n\n\n",
    "preview": "posts/openalex_document_types/distill-preview.png",
    "last_modified": "2024-09-04T15:29:30+02:00",
    "input_file": {},
    "preview_width": 1416,
    "preview_height": 1250
  },
  {
    "path": "posts/oalex_oa_status/",
    "title": "Analysing and reclassifying open access information in OpenAlex",
    "description": "We investigated OpenAlex and found over four million records with incompatible metadata about open access works. To illustrate this issue, we applied Unpaywall's methodology to OpenAlex data. The comparative analysis revealed a shift, with over one million journal articles published in 2023 that were previously labelled as \"closed\" in OpenAlex, being reclassified as \"gold\", \"hybrid\", \"green\", or \"bronze\".",
    "author": [
      {
        "name": "Najko Jahn",
        "url": {}
      },
      {
        "name": "Nick Haupka",
        "url": {}
      },
      {
        "name": "Anne Hobert",
        "url": {}
      }
    ],
    "date": "2023-11-07",
    "categories": [],
    "contents": "\nOver the last few months, we have switched our data source for open access analytics from Unpaywall to OpenAlex. Both open scholarly data services are developed by OurResearch and have a similar metadata format for describing open access full-texts. However, OpenAlex provides monthly data dumps, which we find particularly helpful as the release of free snapshot versions from Unpaywall appear to have been discontinued since March 2022.\nWhile transitioning from Unpaywall to OpenAlex, we noticed more than four million OpenAlex records with contradictory open access metadata. This blog post aims to explore this issue. To better understand this, we reimplemented Unpaywall’s open access classification using OpenAlex data, and compared our relabelled open access status information against OpenAlex’ existing data.\nWhat is the issue?\nOpenAlex provides various methods for identifying open access literature. Within the work object, the open_access and best_oa_location elements, among others, contain information about the open access status at the article level. The sources object, on the other hand, gives information about the open access model of a journal.\nThe issue we have identified is that filtering for open access works with is_oa:true returns more than four million records with the oa_status marked as closed, a discrepancy that is inconsistent with OpenAlex’s own documentation.\nAccordingly, OpenAlex follows Unpaywall’s methodology, tagging openly available works (is_oa) and qualifying their open access status (oa_status) using the following labels:\ngold: Published in an open access journal.\ngreen: Toll-access on the publisher landing page, but there is a free copy in an OA repository.\nhybrid: Free under an open license in a toll-access journal.\nbronze: Free to read on the publisher landing page, but without any identifiable license.\nIn case no open access full-text could be found, the open access status is marked as “closed”.\nUnderstanding the issue\nTo better understand this issue, we analysed the most recent OpenAlex snapshot from October 2023. After importing the data into our BigQuery data warehouse, we created a subset focusing on journal articles published since 2013, excluding retractions and non-scholarly content published in journals.\n\nCREATE OR REPLACE TABLE\n  subugoe-collaborative.resources.oalex_cr_journal_articles_13_23 AS (\n  SELECT\n  doi,\n  publication_year,\n  open_access,\n  best_oa_location,\n  sources.is_oa AS journal_is_oa,\n  sources.is_in_doaj AS journal_is_in_doaj,\n  sources.host_organization_name AS publisher_name\nFROM\n  `subugoe-collaborative.openalex.works`\nLEFT JOIN\n  `subugoe-collaborative.openalex.sources` AS sources\nON\n  primary_location.source.id = sources.id\nWHERE\n  type_crossref = \"journal-article\"\n  AND is_paratext = FALSE\n  AND is_retracted = FALSE\n  AND publication_year BETWEEN 2013\n  AND 2023 )\n\nWe then analysed the open access prevalence over the years, aggregating the record counts across both is_oa and oa_status.\n\nSELECT\n  COUNT(DISTINCT doi) AS articles,\n  publication_year,\n  open_access.is_oa,\n  open_access.oa_status\nFROM\n  `subugoe-collaborative.resources.oalex_cr_journal_articles_13_23`\nGROUP BY\n  open_access.is_oa,\n  open_access.oa_status,\n  publication_year\nORDER BY\n  publication_year DESC\n\nThe resulting figure shows the distribution of open access evidence in OpenAlex over the years. All possible open access status values, as known from Unpaywall, were also represented in OpenAlex. The figure also presents the number of records with (blue bar chart stacks) or without (grey bar chart stacks) open access full-text according to the information provided by is_oa. Notably, the bulk of contradictory open access information could be found in records representing journal articles published in 2023, with 1,197,013 articles tagged as open access, but assigned the open access status “closed”.\n\n\n\nReclassification and analysis of changes\nTo address this inconsistency, we reimplemented Unpaywall’s open access classification methodology. The SQL code snippet shows how we approached reclassification.\n\nCREATE OR REPLACE TABLE\n  `subugoe-collaborative.resources.oalex_reclassify_oa` AS (\n  SELECT\n    DISTINCT doi,\n    publication_year,\n    open_access.is_oa,\n    open_access.oa_status,\n    CASE\n      WHEN best_oa_location IS NULL THEN \"closed\"\n      WHEN best_oa_location.source.type = \"repository\" THEN \"green\"\n      WHEN (journal_is_in_doaj = TRUE OR journal_is_oa = TRUE) THEN \"gold\"\n      WHEN (journal_is_in_doaj = FALSE\n      AND journal_is_oa = FALSE )\n    AND best_oa_location.license IS NOT NULL THEN \"hybrid\"\n      WHEN (journal_is_in_doaj = FALSE AND journal_is_oa = FALSE ) AND best_oa_location.license IS NULL THEN \"bronze\"\n    ELSE\n    NULL\n  END\n    AS oa_new\n  FROM\n    `subugoe-collaborative.resources.oalex_cr_journal_articles_13_23` )\n\nBecause of the inconsistent use is_oa compared to the open access status labels, we used the best_oa_location element instead to determine the availability of at least one open access full-text. If this metadata element was absent, we categorised the work as “closed”. For open access works not exclusively provided by a repository (“green”), we used open access journal information from the source object to distinguish between “gold”, “hybrid”, and “bronze”.\nAfter reclassification, we calculated the updated open access statistics.\n\nSELECT\n  COUNT(DISTINCT doi) AS n,\n  oa_status,\n  oa_new,\n  publication_year\nFROM\n  `subugoe-collaborative.resources.oalex_reclassify_oa`\nGROUP BY\n  oa_status,\n  oa_new,\n  publication_year\n\nThe following figure compares OpenAlex open access classification (black bars) with our approach (pink bars). Notably, the reclassification resulted in many journal articles published in 2023 that were previously tagged as “closed” having one of the open access values “gold”, “hybrid”, “green”, or “bronze”.\n\n\n\nOverall, we reclassified a total of 4,087,711 records representing journal articles published since 2013, with 1,257,175 of them being published in 2023.\nThe following figure demonstrates changes in open access status after our reclassification for 2023.\nThe “gold” category gained 607,896 additional records in 2023, “hybrid” gained 340,351, “green” gained 96,156, and “bronze” gained 211,065. The figure also highlights that we not only relabelled records that previously belonged to the “closed” category but that there were also changes between other categories.\n\n\n\n\n\n\nDiscussion and conclusion\nAnalysing and reclassifying open access data in OpenAlex revealed inconsistencies in the actual implementation. The is_oa filter, which indicates the availability of open access full texts, did not always match the open access status information.\nIn response, we share this detailed problem description to contribute to the ongoing improvement of OpenAlex, a scholarly data source that we enjoy working with on a daily basis. As a practical suggestion in the meantime, we recommend not relying solely on the open access information provided. Instead, we suggest reclassifying open access status information based on OpenAlex’ comprehensive metadata about open access full-text availability, for example by reusing the code snippets provided within this blog post.\nFunding\nThis work is funded by the Bundesministerium für Bildung und Forschung (BMBF) projects KBMINE (16WIK2101F) and KBOPENBIB (16WIK2301E). We acknowledge the support of the German Competence Center for Bibliometrics.\n\n\n\n",
    "preview": "posts/oalex_oa_status/distill-preview.png",
    "last_modified": "2023-12-04T11:25:47+01:00",
    "input_file": {},
    "preview_width": 3600,
    "preview_height": 2224
  },
  {
    "path": "posts/oam_hybrid/",
    "title": "How open are hybrid journals included in nationwide transformative agreements in Germany?",
    "description": "We present hoaddata, an experimental R package that combines open scholarly data from the German Open Access Monitor, Crossref and OpenAlex. Using this package, we illustrate the progress made in publishing open access content in hybrid journals included in nationwide transformative agreements in Germany across journal portfolios and countries.",
    "author": [
      {
        "name": "Najko Jahn",
        "url": "https://twitter.com/najkoja"
      },
      {
        "name": "Nick Haupka",
        "url": "https://github.com/naustica"
      }
    ],
    "date": "2022-06-07",
    "categories": [],
    "contents": "\n\n\n\nAssessing the volume and share of open access articles in hybrid\njournals is crucial for the planning and implementation of transformative\nagreements, an evolving type of contracts between libraries and\npublishers where subscription spending is repurposed for open access\npublishing (Schimmer, Geschuhn, and\nVogler 2015). In particular, library consortia who mainly\nnegotiate transformative agreements with large publishers require such\npublication data according to the recently published ESAC\nReference Guide to Transformative Agreements. Here, we present hoaddata, an experimental\nR package, in which openly available journal-level data about nationwide\ntransformative agreements in Germany are combined with article-level\nopen access status information and country affiliations. Accordingly,\nhoadata provides essential data for the monitoring and benchmarking of\ntransformative agreements across hybrid journal portfolios and\ncountries.\n\nInteracting with data through R packages like hoaddata makes data\nanalytics more transparent because R packages meet generic principles\nfor computational reproducibility: coherent file organisation,\nseparation of data, methods and results, and specification of the\ncomputational environment (Marwick, Boettiger, and Mullen 2018).\nFor data science practitioners, R packages, thus, provide a reliable way\nto re-use data and code. In our specific case, hoaddata not only\ncontains datasets about hybrid open access. It also comprises code used\nto compile the data by interfacing our cloud-based Google\nBig Query data warehouse, where we store open scholarly data from Crossref, OpenAlex and Unpaywall. hoaddata is automatically\nbuilt and updated with GitHub Actions, a continuous integration service.\nEach merge event into the main branch triggers the execution of code to\nobtain up-to-date data about transformative agreements from the most\nrecent open scholarly data snapshots available in our data warehouse.\nData changes including updates will be incorporated in the package and\ntracked with Git that allows to reproduce different version of the data\ncontained in hoaddata.\nIn this blog post, we describe the data analytics workflow behind\nhoaddata. The main purpose of hoaddata is to ship data for monitoring\ndashboards about the progress of nationwide transformative agreements,\nwhich we currently develop in the HOAD project with\nthe support of the Deutsche Forschungsgemeinschaft, but everyone can\ninstall the package from GitHub and use it in R. To demonstrate its\npotential, we will use hoaddata to illustrate the current state of the\ntransition of hybrid journals to fully open access relative to those\njournals, which are included in nationwide transformative agreements in\nGermany.\nData and methods\nhoaddata focuses on nationwide transformative agreements in Germany.\nAs a first step, we draw on the work of the German Open Access Monitor\n(OAM) to obtain a list of journals under these agreements (Pollack et al. 2022). We merged all\njournals into a single data file and enriched it with missing ISSN\nvariants. Because publishers register journal-level metadata in Crossref\nwhen they first deposit metadata for a given journal including ISSNs, we\nfurthermore matched the OAM journal list with Crossref’s\ntitle list to link ISSN variants to journals as they were\nrepresented in Crossref metadata.\n\nThis thread\nis helpful to better understand journal curation workflows in Crossref.\nAfter obtaining a list of hybrid journals linked to nationwide\ntransformative agreements, we determined the article volume by journal\nand year using Crossref. Following Unpaywall’s approach, Crossref\nmetadata records considered as front matter were excluded.1\nOpen access articles were identified through Creative Commons license\nURLs in Crossref metadata records. License URL were mapped to the\ndifferent license versions like CC BY.\nBecause country affiliations are a key data point for nationwide\ntransformative agreements, we used OpenAlex to determine the country\nshare per journal and publisher portfolio. To our knowledge, OpenAlex\ndoes not provide information about corresponding authors and their\naffiliation, which is a key data point in most transformative\nagreements. Instead, we made use of first-author affiliations. A first\nauthor is often regarded as being the lead author who has\nusually undertaken most of the research presented in the article,\nalthough author roles can vary across disciplines. In case OpenAlex did\nnot record any country affiliation, we extracted country names from the\nmetadata field display_name using regular expressions. We\napplied full counting to account for multiple country affiliations.\nAs a result, hoaddata provides the following datasets:\noam_hybrid_jns:\nHybrid journals included in the Open Access Monitor. Data were gathered\nfrom Pollack et al. (2022), validated\nand mapped to Crossref-indexed journals.\ncc_jn_ind:\nPrevalence of Creative Commons license variants by year and hybrid\njournal as obtained from Crossref.\ncc_openalex_inst_jn_ind:\nFirst author country affiliations per journal, year and Creative Commons\nlicense. Country affiliations were gathered from OpenAlex.\nArticle-level data\ncc_openalex_inst:\nArticle-level affiliation data from first authors as obtained from\nOpenAlex. Covers only open access articles under a Creative Commons\nlicense in a hybrid journal.\nhoaddata can be installed from GitHub:\n# install.packages(\"remotes\")\nremotes::install_github(\"subugoe/hoaddash\", dependencies = \"Imports\")\nYou can also directly download the data as csv files from GitHub. The files are\nstored in the data-raw\nfolder of the package together with the code used to create the\ndatasets. Specific SQL queries can be found in inst/sql/.\nUse-Case:\nOpen access uptake in hybrid journals included in nationwide\ntransformative agreements in Germany\n\n\n\nAt the time of writing, hoaddata comprised information about 5,562\nhybrid journals included in twenty consortial transformative agreements\nin Germany. Since 2017, these journals have published 348,978 open\naccess articles with Creative Commons license, representing a share of\n10%.\nOpen\naccess uptake in Germany’s nationwide transformative agreements in\n2021\nUsing OpenAlex’s affiliation information, we can break down the\nperformance of transformative agreements to countries. Showing the\npublication year 2021, the following interactive table compares the\nglobal publication volume including open access with that of lead\nauthors based in Germany by hybrid journal portfolio. The table\nhighlights the dominant position of transformative agreements negotiated\nby the DEAL consortium in\nterms of articles published and open access. It is important to note\nthat DEAL has\nreached no agreement with Elsevier so far, one of the largest\nscholarly publishers, which is, in turn, not included in the\nanalysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsortium Lead\n\n\n\n\n\n\nData sources: Open Access Monitor Zeitschriftenlisten (v2), Crossref, OpenAlex. Last updated: 2022-06-07 .\n\n\n\n\n\n\n\n\n\n\n\nInterestingly, Germany’s open access share in 2021 performed in most\ncases below 80%, suggesting that not all authors made use of open access\noptions, or were not eligible to publish open access, likely because\ntheir institution was not part of a consortium. But also some article\ntypes might no be included in agreements. For instance, many article\ntypes in medical specialist journals from Springer Medizin, which are\ntargeted at medical practitioners, are not covered by DEAL.2 The low open access uptake in the\nhybrid journal portfolios from ACM, AIP, Hogrefe and SPIE suggest that\nthese publishers did not share Creative Commons license information\nthrough Crossref metadata records.\nThe table furthermore shows large discrepancies between the global\nopen access uptake in hybrid journals and Germany’s. In 2021, 16% of\narticles published in hybrid journals included in nationwide\ntransformative agreements in Germany were open access, while the open\naccess percentage among articles published by lead authors based in\nGermany was 65%. Overall, 20% of open access articles published in\nhybrid journals included in transformative agreements in 2021 were from\nGermany, although its total publication volume accounted for 4.9% of\narticles published.\nCountry overview\nAs can be seen from the following figure, which highlights the Top 20\nmost productive countries in terms of articles published, open access\nuptake in hybrid journals included in nationwide transformative\nagreements in Germany varies across countries. Notably, lead authors\nbased in the US, China or India – which together accounted for roughly\n40% of articles published – did utilize open access options to a much\nlesser extent than authors from European countries. Together with\nGermany, the UK, the Netherlands, Sweden and Switzerland have gained an\nopen access share above 50% over the years, most likely because of\nproviding nationwide transformative agreements with similar journal\ncoverage.\n\n\n\n\n\n\n\n\nFigure 1: Percentage of articles with a Creative Commons\nlicense per country. Showing the Top 20 most productive countries in\nterms of articles published by lead authors between 2017 and 2022 in\nhybrid journals included in nationwide transformative agreements in\nGermany. Country-specific charts are sorted by the total number of lead\nauthor articles. Data sources: Open Access Monitor Zeitschriftenlisten\n(v2), Crossref, OpenAlex.\n\n\n\n\nDiscussion and outlook\nOver the past years, Germany’s library consortia successfully\nnegotiated transformative agreements with commercial publishers,\nresulting in an increase of open access articles from lead authors based\nin Germany. But this growth of open access is neither balanced across\nhybrid journal portfolios nor across countries.\nAs illustrated, there are substantial variations across journal\nportfolios in terms of open access articles published by lead authors\nbased in Germany. They can be explained by different agreement terms\nsuch as the number and types of institutions involved or the restriction\nof open access publishing options to specific article types. In its\nrecent Reference\nGuide to Transformative Agreements, and in line with previous\nresearch (Borrego, Anglada, and\nAbadal 2020), the ESAC\ninitiative points out that agreement terms affecting the scope of\ncontracts can have a large impact on the performance of transformative\nagreements.\nIn terms of country variations, although the ESAC\nRegistry of Transformative Agreements discloses an increasing number\nof similar national-level agreements, more than 80% of articles\npublished in hybrid journals included in nationwide transformative\nagreements in Germany are still behind a paywall. Tracking open access\nacross country affiliations reveals that uptake rates are particular low\nin the most productive countries USA, China and India. It remains to be\nseen whether a transition of hybrid journal portfolios to fully open\naccess through transformative agreements is feasible given these\nobserved global differences.\nIn future, we want to use hoaddata as data source for monitoring\ndashboards about the progress of nationwide transformative agreements in\nGermany, which we currently develop in the HOAD project with the support\nof the Deutsche Forschungsgemeinschaft. We want to extend the R\npackage’s current scope on data about publication volume and open access\nuptake across agreements and countries. The aim is to highlight open\nmetadata gaps not only relative to Creative Commons license information.\nFollowing up on our work on metacheck, an email tool\nto check metadata compliance, we will also present information about the\ncoverage of Text and Data Mining support, funder infos, ORCIDs or Open\nAbstracts in Crossref metadata records; these data are critical for an\nopen and reproducible monitoring of transformative agreements.\nThe launch of dashboards specific to nationwide transformative\nagreements in Germany is targeted for September 2022.\n\n\n\nBorrego, Ángel, Lluís Anglada, and Ernest Abadal. 2020.\n“Transformative Agreements: Do They Pave the Way to Open\nAccess?” Learned Publishing 34 (2): 216–32. https://doi.org/10.1002/leap.1347.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\nPollack, Philipp, Barbara Lindstrot, Irene Barbers, and Franziska\nStanzel. 2022. “Open Access Monitor:\nZeitschriftenlisten.” Jülich DATA. https://doi.org/10.26165/JUELICH-DATA/VTQXLM.\n\n\nSchimmer, Ralf, Kai Geschuhn, and Andreas Vogler. 2015. “Disrupting the subscription journals’business model for\nthe necessary large-scale transformation to open access.”\nMax Planck Digital Library. https://doi.org/10.17617/1.3.\n\n\nhttps://support.unpaywall.org/support/solutions/articles/44001894783-what-does-is-paratext-mean-in-the-api-↩︎\nSee contract https://pure.mpg.de/rest/items/item_3174351_1/component/file_3189424/content↩︎\n",
    "preview": "posts/oam_hybrid/distill-preview.png",
    "last_modified": "2023-11-01T09:08:02+01:00",
    "input_file": {},
    "preview_width": 1910,
    "preview_height": 1066
  },
  {
    "path": "posts/oaire_graph_2020/",
    "title": "Accessing and analysing the OpenAIRE Research Graph data dumps",
    "description": "The OpenAIRE Research Graph provides a wide range of metadata about grant-supported research publications. This blog post presents an experimental R package with helpers for splitting, de-compressing and parsing the underlying data dumps. I will demonstrate how to use them by examining the compliance of funded projects with the open access mandate in Horizon 2020.",
    "author": [
      {
        "name": "Najko Jahn",
        "url": "https://twitter.com/najkoja"
      }
    ],
    "date": "2020-04-07",
    "categories": [],
    "contents": "\n\n\n\nOpenAIRE has collected and interlinked scholarly data from various openly available sources for over ten years. In December 2019, this open science network released the OpenAIRE Research Graph(Manghi, Atzori, et al. 2019), a big scholarly data dump that contains metadata about more than 100 million research publications and 8 million datasets, as well as the relationships between them. These metadata are furthermore connected to open access locations and disambiguated information about persons, organisations and funders.\nLike most big scholarly data dumps, the OpenAIRE Research Graph offers many data analytics opportunities, but working with it is challenging. One reason is the size of the dump. Although the OpenAIRE Research Graph is already split into several files, most of these data files are too large to fit the memory of a moderately equipped laptop, when directly imported into computing environments like R. Another challenge is the format. The dump consists of compressed XML-files following the comprehensive OpenAIRE data model(Manghi, Bardi, et al. 2019), from which only certain elements may be needed for a specific data analysis.\nIn this blog post, I introduce the R package openairegraph, an experimental effort, that helps to transform the large OpenAIRE Research Graph dumps into relevant small datasets for analysis. These tools aim at data analysts and researchers alike who wish to conduct their own analysis using the OpenAIRE Research Graph, but are wary of handling its large data dumps. Focusing on grant-supported research results from the European Commission’s Horizon 2020 framework programme (H2020), I present how to subset and analyse the graph using this openairegraph. My analytical use case is to benchmark the open access activities of grant-supported projects affiliated with the University of Göttingen against the overall uptake across the H2020 funding activities.\nWhat is the R package openairegraph about?\nSo far, the R package openairegraph, which is available on GitHub as a development version, has two sets of functions. The first set provides helpers to split a large OpenAIRE Research Graph data dump into separate, de-coded XML records that can be stored individually. The other set consists of parsers that convert data from these XML files to a table-like representation following the tidyverse philosophy, a popular approach and toolset for doing data analysis with R (Wickham et al. 2019). Splitting, de-coding and parsing are essential steps before analysing the OpenAIRE Research Graph.\nInstallation\nopenairegraph can be installed from GitHub using the remotes(Hester et al. 2019) package:\nlibrary(remotes)\nremotes::install_github(\"subugoe/openairegraph\")\nLoading a dump into R\nSeveral dumps from the OpenAIRE Research Graph are available on Zenodo(Manghi, Atzori, et al. 2019). So far, I tested openairegraph to work with the dump h2020_results.gz, which comprises research outputs funded by the European Commission’s Horizon 2020 funding programme (H2020).\nAfter downloading it, the file can be imported into R using the jsonlite package(Ooms 2014). The following example shows that each line contains a record identifier and the corresponding Base64-encoded XML file. Base64 is a standard that allows file compression in a text-based format.\n\n\nlibrary(jsonlite) # tools to work with json files\nlibrary(tidyverse) # tools from the tidyverse useful for data analysis\n# download the file from Zenodo and store it locally\noaire <- jsonlite::stream_in(file(\"data/h2020_results.gz\"), verbose = FALSE) %>%\n  tibble::as_tibble()\noaire\n\n\n#> # A tibble: 92,218 × 2\n#>    `_id`$`$oid`             body$`$binary`                    $`$type`\n#>    <chr>                    <chr>                             <chr>   \n#>  1 5dbc22f81e82127b58c41073 UEsDBBQACAgIAIRiYU8AAAAAAAAAAAAA… 00      \n#>  2 5dbc22f9b531c546e838683d UEsDBBQACAgIAIRiYU8AAAAAAAAAAAAA… 00      \n#>  3 5dbc22fa45e3122d97bdb313 UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#>  4 5dbc22fa45e3122d97bdb31e UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#>  5 5dbc22fa4e0c061a4d17b85d UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#>  6 5dbc22fb81f3c12c00238e25 UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#>  7 5dbc22fb895be12461552bf0 UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#>  8 5dbc22fbe56570673e1bf884 UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#>  9 5dbc22fc81f3c12bfe83e3b6 UEsDBBQACAgIAIViYU8AAAAAAAAAAAAA… 00      \n#> 10 5dbc22fcb531c546e838688a UEsDBBQACAgIAIZiYU8AAAAAAAAAAAAA… 00      \n#> # … with 92,208 more rows\n\nDe-coding and storing OpenAIRE Research Graph records\nThe function openairegraph::oarg_decode() splits and de-codes each record. Storing the records individually allows to process the files independent from each other, which is a common approach when working with big data.\n\n\nlibrary(openairegraph)\nopenairegraph::oarg_decode(oaire, records_path = \"data/records/\", \n  limit = 500, verbose = FALSE)\n\n\n\nopenairegraph::oarg_decode() writes out each XML-formatted record as a zip file to a specified folder. Because the dumps are quite large, the function furthermore has a parameter that allows setting a limit, which is helpful for inspecting the output first. By default, a progress bar presents the current state of the process.\nParsing OpenAIRE Research Graph records\nSo far, there are four parsers available to consume the H2020 results set:\nopenairegraph::oarg_publications_md() retrieves basic publication metadata complemented by author details and access status\nopenairegraph::oarg_linked_projects() parses grants linked to publications\nopenairegraph::oarg_linked_ftxt() gives full-text links including access information\nopenairegraph::oarg_linked_affiliations() parses affiliation data\nThese parsers can be used alone, or together like this:\nFirst, I obtain the locations of the de-coded XML records.\n\n\nopenaire_records <- list.files(\"data/records\", full.names = TRUE)\n\n\n\nAfter that, I read each XML file using the xml2(Wickham, Hester, and Ooms 2019) package, and apply three parsers: openairegraph::oarg_publications_md(), openairegraph::oarg_linked_projects() and openairegraph::oarg_linked_ftxt(). I use the future(Bengtsson 2020b) and future.apply(Bengtsson 2020a) packages to enable reading and parsing these records simultaneously with multiple R sessions. Running code in parallel reduces the execution time.\n\nfuture comes with a great introduction about parallel and distributed processing in R\n\n\nlibrary(xml2) # working with xml files\nlibrary(future) # parallel computing\nlibrary(future.apply) # functional programming with parallel computing\nlibrary(tictoc) # timing functions\n\nopenaire_records <- list.files(\"data/records\", full.names = TRUE)\n\nfuture::plan(multisession)\ntic()\noaire_data <- future.apply::future_lapply(openaire_records, function(files) {\n  # load xml file\n  doc <- xml2::read_xml(files)\n  # parser\n  out <- oarg_publications_md(doc)\n  out$linked_projects <- list(oarg_linked_projects(doc))\n  out$linked_ftxt <- list(oarg_linked_ftxt(doc))\n  # use file path as id\n  out$id <- files\n  out\n})\ntoc()\n\n\n#> 37.156 sec elapsed\n\noaire_df <- dplyr::bind_rows(oaire_data)\n\n\n\nA note on performance: Parsing the whole dump h2020_results using these parsers took me around 2 hours on my MacBook Pro (Early 2015, 2,9 GHz Intel Core i5, 8GB RAM, 256 SSD). I therefore recommend to back up the resulting data, instead of un-packing the whole dump for each analysis. jsonlite::stream_out() outputs the data frame to a text-based json-file, where list-columns are preserved per row.\n\n\njsonlite::stream_out(oaire_df, file(\"data/h2020_parsed_short.json\"))\n\n\n#> \nProcessed 500 rows...\nComplete! Processed total of 500 rows.\n\nUse case: Monitoring the Open Access Compliance across H2020 grant-supported projects at the institutional level\nUsually, it is not individual researchers who sign grant agreements with the European Commission (EC), but the institution they are affiliated with. Universities and other research institutions hosting EC-funded projects are therefore looking for ways to monitor the insitutions’s overall compliance with funder rules. In the case of the open access mandate in Horizon 2020 (H2020), librarians are often assigned this task. Moreover, quantitative science studies have started to investigate the efficacy of funders’ open-access mandates.(Larivière and Sugimoto 2018)\nIn this use case, I will illustrate how to make use of the OpenAIRE Research Graph, which links grants to publications and open access full-texts, to benchmark compliance with the open access mandate against other H2020 funding activities.\nOverview\nAs a start, I load a dataset, which was compiled following the above-described methods using the whole h2020_results.gz dump.\n\nThe parsed file is shared as  GitHub release asset\n\n\noaire_df <-\n  jsonlite::stream_in(file(\"data/h2020_parsed.json\"), verbose = FALSE) %>%\n  tibble::as_tibble()\n\n\n\nIt contains 92,218 grant-supported research outputs. Here, I will focus on the prevalence of open access across H2020 projects using metadata about the open access status of a publication and related project information stored in the list-column linked_projects.\n\n\npubs_projects <- oaire_df %>%\n  filter(type == \"publication\") %>%\n  select(id, type, best_access_right, linked_projects) %>%\n  # transform to a regular data frame with a row for each project\n  unnest(linked_projects) \n\n\n\nThe dataset contains 84,781 literature publications from 9,008 H2020 projects. What H2020 funding activity published most?\n\n\nlibrary(cowplot)\nlibrary(scales)\npubs_projects %>%\n  filter(funding_level_0 == \"H2020\") %>% \n  mutate(funding_scheme = fct_infreq(funding_level_1)) %>%\n  group_by(funding_scheme) %>%\n  summarise(n = n_distinct(id)) %>%\n  mutate(funding_fct = fct_other(funding_scheme, keep = levels(funding_scheme)[1:10])) %>%\n  mutate(highlight = ifelse(funding_scheme %in% c(\"ERC\", \"RIA\"), \"yes\", \"no\")) %>%\n  ggplot(aes(reorder(funding_fct, n), n, fill = highlight)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"#B0B0B0D0\", \"#56B4E9D0\"),\n    name = NULL) +\n  scale_y_continuous(\n    labels = scales::number_format(big.mark = \",\"),\n    expand = expansion(mult = c(0, 0.05)),\n    breaks =  scales::extended_breaks()(0:25000)\n    ) +\n  labs(x = NULL, y = \"Publications\", caption = \"Data: OpenAIRE Research Graph\") +\n  theme_minimal_vgrid(font_family = \"Roboto\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 1: Publication Output of Horizon 2020 funding activities captured by the OpenAIRE Research Graph, released in December 2019.\n\n\n\nFigure 1 shows that most publications in the OpenAIRE Research Graph originate from the European Research Council (ERC), Research and Innovation Actions (RIA) and Marie Skłodowska-Curie Actions (MSCA). On average, 10 articles were published per project. However, the publication performance per H2020 funding activity varies considerably (SD = 33).\nThe European Commission mandates open access to publications. Let’s measure the compliance to this policy using the OpenAIRE Research Graph per project:\n\n\nlibrary(rmarkdown)\noa_monitor_ec <- pubs_projects %>%\n  filter(funding_level_0 == \"H2020\") %>%\n  mutate(funding_scheme = fct_infreq(funding_level_1)) %>%\n  group_by(funding_scheme,\n           project_code,\n           project_acronym,\n           best_access_right) %>%\n  summarise(oa_n = n_distinct(id)) %>% # per pub\n  mutate(oa_prop = oa_n / sum(oa_n)) %>%\n  filter(best_access_right == \"Open Access\") %>%\n  ungroup() %>%\n  mutate(all_pub = as.integer(oa_n / oa_prop)) \nrmarkdown::paged_table(oa_monitor_ec)\n\n\n\n\n\nIn the following, this aggregated data, oa_monitor_ec, will provide the basis to explore variations among and within H2020 funding programmes.\n\n\noa_monitor_ec %>%\n  # only projects with at least five publications\n  mutate(funding_fct = fct_other(funding_scheme, keep = levels(funding_scheme)[1:10])) %>%\n  filter(all_pub >= 5) %>%\n  ggplot(aes(fct_rev(funding_fct), oa_prop)) +\n  geom_boxplot() +\n  geom_hline(aes(\n    yintercept = mean(oa_prop),\n    color = paste0(\"Mean=\", as.character(round(\n      mean(oa_prop) * 100, 0\n    )), \"%\")\n  ),\n  linetype = \"dashed\",\n  size = 1) +\n  geom_hline(aes(\n    yintercept = median(oa_prop),\n    color = paste0(\"Median=\", as.character(round(\n      median(oa_prop) * 100, 0\n    )), \"%\")\n  ),\n  linetype = \"dashed\",\n  size = 1) +\n  scale_color_manual(\"H2020 OA Compliance\", values = c(\"orange\", \"darkred\")) +\n  coord_flip() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 5L),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(x = NULL,\n       y = \"Open Access Percentage\",\n       caption = \"Data: OpenAIRE Research Graph\") +\n  theme_minimal_vgrid(font_family = \"Roboto\") +\n  theme(legend.position = \"top\",\n        legend.justification = \"right\")\n\n\n\n\nFigure 2: Open Access Compliance Rates of Horizon 2020 projects relative to funding activities, visualised as box plot. Only projects with at least five publications are shown individually.\n\n\n\nAbout 77% of research publications under the H2020 open access mandate are openly available. Figure 2 highlights a generally high rate of compliance with the open access mandate, however, uptake levels vary the funding schemes. In particular, ERC grants and Marie Skłodowska-Curie activities show higher levels of compliance compared to the overall average.\nHow does the rate of compliance with the open access mandate of H2020-funded projects from the University of Göttingen benchmark against related projects?\nBecause of their large variations, I want to put the open access rates of H2020-funded projects in context when presenting the share for projects affiliated with the University of Göttingen. Again, the data analysis starts with loading the previously backed up file with decoded and parsed data, choosing project and access information from it.\n\n\noaire_df <- jsonlite::stream_in(file(\"data/h2020_parsed.json\"), verbose = FALSE) %>%\n  tibble::as_tibble()\n\npubs_projects <- oaire_df %>%\n  select(id, type, best_access_right, linked_projects) %>%\n  unnest(linked_projects) \npubs_projects\n\n\n#> # A tibble: 136,298 × 12\n#>    id        type   best_access_right to    project_title      funder \n#>    <chr>     <chr>  <chr>             <chr> <chr>              <chr>  \n#>  1 data-raw… publi… Open Access       proj… Planning and Eval… Europe…\n#>  2 data-raw… publi… Open Access       proj… Cortical algorith… Europe…\n#>  3 data-raw… publi… Open Access       proj… Human Brain Proje… Europe…\n#>  4 data-raw… publi… Restricted        proj… Implementation of… Europe…\n#>  5 data-raw… publi… Open Access       proj… The power of imag… Europe…\n#>  6 data-raw… publi… Open Access       proj… A psychological a… Wellco…\n#>  7 data-raw… publi… Open Access       proj… Effects of Nutrit… Europe…\n#>  8 data-raw… publi… Open Access       proj… Aggression subtyp… Europe…\n#>  9 data-raw… publi… Open Access       proj… Global trends in … Europe…\n#> 10 data-raw… publi… Open Access       proj… Mapping gravitati… Europe…\n#> # … with 136,288 more rows, and 6 more variables:\n#> #   funding_level_0 <chr>, funding_level_1 <chr>, project_code <chr>,\n#> #   project_acronym <chr>, contract_type <chr>, funding_level_2 <chr>\n\nNext, I want to identify H2020 projects with participation from the university. There are at least two ways to obtain links between projects and organisations: One is the OpenAIRE Research Graph. It provides project details from 29 funders in a separate dump, project.gz. Another option is to relate our dataset to open data provided by CORDIS, the European Commission’s research information portal. For convenience, I am going to follow the second option.\n\n\n# load local copy downloaded from the EC open data portal\ncordis_org <-\n  readr::read_delim(\n    \"data/cordis-h2020organizations.csv\",\n    delim = \";\",\n    locale = locale(decimal_mark = \",\")\n  ) %>%\n  # data cleaning\n  mutate_if(is.double, as.character) \n\n\n\nAfter loading the file, I am able to tag projects affiliated with the University of Göttingen.\n\n\nugoe_projects <- cordis_org %>%\n  filter(shortName %in% c(\"UGOE\", \"UMG-GOE\")) %>% \n  select(project_id = projectID, role, project_acronym = projectAcronym)\n\npubs_projects_ugoe <- pubs_projects %>%\n  mutate(ugoe_project = funding_level_0 == \"H2020\" & project_code %in% ugoe_projects$project_id)\n\n\n\nLet’s put it all together and benchmark the rates of compliance with the H2020 open access mandate using data from the OpenAIRE Research Graph. The package plotly(Sievert 2018) allows presenting the figure as an interactive chart.\n\n\n# funding programmes with Uni Göttingen participation\nugoe_funding_programme <- pubs_projects_ugoe %>% \n  filter(ugoe_project == TRUE) %>%\n  group_by(funding_level_1, project_code) %>% \n  # min 5 pubs\n  summarise(n = n_distinct(id)) %>%\n  filter(n >= 5) %>%\n  distinct(funding_level_1, project_code)\ngoe_oa <- oa_monitor_ec %>%\n  # min 5 pubs\n  filter(all_pub >=5) %>%\n  filter(funding_scheme %in% ugoe_funding_programme$funding_level_1) %>%\n  mutate(ugoe = project_code %in% ugoe_funding_programme$project_code) %>%\n  mutate(`H2020 project` = paste0(project_acronym, \" | OA share: \", round(oa_prop * 100, 0), \"%\"))\n# plot as interactive graph using plotly\nlibrary(plotly)\np <- ggplot(goe_oa, aes(funding_scheme, oa_prop)) +\n  geom_boxplot() +\n  geom_jitter(data = filter(goe_oa, ugoe == TRUE),\n               aes(label = `H2020 project`),\n             colour = \"#AF42AE\",\n             alpha = 0.9,\n             size = 3,\n             width = 0.25) +\n  geom_hline(aes(\n    yintercept = mean(oa_prop),\n    color = paste0(\"Mean=\", as.character(round(\n      mean(oa_prop) * 100, 0\n    )), \"%\")\n  ),\n  linetype = \"dashed\",\n  size = 1) +\n  geom_hline(aes(\n    yintercept = median(oa_prop),\n    color = paste0(\"Median=\", as.character(round(\n      median(oa_prop) * 100, 0\n    )), \"%\")\n  ),\n  linetype = \"dashed\",\n  size = 1) +\n  scale_color_manual(NULL, values = c(\"orange\", \"darkred\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 5L)) +\n  labs(x = NULL,\n       y = \"Open Access Percentage\",\n       caption = \"Data: OpenAIRE Research Graph\") +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(legend.position = \"top\",\n        legend.justification = \"right\")\nplotly::ggplotly(p, tooltip = c(\"label\"))\n\n\n\n\n\nFigure 3: Open Access Compliance Rates of Horizon 2020 projects affiliated with the University of Göttingen (purple dots) relative to the overall performance of the funding activity, visualised as a box plot. Only projects with at least five publications were considered. Data: OpenAIRE Research Graph(Manghi, Atzori, et al. 2019)\n\n\n\nFigure 3 shows that many H2020-projects with University of Göttingen participation have an uptake of open access to grant-supported publications that is above the average in the peer group. At the same time, some perform below expectation. Together, this provides a valuable insight into open access compliance at the university-level, especially for research support librarians who are in charge of helping grantees to make their work open access. They can, for instance, point grantees to OpenAIRE-compliant repositoires for self-archiving their works.\nDiscussion and conclusion\nUsing data from the OpenAIRE Research Graph dumps makes it possible to put the results of a specific data analysis into context. Open access compliance rates of H2020 projects vary. These variations should be considered when reporting compliance rates of specific projects under the same open access mandate.\nAlthough the OpenAIRE Research Graph is a large collection of scholarly data, it is likely that it still does not provide the whole picture. OpenAIRE mainly collects data from open sources. It is still unknown how the OpenAIRE Research Graph compares to well-established toll-access bibliometrics data sources like the Web of Science in terms of coverage and data quality.\nAs a member of the OpenAIRE consortium, improving the re-use of the OpenAIRE Research Graph dumps has become a SUB Göttingen working priority. In the scholarly communication analysts team, we want to support this with a number of data analyses and outreach activities. In doing so, we will add more helper functions to the openairegraph R package. It targets data analysts and researchers who wish to conduct their own analysis using the OpenAIRE Research Graph, but are wary of handling its large data dumps.\nIf you like to contribute, head on over to the packages’ source code repository and get started!\nAcknowledgments\nThis work was supported by the European Commission [OpenAIRE-Advance - OpenAIRE Advancing Open Scholarship (777541)].\n\n\n\nBengtsson, Henrik. 2020a. Future.apply: Apply Function to Elements in Parallel Using Futures. https://CRAN.R-project.org/package=future.apply.\n\n\n———. 2020b. Future: Unified Parallel and Distributed Processing in r for Everyone. https://CRAN.R-project.org/package=future.\n\n\nHester, Jim, Gábor Csárdi, Hadley Wickham, Winston Chang, Martin Morgan, and Dan Tenenbaum. 2019. Remotes: R Package Installation from Remote Repositories, Including ’GitHub’. https://CRAN.R-project.org/package=remotes.\n\n\nLarivière, Vincent, and Cassidy R. Sugimoto. 2018. “Do Authors Comply When Funders Enforce Open Access to Research?” Nature 562 (7728): 483–86. https://doi.org/10.1038/d41586-018-07101-w.\n\n\nManghi, Paolo, Claudio Atzori, Alessia Bardi, Jochen Schirrwagen, Harry Dimitropoulos, Sandro La Bruzzo, Ioannis Foufoulas, et al. 2019. “OpenAIRE Research Graph Dump.” Zenodo. https://doi.org/10.5281/zenodo.3516918.\n\n\nManghi, Paolo, Alessia Bardi, Claudio Atzori, Miriam Baglioni, Natalia Manola, Jochen Schirrwagen, and Pedro Principe. 2019. “The OpenAIRE Research Graph Data Model.” Zenodo. https://doi.org/10.5281/zenodo.2643199.\n\n\nOoms, Jeroen. 2014. “The Jsonlite Package: A Practical and Consistent Mapping Between JSON Data and r Objects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\nSievert, Carson. 2018. Plotly for r. https://plotly-r.com.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2019. Xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\n\n\n\n\n",
    "preview": "posts/oaire_graph_2020/distill-preview.png",
    "last_modified": "2023-11-01T09:08:02+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/unpaywall_python/",
    "title": "Exploring the Open Access Evidence Base in Unpaywall with Python",
    "description": "Open Access evidence sources constantly change. In this blog post, I present a Python based approach for analysing the most recent snapshots from the open access discovery service Unpaywall. Results shows a growth in open access content, partly because of newly introduced evidence sources like Semantic Scholar.",
    "author": [
      {
        "name": "Nick Haupka",
        "url": {}
      }
    ],
    "date": "2020-03-30",
    "categories": [],
    "contents": "\n\n\n\nUnpaywall makes millions of scholarly fulltexts from a variety of different repositories and services like PubMed Central and the Directory of Open Access Journals (DOAJ) discoverable. Although Unpaywall offers a REST API to query the Unpaywall database, sometimes it is more convenient and efficient to use database snapshots, which the Unpaywall team usually released twice a year.\nIn this blog post, I discuss the results from the former blog post “Open Access Evidence in Unpaywall” by comparing different database snapshots obtained from Unpaywall. Since the article´s publication on May 7th, 2019, Unpaywall has updated its data snapshots. Therefore, I want to explore potential changes in the Unpaywall data over time. For this post I used the most recent Unpaywall data dump from February 2020. Instead of using R, I present how to analyse the Unpaywall dump with Python.\nThe SUB scholarly communication analytics team regularly stores the Unpaywall snapshot on Google BigQuery. To query it in Python, I use the google-cloud-bigquery package. By default, this package does not ship with the data analysis tool pandas. However, I recommend to use this package along with pandas for a better experience, because the queried data can be represented as a pandas DataFrame.\n\n\nfrom google.cloud import bigquery\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nimport upsetplot\n\nThe setup for the Google BigQuery Python client is pretty straight forward. First, I import bigquery from the google.cloud package and create a client object by passing the project name as a parameter. If you did not set your credentials for the Google Cloud service yet, Google asks you to verify your client. This can be done by exporting an API-Key provided by Google into the working environment or by installing the official Google Cloud SDK. A detailed description on how to authenticate the client can be found here. Notice that the project-database has restricted access.\n\nGoogle BigQuery is a paid service (with a large free contingent). If you would like to work with the access-restricted instance, please contact us.\n\n\nclient = bigquery.Client(project='api-project-764811344545')\n\nLike in the aforementioned blog post, also the recent dataset is stored in two tables, containing records between 2008 and February 2020. For the analysis, I will restrict the dataset being used here to publication years 2008 until 2019, the time period covered in the previous post plus the recent year 2019. For reusability, I define two variables which are holding the tables’ names to call them in SQL queries. I will also use the improved string formatting syntax which is a new feature since Python 3.6.\n\n\n# database snapshots\nupw_08_12 = '`oadoi_full.mongo_upwFeb20_08_12`'\nupw_13_19 = '`oadoi_full.mongo_upwFeb20_13_20`'\n\nI can query the project-database by using the query method on my created client object. In this example I requested ten journal articles that were published in 2019. I can pass the SQL query simply as a string into the query method. Next I can chain the to_dataframe method on my query to get a pandas DataFrame.\n\n\nclient.query(f\"\"\"\n            SELECT * \n            FROM {upw_13_19} \n            WHERE year=2019 AND genre=\"journal-article\" \n            LIMIT 10\n            \"\"\").to_dataframe()\n#>                         doi  ...  year\n#> 0       10.3906/kim-1808-51  ...  2019\n#> 1  10.3389/fimmu.2019.01664  ...  2019\n#> 2  10.3847/1538-4357/ab464c  ...  2019\n#> 3            10.5334/pb.443  ...  2019\n#> 4         10.1063/1.5088767  ...  2019\n#> 5  10.3389/fimmu.2019.02009  ...  2019\n#> 6         10.1530/ey.16.7.4  ...  2019\n#> 7       10.7554/elife.44364  ...  2019\n#> 8        10.1242/bio.041467  ...  2019\n#> 9      10.1051/cocv/2019042  ...  2019\n#> \n#> [10 rows x 14 columns]\n\nParatexts in Unpaywall (is_paratext)\nAs can be seen from the outcome of the previous query, Unpaywall has recently introduced a new attribute is_paratext in the updated February 2020 database snapshot. It contains a boolean value which indicates whether a DOI is linked to a paratext or not. Because additional content to a journal article can also get classified as a journal article by the publisher, such as table of contents, it can mislead the analysis of scholarly articles in Unpaywall. Therefore I will ignore DOI´s that are related to paratexts in the following. For more information about the is_paratext field visit this page.\nBut firstly, I will analyze the share of paratexts in the current data dump. For that I request the total number of DOI´s and the number of DOI´s that are related to paratexts in both tables.\n\n\nparatext_08_12 = client.query(f\"\"\"\n                        SELECT \n                            COUNT(nullif(is_paratext = true, false)) \n                                AS number_of_paratexts,\n                            COUNT(doi) \n                                AS number_of_all_dois,\n                        FROM (\n                            SELECT DISTINCT(doi), is_paratext, genre \n                            FROM {upw_08_12} \n                            WHERE genre=\"journal-article\"\n                            )\n                        \"\"\").to_dataframe()\n\nparatext_13_19 = client.query(f\"\"\"\n                        SELECT \n                            COUNT(nullif(is_paratext = true, false)) \n                                AS number_of_paratexts,\n                            COUNT(doi) \n                                AS number_of_all_dois,\n                        FROM (\n                            SELECT DISTINCT(doi), is_paratext, year, genre \n                            FROM {upw_13_19} \n                            WHERE year<2020 AND genre=\"journal-article\"\n                            )\n                        \"\"\").to_dataframe()\n\n\n\nparatext_df = pd.concat([paratext_08_12, paratext_13_19])\nparatext_df = paratext_df.sum().to_frame().transpose()\nparatext_df = paratext_df.eval('prop = (number_of_paratexts/number_of_all_dois) * 100')\nparatext_df.prop = paratext_df.prop.apply(lambda x: '{0:.2f}'.format(x))\nparatext_df.columns = ['Number of DOI´s identified as paratext',\n                       'Number of all DOI´s',\n                       'Proportion of all Paratexts in %']\n\nparatext_df\n\n\nTable 1: Number of paratexts in Unpaywall.\nNumber of DOI´s identified as paratext\nNumber of all DOI´s\nProportion of all Paratexts in %\n321385\n35875237\n0.90\n\nYet, the proportion of paratexts in the dataset amounts to approximately 1%. In total, 321385 paratexts are linked to a specific DOI. You can also see that 35,875,237 distinct DOIs of genre journal-article are included in the excerpt of the current database snapshot, containing publications from 2008 onwards.\nOpen Access availability (is_oa)\nTo contrast the previous results from the blog post with the recent results, I begin with a comparison of the total number of articles between the two datasets from February 2019 and February 2020. More importantly, I will investigate the open access share between these two.\nBefore I can compute the open access proportion in the dataset, I must query the database and count the number of distinct DOI´s by year and open access status. Since I have two tables, I need to concatenate the dataframes in the next step. I also have to convert the column which contains the year from string to datetime. This allows me to handle date information much better when visualizing data. After I have calculated the proportion of open access by year, I will now sort the values by time.\n\n\noa_08_12 = client.query(f\"\"\"\n                        SELECT year, is_oa, COUNT(DISTINCT(doi)) AS n \n                        FROM {upw_08_12} \n                        WHERE genre=\"journal-article\" AND is_paratext=False\n                        GROUP BY year, is_oa\n                        \"\"\").to_dataframe()\n\noa_13_19 = client.query(f\"\"\"\n                        SELECT year, is_oa, COUNT(DISTINCT(doi)) AS n \n                        FROM {upw_13_19} \n                        WHERE year<2020 AND genre=\"journal-article\" AND is_paratext=False\n                        GROUP BY year, is_oa\n                        \"\"\").to_dataframe()\n\n\n\ndf = pd.concat([oa_08_12, oa_13_19])\ndf.year = pd.to_datetime(df.year.apply(lambda x: str(x) + \"-01-01\"))\ndf['prop'] = df.groupby(['year'])['n'].transform(lambda x: x / x.sum())\ndf = df.sort_values(by=['year', 'is_oa'], ascending=[True, False]).reset_index(drop=True)\ndf.head()\n#>         year  is_oa        n      prop\n#> 0 2008-01-01   True   616069  0.299712\n#> 1 2008-01-01  False  1439469  0.700288\n#> 2 2009-01-01   True   697145  0.310508\n#> 3 2009-01-01  False  1548033  0.689492\n#> 4 2010-01-01   True   789565  0.317344\n\nFor visualization, I will use matplotlib. Matplotlib is a commonly used plotting library for Python that is influenced by Matlab and enables a wide range of different plot figures. By using matplotlib I can also take advantage of pandas plotting methods which uses the library as the standard backend.\nIn the first plot I visualize the open access share to journal articles over time. Whereas the plot is not interactive like in the stated blog post, if needed, this could be achieved by using the same plotting library named Plotly which can be found here. Plotly is available for R, Javascript and Python.\n\n\nplt.style.use('seaborn-whitegrid')\n\nax = df.groupby(['year', 'is_oa'], sort=False) \\\n            ['n'].sum().unstack() \\\n            .plot.area(figsize=(10,4),\n                       alpha=0.8,\n                       xlim=(37.5, 49.5),\n                       linewidth=0,\n                       color=['#56B4E9', '#b3b3b3a0'])\n\nax.grid(False, which='both', axis='x')\nhandles, labels = ax.get_legend_handles_labels()\nplt.box(False)\n\nplt.title('Open Access to Journal Articles', \n          fontdict={'fontsize': 12, 'fontweight': 600}, pad=10)\nplt.xlabel('Year published', labelpad=10, fontdict={'fontsize': 11, 'fontweight': 500})\nplt.ylabel('Journal Articles', labelpad=10, fontdict={'fontsize': 11, 'fontweight': 500})\n\nplt.legend(handles, ['TRUE', 'FALSE'], \n           title='Is OA?', \n           fontsize='medium', \n           bbox_to_anchor=(1.15, 1.02), \n           labelspacing=1.2)\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\nFigure 1: Open access to journal articles according to Unpaywall.\n\n\n\nAs expected, the total number of journal articles has increased, compared to the previous results from the blog post. In fact, the number of articles with a distinct DOI included in the Unpaywall data dump from February 2020 has increased by 14% (regardless of paratexts that were not specified in previous database snapshots) in comparison to the findings from the original blog post. In addition, the share of open access articles has increased by 7%. Overall, 15,487,801 articles that were published between 2008 and 2019 are freely available by February 2020.\nUnpaywall Open Access Hosting Types (host_type)\nNext, I investigate the differences between the distribution of the host types specified in the data dumps. The host type describes the type of location that serves open access full-texts and accepts two values: publisher and repository. As mentioned in the blog post, the host type variable is determined by Unpaywall’s algorithm.\n\n\nHOST_TYPE_08_12_QUERY = f\"\"\"\n                        SELECT year, host_type, journal_is_in_doaj, \n                            COUNT(DISTINCT(doi)) AS number_of_articles \n                        FROM {upw_08_12}, UNNEST (oa_locations) \n                        WHERE genre=\"journal-article\" AND is_best=true AND is_paratext=False\n                        GROUP BY year, host_type, journal_is_in_doaj\n                        \"\"\"\n\nHOST_TYPE_13_19_QUERY = f\"\"\"\n                        SELECT year, host_type, journal_is_in_doaj, \n                            COUNT(DISTINCT(doi)) AS number_of_articles \n                        FROM {upw_13_19}, UNNEST (oa_locations) \n                        WHERE genre=\"journal-article\" AND year<2020 AND is_best=true \n                            AND is_paratext=False\n                        GROUP BY year, host_type, journal_is_in_doaj\n                        \"\"\"\n\nLike in the original blog post, I create a host column with the pandas provided loc method to highlight freely available full-texts provided by DOAJ-indexed journals in addition to the regular host types. Because the DOAJ has comprehensive standards for journal inclusion, it might be interesting to see whether an increase of publishing in potentially less strict open access journals can be observed or not.\n\n\nhost_type_08_12_query_df = client.query(HOST_TYPE_08_12_QUERY).to_dataframe()\nhost_type_13_19_query_df = client.query(HOST_TYPE_13_19_QUERY).to_dataframe()\n\nhost_type_df = pd.concat([host_type_08_12_query_df, host_type_13_19_query_df])\nhost_type_df.year = pd.to_datetime(host_type_df.year.apply(lambda x: str(x) + \"-01-01\"))\nhost_type_df = host_type_df.sort_values(by=['year']).reset_index(drop=True)\n\nhost_type_df.loc[host_type_df['host_type'] == 'publisher', 'host'] = 'Other Journals'\nhost_type_df.loc[host_type_df['host_type'] == 'repository', 'host'] = 'Repositories only'\nhost_type_df.loc[host_type_df['journal_is_in_doaj'] == True, 'host'] = 'DOAJ-listed Journal'\n\nhost_type_df.head()\n#>         year   host_type  ...  number_of_articles                 host\n#> 0 2008-01-01   publisher  ...              309659       Other Journals\n#> 1 2008-01-01  repository  ...              213820    Repositories only\n#> 2 2008-01-01   publisher  ...               92590  DOAJ-listed Journal\n#> 3 2009-01-01   publisher  ...              349556       Other Journals\n#> 4 2009-01-01  repository  ...              232924    Repositories only\n#> \n#> [5 rows x 5 columns]\n\nAgain, I visualize the data with matplotlib. In contrast to the R package ggplot2, I found it a bit more inconvenient to prepare and plot data with matplotlib in Python. This starts by iterating over the host types to generate a subplot for each. Although I can make use of pandas plotting methods to display complicated graphics from DataFrames, it requires much effort to get publication quality figures.\n\n\nall_articles = host_type_df.groupby(['year'])['number_of_articles'].sum() \\\n                            .reset_index(name='number_of_articles')\n\nx = all_articles['year'].dt.year\ny_total = all_articles.number_of_articles\n\nplt.style.use('seaborn-whitegrid')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,3.8))\n\nfig.suptitle('Open Access to Journal Articles by Unpaywall host', \n             fontsize=14, \n             fontweight=600, \n             y=1.10)\n\nfor i, host in enumerate(host_type_df.host.unique(), 1):\n    \n    ax = plt.subplot(1,3,i)\n    \n    y_stacked = host_type_df[host_type_df.host==host] \\\n                    .groupby(['year'])['number_of_articles'].sum() \\\n                    .reset_index(name='number_of_articles').number_of_articles\n    \n    ax.bar(x, y_total, color='#b3b3b3a0', label='All OA Articles')\n    ax.bar(x, y_stacked, color='#56B4E9', label='by Host')\n    \n    ax.set_title(host, fontdict={'fontsize': 12, 'fontweight': 500})\n\n    ax.set_frame_on(False)\n    ax.grid(False, which='both', axis='x')\n    ax.set(xlabel='', ylabel='')\n\n    if i > 1:\n        ax.set_yticklabels([], visible=False)\n        \n# common xlabel   \nfig.text(0.45, -0.03, 'Year', ha='center', \n         fontdict={'fontsize': 12, 'fontweight': 500})\n\n# common ylabel\nfig.text(-0.02, 0.5, 'OA Articles (Total)', va='center', rotation='vertical', \n         fontdict={'fontsize': 12, 'fontweight': 500})\n\nplt.legend(fontsize='medium', bbox_to_anchor=(1.05, 0.9), labelspacing=1.2)\n\nfig.tight_layout()\n\nplt.show()\n\n\n\n\nFigure 2: Open access to journal articles by open access hosting location.\n\n\n\nThe figure highlights that the proportion of open access articles provided by journals which are not listed in DOAJ rapidly increased in the year 2018 in comparison to the previous results. Nonetheless, the overall share of articles obtained from journals that are not listed in DOAJ decreased from 56% to 51%. Consequently, the share of open access articles provided by DOAJ-listed journals increased.\nUnpaywall Open Access Evidence Types (evidence)\nIn the following, I explore varieties between the evidence types of the two data dumps. The evidence type is a variable determined by Unpaywall, which expresses the location at which an article was found and how Unpaywall was able to identify the open access status of an article.\n\n\nEVIDENCE_08_12_QUERY = f\"\"\"\n                        SELECT evidence, year, is_best, \n                            COUNT(DISTINCT(doi)) AS number_of_articles \n                        FROM {upw_08_12}, UNNEST (oa_locations) \n                        WHERE genre=\"journal-article\" AND is_paratext=False\n                        GROUP BY evidence, year, is_best\n                        \"\"\"\n\nEVIDENCE_13_19_QUERY = f\"\"\"\n                        SELECT evidence, year, is_best, \n                            COUNT(DISTINCT(doi)) AS number_of_articles \n                        FROM {upw_13_19}, UNNEST (oa_locations) \n                        WHERE genre=\"journal-article\" AND year < 2020 AND is_paratext=False\n                        GROUP BY evidence, year, is_best\n                        \"\"\"\n\n\n\nevidence_08_12 = client.query(EVIDENCE_08_12_QUERY).to_dataframe()\nevidence_13_19 = client.query(EVIDENCE_13_19_QUERY).to_dataframe()\n\n\n\nevidence_df = pd.concat([evidence_08_12, evidence_13_19])\nevidence_df.year = pd.to_datetime(evidence_df.year.apply(lambda x: str(x) + \"-01-01\"))\n\nevidence_df.head()\n#>                                   evidence  ... number_of_articles\n#> 0  oa repository (semantic scholar lookup)  ...              54888\n#> 1  oa repository (semantic scholar lookup)  ...              74108\n#> 2         oa repository (via pmcid lookup)  ...               5397\n#> 3  oa repository (semantic scholar lookup)  ...              64668\n#> 4  oa repository (semantic scholar lookup)  ...              56898\n#> \n#> [5 rows x 4 columns]\n\nFor each evidence type I calculate the total number of articles and the related proportion, as well as the cumulative proportion with respect to the number of all articles. Then I display the results in form of a table. To specify the precision of the resulting floating point numbers I again make use of the Python built-in string formatting function. Here, I specify by 2 digits of accuracy.\n\n\narticles_per_type_df = evidence_df.groupby(['evidence']).number_of_articles \\\n                        .sum() \\\n                        .to_frame() \\\n                        .reset_index() \\\n                        .sort_values(by=['number_of_articles'], ascending=False) \\\n                        .reset_index(drop=True)\n    \narticles_per_type_df['prop'] = articles_per_type_df['number_of_articles'] \\\n                                .transform(lambda x: \n                                           x / articles_per_type_df['number_of_articles'] \\\n                                           .sum() * 100)\n\narticles_per_type_df['cumul'] = articles_per_type_df.prop.cumsum()\n\n\n\narticles_per_type_table = articles_per_type_df.copy()\narticles_per_type_table.prop = articles_per_type_table.prop \\\n                                    .apply(lambda x: '{0:.2f}'.format(x))\n\narticles_per_type_table.cumul = articles_per_type_table.cumul \\\n                                    .apply(lambda x: '{0:.2f}'.format(x))\n\narticles_per_type_table.columns = ['Evidence Types', \n                                   'Number of Articles', \n                                   'Proportion of all Articles in %', \n                                   'Cumulative Proportion in %']\n                                   \narticles_per_type_table\n\n\nTable 2: Number of articles per evidence type.\n\nEvidence Types\nNumber of Articles\nProportion of all Articles in %\nCumulative Proportion in %\n0\nopen (via free pdf)\n5342059\n16.33\n16.33\n1\noa repository (via OAI-PMH doi match)\n5069032\n15.50\n31.83\n2\nopen (via page says license)\n4948332\n15.13\n46.96\n3\noa repository (semantic scholar lookup)\n4621741\n14.13\n61.09\n4\noa journal (via doaj)\n4320534\n13.21\n74.30\n5\noa repository (via pmcid lookup)\n3704354\n11.33\n85.63\n6\noa repository (via OAI-PMH title and first author match)\n1950563\n5.96\n91.59\n7\noa journal (via observed oa rate)\n1508233\n4.61\n96.20\n8\nopen (via crossref license)\n614330\n1.88\n98.08\n9\nopen (via free article)\n265035\n0.81\n98.89\n10\nopen (via page says Open Access)\n95838\n0.29\n99.18\n11\noa journal (via publisher name)\n77084\n0.24\n99.42\n12\nopen (via crossref license, author manuscript)\n75537\n0.23\n99.65\n13\noa repository (via OAI-PMH title match)\n67689\n0.21\n99.86\n14\noa repository (via OAI-PMH title and last author match)\n46905\n0.14\n100.00\n15\nmanual\n29\n0.00\n100.00\n16\nhybrid (via page says license)\n1\n0.00\n100.00\n\nInterestingly, the evidence type oa repository (semantic scholar lookup) ranked fourth was not included in the database snapshot used in the May 2019 analysis. Further, the same phenomenon as in the previous work can be observed: the least frequent eight evidence types summarized only make up 1.9% of all articles in total. In the following, I will collate these evidence types in the category other.\n\n\nlist_of_small_evidence_types = articles_per_type_df \\\n                                .loc[articles_per_type_df['prop'] < 1] \\\n                                .evidence.tolist()\n\narticles_per_type_grouped_df = articles_per_type_df.copy()\n\narticles_per_type_grouped_df.evidence = articles_per_type_grouped_df \\\n                                .evidence.replace(list_of_small_evidence_types, 'other')\n\narticles_per_type_grouped_df = articles_per_type_grouped_df \\\n                                .groupby(['evidence']) \\\n                                .number_of_articles.sum() \\\n                                .to_frame().reset_index() \\\n                                .sort_values(by=['number_of_articles'], ascending=False) \\\n                                .reset_index(drop=True)\n\narticles_per_type_grouped_df['prop'] = articles_per_type_grouped_df['number_of_articles'] \\\n                        .transform(lambda x: \n                        x / articles_per_type_grouped_df['number_of_articles'].sum() * 100)\n\narticles_per_type_grouped_df['cumul'] = articles_per_type_grouped_df.prop.cumsum()\n\n\n\nevidence_grouped_df = evidence_df.copy()\n\nevidence_grouped_df.evidence = evidence_grouped_df \\\n                                .evidence.replace(list_of_small_evidence_types, 'other')\n\nevidence_grouped_df = evidence_grouped_df.groupby(['evidence', 'is_best', 'year']) \\\n                                .number_of_articles.sum() \\\n                                .to_frame().reset_index() \\\n                                .sort_values(by=['number_of_articles'], ascending=False) \\\n                                .reset_index(drop=True)\n\nTo illustrate the best open access locations according to Unpaywall, I will visualize the quantity of articles that were obtained from data sources with the is_best attribute given by Unpaywall in contrast to the total number of articles found in each evidence type.\nThis time I will use the seaborn package for generating bar plots. Seaborn is a visualization library which is build on top of matplotlib. It is well designed to work with pandas DataFrames and it also enables smoother plots. Also, I can continue to use matplotlib methods.\n\n\nevidence_grouped_plot_df = evidence_grouped_df[evidence_grouped_df.is_best == True] \\\n                                .groupby(['evidence'])['number_of_articles'] \\\n                                .sum().to_frame() \\\n                                .reset_index() \\\n                                .rename(columns={'number_of_articles': 'is_best_sum'})\n\nevidence_grouped_plot_df = pd.merge(articles_per_type_grouped_df, \n                                    evidence_grouped_plot_df,\n                                    how='left',\n                                    on='evidence')\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(7,5))\nplt.box(False)\n\nax1 = sns.barplot('number_of_articles',\n                  'evidence',\n                  data=evidence_grouped_plot_df,\n                  label='FALSE',\n                  color='#b3b3b3a0',\n                  alpha=0.6, \n                  saturation=1)\n\nax2 = sns.barplot('is_best_sum', \n                  'evidence',\n                  data=evidence_grouped_plot_df,\n                  label='TRUE',\n                  color='#56B4E9',\n                  alpha=1, \n                  saturation=1)\n\nplt.title('Number of Open Access Articles per Unpaywall Evidence Type', \n          fontdict={'fontsize': 12, 'fontweight': 600}, pad=20, x=0.3)\nplt.xlabel('Number of Open Access Articles', labelpad=10, \n           fontdict={'fontsize': 11, 'fontweight': 500})\n\nplt.ylabel('Evidence Type', labelpad=10, \n           fontdict={'fontsize': 11, 'fontweight': 500})\n\nplt.legend(title='Is best?', fontsize='medium', bbox_to_anchor=(1.2, 1.05), labelspacing=1.2)\n\n\nplt.show()\n\n\n\n\nFigure 3: Number of articles per evidence type.\n\n\n\nThe figure provides two types of evidence that were not specifically mentioned in the previous work: oa repository (semantic scholar lookup) and oa journal (via observed rate). Apparently, these evidence types are responsible for the overall increase in articles between the two data dumps from February 2019 and February 2020. Furthermore an increase of articles in repositories can be observed. Indeed, the evidence type with the second most associated articles is referring to repository locations. However, Unpaywall still prioritises publisher hosted content over repository depositions.\nTo distinguish the classification as best open access location of data sources by Unpaywall over time, I again make use of matplotlib. Although the FacetGrid function provided by seaborn would be very useful for this task it unfortunately doesn´t support stacked plots as mentioned in this issue on GitHub.\n\n\nx = evidence_grouped_df.sort_values(by=['year'], ascending=True) \\\n                        .reset_index(drop=False) \\\n                        ['year'].dt.year.unique()\n\nplt.style.use('seaborn-whitegrid')\n\nfig, axes = plt.subplots(nrows=5, \n                           ncols=2, \n                           sharex=True, \n                           sharey=True, \n                           figsize=(12,12))\n\n# get a one-dimensional array\naxes = axes.reshape(-1)\n\nfig.suptitle('Unpaywall Open Access Evidence Categories per Year', \n             fontsize=15, \n             fontweight=600, \n             x=0.53,\n             y=1.04)\n\nfor i, ax in enumerate(axes, 1):\n    \n    ax = plt.subplot(5,2,i, sharey=axes[0], sharex=axes[0])    \n    \n    if i % 2 == 0:\n        plt.setp(ax.get_yticklabels(), visible=False)\n    \n    evidence = evidence_grouped_df.evidence.unique()[i-1]\n\n    y_total = evidence_grouped_df[evidence_grouped_df.evidence == evidence] \\\n                            .groupby(['year']) \\\n                            .number_of_articles.sum() \\\n                            .to_frame().reset_index() \\\n                            .number_of_articles\n\n    y_stacked = evidence_grouped_df[evidence_grouped_df.evidence == evidence] \\\n                            .loc[evidence_grouped_df.is_best == True] \\\n                            .groupby(['year']) \\\n                            .number_of_articles.sum() \\\n                            .to_frame().reset_index() \\\n                            .number_of_articles\n\n    ax = plt.bar(x, y_total, color='#b3b3b3a0')\n    ax = plt.bar(x, y_stacked, color='#56B4E9')\n    plt.title(evidence, fontdict={'fontsize': 12, 'fontweight': 500}, pad=0.2)\n    \n    plt.grid(False, which='both', axis='x')\n    plt.box(False)\n\n# common xlabel   \nfig.text(0.52, -0.03, 'Publication Year', ha='center', \n         fontdict={'fontsize': 13, 'fontweight': 500})\n\n# common ylabel\nfig.text(-0.04, 0.5, 'Number of Open Access Articles', va='center', rotation='vertical', \n         fontdict={'fontsize': 13, 'fontweight': 500})\n\nis_best_patch = mpatches.Patch(color='#56B4E9', label='TRUE')\nis_not_best_patch = mpatches.Patch(color='#b3b3b3a0', label='FALSE')\nfig.legend(handles=[is_best_patch, is_not_best_patch], title='Is best?', \n           fontsize='large', title_fontsize='x-large', \n           bbox_to_anchor=(1.15, 0.95), labelspacing=1.2)\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\nFigure 4: Development of the number of articles per evidence type over time.\n\n\n\nOverlap of Open Access Provision and Evidence Types\nOwing to possible multiple associations between an article and evidence types in Unpaywall (there might be multiple free access locations for the same article), I investigate the intersection between host types in the next step. Again, I compare the findings with the previous results.\nOverlap between Host Types\n\n\nHOST_TYPE_INTERSECT_08_12_QUERY = f\"\"\"\n                                    SELECT year, host_type_count, \n                                        COUNT(DISTINCT(doi)) AS number_of_articles \n                                    FROM \n                                        (SELECT doi, year,\n                                            STRING_AGG(DISTINCT(host_type) \n                                        ORDER BY host_type) AS host_type_count \n                                        FROM {upw_08_12}, UNNEST (oa_locations)\n                                        WHERE genre=\"journal-article\" AND is_paratext=False\n                                        GROUP BY doi, year) \n                                    GROUP BY host_type_count, year \n                                    ORDER BY number_of_articles DESC\n                                    \"\"\"\n\nHOST_TYPE_INTERSECT_13_19_QUERY = f\"\"\"\n                                    SELECT year, host_type_count, \n                                        COUNT(DISTINCT(doi)) AS number_of_articles \n                                    FROM \n                                        (SELECT doi, year, \n                                        STRING_AGG(DISTINCT(host_type) \n                                        ORDER BY host_type) AS host_type_count\n                                        FROM {upw_13_19}, UNNEST (oa_locations) \n                                        WHERE genre=\"journal-article\" AND year < 2020 \n                                            AND is_paratext=False\n                                        GROUP BY doi, year) \n                                    GROUP BY host_type_count, year \n                                    ORDER BY number_of_articles DESC\n                                    \"\"\"\n\nI start by querying the project-database and setting up the dataframe. Then I will merge the outcome with the articles_total_by_year dataframe which I have created before. This enables me to investigate the relative share of articles provided by different host types to the total number of articles across host types.\n\n\nhost_type_08_12_intersect_df = client.query(HOST_TYPE_INTERSECT_08_12_QUERY).to_dataframe()\nhost_type_13_19_intersect_df = client.query(HOST_TYPE_INTERSECT_13_19_QUERY).to_dataframe()\nhost_type_intersect_df = pd.concat([host_type_08_12_intersect_df, \n                                    host_type_13_19_intersect_df])\n\nhost_type_intersect_df.year = pd.to_datetime(host_type_intersect_df.year \\\n                                             .apply(lambda x: str(x) + \"-01-01\"))\n\nhost_type_intersect_df \\\n    .loc[host_type_intersect_df['host_type_count'] == 'publisher', 'host'] = 'Publisher only'\nhost_type_intersect_df \\\n    .loc[host_type_intersect_df['host_type_count'] == 'publisher,repository', \n         'host'] = 'Publisher & Repository'\nhost_type_intersect_df \\\n    .loc[host_type_intersect_df['host_type_count'] == 'repository', \n         'host'] = 'Repositories only'\n\n\n\narticles_total_by_year_df = df.groupby(['year']).n.sum().to_frame().reset_index()\narticles_total_by_year_df.columns = ['year', 'all_articles']\n\nhost_type_intersect_df = pd.merge(articles_total_by_year_df, \n                                  host_type_intersect_df, on='year', how='right')\n\nhost_type_intersect_df = host_type_intersect_df.groupby(['year', 'host']) \\\n                                    .sum().eval('prop = number_of_articles/all_articles') \\\n                                    .reset_index()\n\nhost_type_intersect_df.head()\n#>         year                    host  ...  number_of_articles      prop\n#> 0 2008-01-01  Publisher & Repository  ...              187109  0.091027\n#> 1 2008-01-01          Publisher only  ...              215140  0.104664\n#> 2 2008-01-01       Repositories only  ...              213820  0.104021\n#> 3 2009-01-01  Publisher & Repository  ...              221294  0.098564\n#> 4 2009-01-01          Publisher only  ...              242927  0.108199\n#> \n#> [5 rows x 5 columns]\n\n\n\nhost_type_all = host_type_intersect_df.copy()\nhost_type_all['prop'] = host_type_intersect_df.groupby(['year'])['prop'] \\\n                    .transform(lambda x: x.sum())\n\nAgain, I will use seaborn to display the results.\n\n\n# convert year column to work properly with seaborn\nx = host_type_intersect_df['year'].dt.year\n\nplt.style.use('seaborn-whitegrid')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(11, 3.5))\n\nfig.suptitle('Overlap between Open Access Host Types in Unpaywall', \n             fontsize=14, \n             fontweight=600,\n             x=0.48,\n             y=1.10)\n    \nfor i, host in enumerate(host_type_intersect_df.host.unique(), 1):\n    ax = plt.subplot(1,3,i)\n    y_stacked = host_type_intersect_df[host_type_intersect_df.host == host].prop\n    sns.barplot(x, \n                'prop', \n                data=host_type_all,\n                color='#b3b3b3a0', \n                label='All OA Articles',\n                alpha=0.6, \n                saturation=1, \n                ci=None)\n    \n    sns.barplot(x, \n                y_stacked,\n                color='#56B4E9', \n                label='by Host',\n                alpha=1, \n                saturation=1, \n                ci=None)\n                \n    ax.set_title(host, fontdict={'fontsize': 12, 'fontweight': 500})\n    \n    ax.set_frame_on(False)\n    ax.grid(False, which='both', axis='x')\n    ax.set(xlabel='', ylabel='')\n    \n    # for readability I hide every second tick on the x axis\n    for label in ax.get_xticklabels()[1::2]:\n        label.set_visible(False)\n        \n    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n    \n    if i > 1:\n        ax.set_yticklabels([], visible=False)\n        \n# common xlabel   \nfig.text(0.45, -0.03, 'Year', ha='center', \n         fontdict={'fontsize': 12, 'fontweight': 500})\n\n# common ylabel\nfig.text(-0.02, 0.5, 'OA Share', va='center', rotation='vertical', \n         fontdict={'fontsize': 12, 'fontweight': 500})\n    \nplt.legend(bbox_to_anchor=(1.05, 0.9), labelspacing=1.2)\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\nFigure 5: Open access to journal articles by open access hosting location.\n\n\n\nThe figure shows that, in terms of percentage, fewer articles were found on publisher websites than in the previous blog post. Indeed, 79% of all open access full-texts are available through publisher websites which is a total of 12,246,005 articles. Relatively speaking, this is a decrease by 3% compared to the previous results. Also, the proportion of articles that are not archived in a repository has declined from 56% to 40%. Consequently, the proportion of articles that are available from both the publisher websites, as well as those that are archived in a repository have increased from 26% to 39%.\nOverlaps between Evidence types\nSo far, I investigated the overlap of host types in Unpaywall in this section. Next, I am going to analyze evidence types. Various articles are associated with multiple evidence types in Unpaywall as mentioned at the beginning of this section, so I examine the intersection between evidence types in the next step.\n\n\nEVIDENCE_SINGLE_CAT_08_12_QUERY = f\"\"\"\n                                    SELECT ev_cat, COUNT(DISTINCT(doi)) AS number_of_articles\n                                    FROM \n                                        (SELECT doi, STRING_AGG(DISTINCT(evidence), \"&\" \n                                        ORDER BY evidence) AS ev_cat\n                                        FROM {upw_08_12}, UNNEST (oa_locations)\n                                        WHERE genre=\"journal-article\" AND is_paratext=False\n                                        GROUP BY doi\n                                        )\n                                    GROUP BY ev_cat\n                                   \"\"\"\n\nEVIDENCE_SINGLE_CAT_13_19_QUERY = f\"\"\"\n                                    SELECT ev_cat, COUNT(DISTINCT(doi)) AS number_of_articles\n                                    FROM \n                                        (SELECT doi, STRING_AGG(DISTINCT(evidence), \"&\" \n                                        ORDER BY evidence) AS ev_cat\n                                        FROM {upw_13_19}, UNNEST (oa_locations)\n                                        WHERE genre=\"journal-article\" AND year < 2020\n                                            AND is_paratext=False\n                                        GROUP BY doi\n                                        )\n                                    GROUP BY ev_cat\n                                   \"\"\"\n\n\n\nevidence_categories_08_12_df = client.query(EVIDENCE_SINGLE_CAT_08_12_QUERY).to_dataframe()\nevidence_categories_13_19_df = client.query(EVIDENCE_SINGLE_CAT_13_19_QUERY).to_dataframe()\nevidence_categories_df = pd.concat([evidence_categories_08_12_df, \n                                    evidence_categories_13_19_df])\n\nevidence_categories_df = evidence_categories_df.groupby(['ev_cat']) \\\n                                    .sum().reset_index() \\\n                                    .sort_values(by=['number_of_articles'], ascending=False) \\\n                                    .reset_index(drop=True)\n\nevidence_categories_df.head()\n#>                                               ev_cat  number_of_articles\n#> 0                                open (via free pdf)             2476980\n#> 1              oa repository (via OAI-PMH doi match)              771505\n#> 2  oa journal (via doaj)&open (via page says lice...              734679\n#> 3  oa repository (semantic scholar lookup)&open (...              726074\n#> 4            oa repository (semantic scholar lookup)              665390\n\nAgain, I merge the dataframe with a previously created dataframe called evidence_df. Then I compute the frequency of open access full-texts related to a single data source. The result can be contrasted with the total number of articles found in each evidence type.\n\n\nevidence_single_cat_df = evidence_df.groupby(['evidence']) \\\n                                .number_of_articles.sum() \\\n                                .reset_index()\n\nevidence_single_cat_df = pd.merge(evidence_single_cat_df, evidence_categories_df, \n                                 how='left', left_on=['evidence'], right_on=['ev_cat']) \\\n                                    .drop(['ev_cat'], axis=1) \nevidence_single_cat_df.columns = ['evidence', 'number_of_articles', 'number_of_single_cat']\n\n\n\nevidence_single_cat_df.evidence = evidence_single_cat_df \\\n                                    .evidence.replace(list_of_small_evidence_types, 'other')\n\nevidence_single_cat_grouped_df = evidence_single_cat_df.groupby(['evidence']) \\\n                                    .sum() \\\n                                    .eval('prop = number_of_single_cat/number_of_articles') \\\n                                    .reset_index() \\\n                                    .sort_values(by=['number_of_articles'], ascending=False) \\\n                                    .reset_index(drop=True)\n                                    \nevidence_single_cat_grouped_df['number_of_single_cat'] = evidence_single_cat_grouped_df \\\n                                                    .number_of_single_cat \\\n                                                    .astype(int)\n\nevidence_single_cat_grouped_df\n#>                                             evidence  ...      prop\n#> 0                                open (via free pdf)  ...  0.463675\n#> 1              oa repository (via OAI-PMH doi match)  ...  0.152200\n#> 2                       open (via page says license)  ...  0.131558\n#> 3            oa repository (semantic scholar lookup)  ...  0.143970\n#> 4                              oa journal (via doaj)  ...  0.105752\n#> 5                   oa repository (via pmcid lookup)  ...  0.038285\n#> 6  oa repository (via OAI-PMH title and first aut...  ...  0.247134\n#> 7                  oa journal (via observed oa rate)  ...  0.108509\n#> 8                                              other  ...  0.509406\n#> 9                        open (via crossref license)  ...  0.412148\n#> \n#> [10 rows x 4 columns]\n\nNext, I visualize the result in a horizontal barplot. For each evidence type I display the unique occurrences across related articles.\n\n\nfig, axes = plt.subplots(figsize=(7,5))\n\nplt.style.use('seaborn-whitegrid')\nplt.box(False)\n\nax1 = sns.barplot(x=[1] * len(evidence_single_cat_grouped_df), \n                  y='evidence', \n                  data=evidence_single_cat_grouped_df, \n                  label='FALSE', \n                  color='#b3b3b3a0',\n                  saturation=1,\n                  alpha=0.6\n                 )\n\nax2 = sns.barplot(x='prop', \n                  y='evidence', \n                  data=evidence_single_cat_grouped_df, \n                  label='TRUE', \n                  color='#56B4E9',\n                  saturation=1,\n                  alpha=1\n                 )\n\naxes.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\nplt.title('Proportion of Articles per Evidence Type', \n          fontdict={'fontsize': 12, 'fontweight': 600}, x=0.25, pad=15)\nplt.xlabel('Proportion of Articles', labelpad=10, \n           fontdict={'fontsize': 11, 'fontweight': 500})\n\nplt.ylabel('Evidence Type', labelpad=10, \n           fontdict={'fontsize': 11, 'fontweight': 500})\n\naxes.legend(title='Is unique?', \n            fontsize='medium', \n            bbox_to_anchor=(1.2, 1.05), \n            labelspacing=1.2)\n\nplt.show()\n\n\n\n\nFigure 6: Proportion of articles per evidence type.\n\n\n\nWhile forms of open access provision identified via pdf or license appear uniquely most often, it can be seen that their share has decreased, compared to the results from the previous analysis. On the other hand, the share of unique occurrences of less frequent evidence types which were collated in the category other has increased rapidly.\nTo visualize the intersection between multiple evidence types, I will use the UpSetPlot package which is better maintained in contrast to py-upset. It offers support for pandas and, I think, it is also well documented. Especially, I felt that the input format for the UpSet plot is well described.\nThe input format can be generated with the from_memberships function provided by UpSetPlot. It accepts two parameters: a nested list with elements corresponding to a set and secondly a list containing additional data which has the same length as the nested list. Oddly, the parameters intersection_plot_elements and totals_plot_elements from the UpSet class adjust the aspect ratio of the corresponding subplots in the UpSet figure and not the actual limit of intersections and sets to display, as it should be.\nConsidering this issue, I create a subset of the fifteen most frequent evidence type combinations manually. Therefore, the set sizes of the respective evidence types depend on the number of articles extracted in the subset rather than the total number of articles.\n\n\nevidence_categories_upset_df = evidence_categories_df.groupby(['ev_cat']) \\\n                                        .sum().reset_index() \\\n                                        .sort_values(by=['number_of_articles'], \n                                                     ascending=False) \\\n                                        .reset_index(drop=True)\n\n\n\n# subset of the fifteen most frequent evidence type combinations\nevidence_categories_upset_most_frequent = evidence_categories_upset_df[:15]\n\nev_list = evidence_categories_upset_most_frequent.ev_cat.tolist()\nev_list = [ev.split('&') for ev in ev_list]\nn_list = evidence_categories_upset_most_frequent.number_of_articles.tolist()\nevidence_categories_upset_expr = upsetplot.from_memberships(ev_list, data=n_list)\n\n\n\nfig = plt.figure(figsize=(10,6))\n\naxes = upsetplot.UpSet(evidence_categories_upset_expr, \n                sort_by='cardinality',\n                sort_categories_by='cardinality',\n                element_size=20, \n                intersection_plot_elements=15, \n                totals_plot_elements=9\n               ).plot(fig=fig)\n\naxes['intersections'].yaxis.grid(False)\n\naxes['totals'].xaxis.grid(False)\n\n# reduce overlap with text\nplt.subplots_adjust(left=0, bottom=0, right=1.1, top=1, wspace=0, hspace=0)\n\nfig.text(0.1, -0.07, 'Set size', ha='center', \n         fontdict={'fontsize': 10, 'fontweight': 500})\n\n# delete shading axis for better readability\nfig.delaxes(axes['shading'])\n    \nplt.show()\n\n\n\n\nFigure 7: Most frequent combinations of evidence types.\n\n\n\nIt can be seen that there is an even distribution of intersections between publisher-based evidence sources and repository-based evidence types with respect to the subset. Apparently, it is also common that multiple repositories contribute to an intersection, which means that an open access full-text is often available by several repositories.\nDiscussion and Conclusion\nIn this blog post, I provided an updated analysis of open access evidence in Unpaywall by examining database snapshots obtained from Unpaywall. Using Python, I was able to find 15,487,801 scholarly articles in Unpaywall in the period from 2008 to 2019, that are freely available. Fortunately, I can compare these results with the previous database snapshot from February 2019 which were described in the mentioned blog post. Thus, I can interpret upcoming trends more precisely.\nBased on the previous results, the analysis exposes an increase of open access full-texts by 7% from 37% to 44%. Furthermore, I was able to identify new evidence types that were introduced in the recently released database snapshots. However, the results should be viewed with caution, since I have ancillary observed the year 2019 and have excluded DOI’s that were identified as paratexts which was not the case in the previous work. Eventually, I suggest that more articles had left closed access and Unpaywall was able to link more DOI’s to full-texts because of newly introduced data sources. My results confirm the rise of repositories as hosting provider of open access full texts. Despite that, the share of articles published in DOAJ-listed journals rose as well, probably more open access journals were launched and registered with DOAJ.\nAdditionally, this work demonstrates similarities between data analysis in Python and R. This work translated R source code, demonstrating how to leverage Python and R in a diverse team of data analyst.\nAcknowledgements\nThis work was supported by the Federal Ministry of Education and Research of Germany (BMBF) in the framework Quantitative research on the science sector (Project: “OAUNI Entwicklung und Einflussfaktoren des Open-Access-Publizierens an Universitäten in Deutschland”, Förderkennzeichen: 01PU17023A).\n\n\n",
    "preview": "posts/unpaywall_python/distill-preview.png",
    "last_modified": "2023-11-01T09:08:02+01:00",
    "input_file": {},
    "preview_width": 3385,
    "preview_height": 1256
  },
  {
    "path": "posts/elsevier_invoice/",
    "title": "Mining and analysing invoice data from Elsevier relative to hybrid open access",
    "description": "Publishers rarely make publication fee spending for hybrid journals transparent. Elsevier is a remarkable exception, as the publisher provides open and machine-readable data relative to its central invoicing with funding bodies and fee waivers at the article level. This blogpost illustrates how to mine Elsevier full-texts for these data with the data science tool R and presents new insights by analysing the resulting dataset: of 70,657 articles published open access in 1,753 hybrid journals from 2015 to date, around one third of the publication fees were paid through central agreements. Nevertheless, the majority of funding sources for hybrid open access remains unclear.",
    "author": [
      {
        "name": "Najko Jahn",
        "url": "https://twitter.com/najkoja"
      }
    ],
    "date": "2019-11-25",
    "categories": [],
    "contents": "\n\n\n\nIntroduction and background\nIn September 2018, cOAltion\nS, a group of international research funders, announced its widely\ndiscussed Plan\nS. According to its principles, research funding organisations\naligned in cOAlition S will cover open access publication fees, also\nknown as article-processing charges (APC), but they expressed the intent\nto suspend financial support of such fees associated with open access\npublishing in hybrid journals. An exception are cases within the\ncontrolled setting of transformative agreements. These are institutional\nor consortial agreements that repurpose subscription expenditures for\nopen access publishing in order to drive the transition of\nsubscription-based journal publishing to fully open access; research\nperforming organisations initiated transformative agreements in recent\nyears as a strategy to rein in uncontrolled and unmonitored spending on\npublication fees in hybrid journals and to accelerate the open access\ntransition.\nWith the aforementioned financial restrictions in place from 2021,\ncOAltionS also intends to monitor compliance with the Plan S principles.\nTo date, however, the monitoring of spending for open access publishing\nin hybrid journals has been limited, due to a lack of data around these\nfinancial transactions. Although surveys(Solomon and Björk 2011; Dallmeier-Tiessen et al. 2011) suggest\nthat already many authors do not pay publication fees themselves,\nkeeping track of these funding streams is challenging, because\npublishers rarely share invoice data (Björk 2017). But also not all funders\nand research organisations track or make the payment information\navailable to fill this gap, despite examples like the British Charity\nOpen Access Fund or the Open APC Initiative\n(Jahn and Tullney 2016).\nAt the SUB Göttingen, we will address this lack of transparency in a\nnew project\nfunded by the Deutsche Forschungsgemeinschaft (DFG) in the context of\nits programme “Open\nAccess Transition Agreements”(Holzer\n2017). Building on our pilot project, the interactive Shiny app Hybrid OA Journal\nMonitor, this project will investigate the data needs of German\nlibrary consortia and how they can be addressed through metadata\nrequirements in transformative agreements. Case-studies and data\nproducts will monitor levels of compliance with policy recommendations.\nHere, invoice data will be essential to make the various funding streams\nof open access publishing in hybrid journals visible.\nAgainst this background, this blogpost presents a dataset comprising\npublicly available invoice data relative to open access articles in\nhybrid journals published by Elsevier, a major publisher of scholarly\njournals. This dataset brings together metadata from Crossref and\ninformation retrieved from open access full-texts. The methods used to\nobtain the data address challenges to discover open access articles in\nhybrid journals(Laakso and Björk\n2016) including related funding and affiliation information\nusing open data and tools. I will argue that Elsevier’s approach of\nsharing invoice recipients serves as an example of good business\npractise for other publishers offering hybrid open access options and\ncentral open access agreements. It is, thus, relevant for\nstandardisation efforts like the “ESAC Workflow\nRecommendations for Transformative Agreements” (Geschuhn and Stone\n2017).\nTo demonstrate the potential of publisher-provided data to enable\nmonitoring Plan S compliance, transformative agreements and the\ntransition of subscription journals to open access, the dataset will be\nused to analyse the number and the proportion of open access articles in\nElsevier hybrid journals. Drawing on Elsevier’s funding information, I\nwill also investigate whether Elsevier sent invoices to authors or to\nfunders and research organisations that, presumably, have either a central\npayment agreement or a transformative\nagreement with Elsevier, or whether the fees were waived. Moreover,\ntext-mined author email domains will provide a rough approximation of\nthe affiliation of the first corresponding author, an important data\npoint for delineating open access funding; it is now standard practise\nfor the first, or submitting corresponding author, or her institution,\nto take on responsibility for payment of the relative open access\npublishing fees (Geschuhn and Stone\n2017). Finally, the publisher-provided invoice data will be\ncompared with crowd-sourced spending data from the Open APC\nInitiative.\nTo allow for a data-driven discussion about Elsevier’s approach and\nits potential for monitoring Plan S compliance and transformative\nagreements, I made the resulting dataset openly available on GitHub\nalong with the source code used to obtain the data.\nMethods\nAs a start, I used the Elsevier\npublication fee price list, an openly available pdf document, to\ndetermine current hybrid open access journals in Elsevier’s journal\nportfolio. The rOpenSci tabulizer package (Leeper 2018) allowed me to extract data\nabout these journals from this file.\n\nSee analytical script fetch_apc_list.R\nfor more details.\nThen, I interfaced the Crossref REST API with the R package rcrossref (Chamberlain et al. 2019). The first API\ncall retrieved facet field counts for license URLs and the yearly\narticle volumes for the period 2015-19 for every journal. After matching\nCreative Commons license URLs indicating open access articles, a second\nAPI call retrieved article-level metadata per journal. Next, I used the\nmetadata field delay-in-days to exclude open access\narticles published after an embargo period (“delayed open access”).\nBecause a few records had different date formats, which were used for\nthe delay calculation by Crossref, I allowed for a lag of 31 days.\n\nSee analytical script fetch_cr_md.R\nfor more details.\nElsevier participates in the Crossref\nText and Data Mining Services (Crossref-TDM) and provides access to\nfull-texts as html and xml documents.\nSurprisingly, the xml representation not only contains the\nfull-text, but also comprises embedded metadata including information\nabout open access sponsorship in the <core> node:\n<openaccess>1<\/openaccess>\n<openaccessArticle>true<\/openaccessArticle>\n<openaccessType>Full<\/openaccessType>\n<openArchiveArticle>false<\/openArchiveArticle>\n<openaccessSponsorName>\n  BMBF - German Federal Ministry of Education and Research\n<\/openaccessSponsorName>\n<openaccessSponsorType>FundingBody<\/openaccessSponsorType>\n<openaccessUserLicense>\n  http://creativecommons.org/licenses/by/4.0/\n<\/openaccessUserLicense>\nSnapshot of open access metadata in Elsevier XML full-texts. https://api.elsevier.com/content/article/PII:S0169409X18301479?httpAccept=text/xml\nAfter downloading the Elsevier full-texts with the crminer package(Chamberlain 2018), I extracted the\nabove-highlighted open access information from the xml\ndocuments.\n\nSee analytical script tdm_oa_info.R\nfor more details.\nMoreover, I parsed the first occurrence of an author email, assuming\nthat email domains roughly indicate the affiliation of the relevant\ncorresponding author at the time of publication. The package urltools (Keyes et al. 2019) made it possible to\nextract email domains and to split them into meaningful parts.\n\nSee analytical script extract_email_domains.R\nfor more details.\nFinally, to measure the overlap between crowd-sourced and\npublisher-provided invoice data, I downloaded spending data from the\nOpen APC Initiative (Aasheim et al.\n2019). To my knowledge, the Open APC Initiative maintains the\nlargest evidence-base for institutional spending on open access\npublication fees.\nThroughout the data analysis, I used tools from the Tidyverse (Wickham et al. 2019). Data were gathered\non 15 November 2019. To make this project more reproducible, I shared it\nas a research compendium using the holepunch package (Ram 2019). A research compendium\ncontains data, code, and text associated with it (Marwick, Boettiger, and Mullen 2018).\nThe research compendium belonging this blog post is accessible here: https://github.com/subugoe/elsevier_hybrid_volume\nDataset characteristics\nIn the following data analysis, I will be using two files that I\ncompiled. The first file, journal_facets.json,\ncontains the publication volume per Elsevier journal offering hybrid\nopen access. It furthermore summarises the various license URLs found\nthrough Crossref per Elsevier journal.\nThe second file, elsevier_hybrid_oa_df.csv,\ncomprises article-level data. Each row holds information for a single\nhybrid open access article published in a hybrid journal, and the\ncolumns represent:\nVariable\nDescription\ndoi\nDOI\nlicense\nOpen Content License\nissued\nEarliest publication date\nissued_year\nEarliest publication year\nissn\nISSN, a journal identifier\njournal_title\nThe title of the journal\njournal_volume\nYearly publication volume\ntdm_link\nLink to the XML full-text\noa_sponsor_type\nInvoice recipient type\noa_sponsor_name\nInstitution that directly received an\ninvoice\noa_archive\nWas open access provided through\nElsevier’s open archive programme, in which articles are made openly\navailable after an embargo?\nhost\nEmail host,\ne.g. med.cornell.edu\ntld\nTop-level domain,\ne.g. edu\nsuffix\nExtracted suffix from domain name as\ndefined by the public suffix list, e.g. ac.uk\ndomain\nEmail domain,\ne.g. cornell.edu\nsubdomain\nEmail subdomain,\ne.g. med\nIt should be noted, however, that Elsevier did not provide an\nofficial documentation of its open access and invoice data at the time\nof writing of this blogpost.\nResults\n\n\n\nIn total, 1,753 out of 1,990 hybrid journals published at least one\nopen access article from 2015 to date, corresponding to about 88% of\njournal titles in Elsevier’s hybrid journal portfolio. In these\njournals, 70,657 articles were published open access. The total share of\nhybrid open access in the publication volume of Elsevier journals was\n2.4%.\nWhat\nis the uptake of open access in Elsevier hybrid journals?\n\n\n\nThe open access share varied across Elsevier hybrid journals. Figure\n1, which replicates a boxplot aesthetics from The\nEconomist magazine using the ggeconodist package (Rudis 2019), shows a slow, but steady\nhybrid open access uptake. The median open access proportion was around\n3% in the first eleven months in 2019.\n\n\n\nFigure 1: Open access uptake in Elsevier journals per year in\npercent, visualised as diminutive distribution chart. Since 2015, most\nhybrid journals have had a slow uptake rate of open access articles. In\ngeneral, open access via the hybrid open access publishing model played\na marginal role in the context of Elsevier’s total publication volume.\nData Sources: Crossref, Elsevier B.V.\n\n\n\nHow\nmany payments for open access articles in hybrid journals were\nfacilitated by central invoicing?\nIn most cases, Elsevier sent invoices for hybrid open access\npublication fees to individual authors (59%). For around 33% of\narticles, the publisher directly billed funders and research\norganisations. Elsevier granted publication fee waivers to 6.2% of open\naccess articles in hybrid journals.\nFigure 2 shows the annual development per\ninvocing type. Inspired by Claus O. Wilke’s “Fundamentals of Data\nVisualisation” (Wilke 2019), each type\nis visualised separately as parts of the total. The figure reveals a\ngeneral growth of open access articles in hybrid journals. It\nillustrates that this development was mainly driven by billing\nindividual authors, while central invoicing stagnated. Also, the amount\nof fee-waived articles remained more or less constant from 2015 to\ndate.\n\n\n\nFigure 2: Development of fee-based open access publishing in\nElsevier hybrid journals by invoicing type. Colored bars represent the\ninvoice recipient, or whether the fee was waived. Grey bars show the\ntotal number of hybrid open access articles published in Elsevier\njournals from 2015 to date. Data Sources: Crossref, Elsevier B.V.\n\n\n\nThe following interactive visualisation (Figure\n3), created with the echarts4r package(Coene 2019), lets you browse the\ninvoicing data. I recommend using a recent Chrome browser ot interact\nwith the visualisation.\n\n\n\n\nFigure 3: Breakdown of Elsevier hybrid open access journal\narticles by invoice recipient. Each rectangle represents an invoicing\ntype and can be broken down by recipient. Data Source: Elsevier B.V.\n\n\n\n\nI recommend using a recent Chrome browser to interact with the\nvisualisation.\nClicking on “Agreement” shows the funders or research organisations\nthat paid for open access publication fees as part of a central or\ntransformative agreement. In total, Elsevier disclosed 74 different\ninstitutions that received an invoice for open access publication. Not\nsurprisingly, mostly British and Dutch funders or consortia paid for\nhybrid open access in Elsevier hybrid journals. The German Federal\nMinistry of Education and Research (BMBF) is, however, also represented\ndespite the current boycott from most universities and research\norganisations in Germany (Else 2018). In fact, the BMBF is not\npart of the Alliance of\nScience Organisations in Germany, whose members want to negotiate a\ntransformative agreement with Elsevier (Mittermaier 2017). Since 2018, the BMBF\nhas financially supported 181 open access articles that appeared in 129\nElsevier hybrid journals according to data from the publisher.\nWho\npublished hybrid open access in Elsevier journals?\nIn addition to funding information, email domains were parsed from\nElsevier full-texts. These domains roughly indicate the affiliation of\nthe first corresponding author, a data point used to delineate open\naccess funding (Geschuhn and Stone\n2017).\n\n\n\nFigure 4: Email domain analysis of first corresponding authors\npublishing open access in Elsevier hybrid journals. Around every fourth\nopen access article in an Elsevier hybrid journal from 2015 to date had\na corresponding author affiliated with an UK-based academic institution.\nData Source: Elsevier B.V.\n\n\n\nFigure 4 presents a breakdown by email domain\nsuffix. In total, 67,900 email addresses were retrieved and parsed from\nElsevier full-texts, corresponding to a share of 96%. Most corresponding\nauthor emails originate from academic institutions in the UK (“.ac.uk”),\nreflecting the country’s leading role in supporting hybrid open access\n(Pinfield, Salter, and\nBath 2015). They are followed by domains from commercial\norganisations (“.com”), and US-American institutions of higher education\n(“.edu”). The figure illustrates that European institutions from Germany\n(“.de”), the Netherlands (“.nl”), and Sweden (“.se”) were also well\nrepresented. In total, 330 domain suffixes were retrieved.\nIn the following figure, a hierarchical, interactive treemap\nvisualises the distribution of the email domains (see Figure\n5). It appears that this distribution roughly\nrepresents the overall national research landscapes measured by\npublication output. However, the dominance of domains from commercial\norganisations, mostly email providers like “gmail.com” or the Chinese\n“163.com” and “126.com”, highlights the limitations of this approach to\ninfer eligible funding institutions with author email addresses.\n\n\n\n\nFigure 5: Email domain analysis of first corresponding authors\npublishing open access in Elsevier hybrid journals. Each top-level\ndomain can be subdivided further into domain names representing academic\ninstitutions or companies. Data Source: Elsevier B.V.\n\n\n\n\nI recommend using a recent Chrome browser to interact with the\nvisualisation.\nHow\ndoes Elsevier invoice data compare to spending information from the Open\nAPC Initiative?\n\n\n\nFinally, I was interested in the overlap between publisher-provided\ninvoice data from Elsevier and institutional spending data from the Open\nAPC Initiative. In total, the Open APC Initiative tracked 8,213 out of\n70,657 published open access articles in hybrid journals, corresponding\nto a share of 12%. Institutional expenditures for these articles\namounted to 24,008,889 € according to Open APC data. However, the Open\nAPC Initiative listed 683 additional open access articles. One likely\nexplanation is that the Crossref metadata representing these articles\ndid not meet my criteria; another explanation could be that they\nappeared in journals that recently transitioned from hybrid to fully\nopen access (e.g. the journal “NeuroImage”). At the journal level, the\noverlap was 58%.\nFigure 6 presents the annual development of\nspending disclosure relative to open access articles in Elsevier hybrid\njournals as reported in the Open APC Initiative grouped by invoicing\ntype. The Open APC Initiative mostly tracked articles covered under\ncentral invoicing agreements. The figure also suggests that invoices\nbilled to authors were covered by institutions participating in Open\nAPC. Generally, the results confirm a delay between invoicing and\nreporting to the Open APC Initiative (Jahn and Tullney 2016). Surprisingly,\nOpen APC listed institutional payments for 13 articles, for which\nElsevier reported that the relative fee was waived.\n\n\n\nFigure 6: Development of fee-based open access publishing in\nElsevier hybrid journals by invoicing type and disclosure of\ninstitutional payment by the Open APC Initiative. Grey bars show the\ntotal number of hybrid open access articles published by invoicing type\nfrom 2015 to date. Colored bars represent the number of articles that\nare also tracked in Open APC. Data Sources: Crossref, Elsevier B.V.,\nOpen APC Initiative.\n\n\n\nFigure 7 presents the gap between publisher-provided\ninvoice data and Open APC for the ten greatest contributing funding\nbodies. It highlights that British funders had the largest overlap\nrates, which reflects Open APC efforts to re-use openly available\nspending data from these institutions (Pieper and Broschinski 2018). On the\nother hand, Open APC did not track Dutch (“VSNU”), U.S. (“Melinda &\nBill Gates Foundation”) or European funding activities (“European\nResearch Council”) for hybrid open access publication fees.\n\n\n\nFigure 7: Proportion of fee-based open access articles in Elsevier\nhybrid journals disclosed by the Open APC Initiative. Blue areas\nrepresent an overlap in spending data availability, grey areas reflect\ncentrally paid articles, which were not present in the Open APC data.\nData Source: Crossref, Elsevier B.V., Open APC Initiative.\n\n\n\nDiscussion and conclusion\nIn this blog post, I have illustrated how it is possible to obtain\ninvoice data from Elsevier, which is embedded in full-texts. This data\ncan be used to determine whether Elsevier sent invoices to authors, to\nfunders or research organisations that have a central payment agreement\nor a transformative agreement with Elsevier, or whether the fee was\nwaived. Providing such machine-readable data, makes funding streams for\nhybrid open access more transparent.\nAt the same time, the data analysis highlights various critical\naspects related to open access publishing in hybrid journals. Despite\nincreased funding activities, only a small proportion of journal\narticles were made openly available under this model. Furthermore,\nElsevier sent the majority of invoices directly to the authors. This\npractise not only imposes administrative burdens and costs to all\nparties involved, but also conceals funding sources for publication\nfees. Existing spending data from funders and research organisations can\nonly partly overcome this gap. Moreover, publishers offer different\nkinds of funding opportunities for hybrid open access at the same time,\nincluding central invoicing. However, it is likely that not all\nagreements with central invoicing as they currently stand meet the Plan\nS requirements for transformative agreements.\nImplementation of Plan S is underway to change current practises of\nfunding open access publication in hybrid journals. Because Elsevier’s\ncurrent transparency related to their invoicing is a remarkable\nexception, workflow guidelines for transformative agreements should\nconsider taking the publisher’s example of sharing invoice data as a\nrecommended good business practise for publishers. Although future work\nneeds to tackle the remaining questions about the data quality and\ncoverage, publisher-provided invoice data make publishers more\naccountable and extends the evidence base relative to hybrid open\naccess. As a result, the data analysis presented here provides a basis\nto improve the monitoring of funding streams in the context of\ntransformative agreements.\nAcknowledgments\nThis work was supported by the Deutsche Forschungsgemeinschaft,\nproject “Hybrid\nOA Dashboards: Mehrwertorientierte Analytics-Anwendungen zur Förderung\nder Kostentransparenz bei Transformationsverträgen”, project id\n416115939.\n\n\n\nAasheim, Jens Harald, Benjamin Ahlborn, Chelsea Ambler, Magdalena\nAndrae, Jochen Apel, Hans-Georg Becker, Roland Bertelmann, et al. 2019.\nThe Open APC Initiative. Bielefeld University Library. https://github.com/OpenAPC/openapc-de.\n\n\nBjörk, Bo-Christer. 2017. “Growth of Hybrid Open Access,\n2009-2016.” PeerJ 5: e3878. https://doi.org/10.7717/peerj.3878.\n\n\nChamberlain, Scott. 2018. Crminer: Fetch ’Scholary’ Full Text from\n’Crossref’. https://CRAN.R-project.org/package=crminer.\n\n\nChamberlain, Scott, Hao Zhu, Najko Jahn, Carl Boettiger, and Karthik\nRam. 2019. Rcrossref: Client for Various ’CrossRef’ ’APIs’. https://CRAN.R-project.org/package=rcrossref.\n\n\nCoene, John. 2019. Echarts4r: Create Interactive Graphs with\n’Echarts JavaScript’ Version 4. http://echarts4r.john-coene.com/.\n\n\nDallmeier-Tiessen, Suenje, Robert Darby, Bettina Goerner, Jenni\nHyppoelae, Peter Igo-Kemenes, Deborah Kahn, Simon C. Lambert, et al.\n2011. “Highlights from the SOAP Project Survey. What Scientists\nThink about Open Access Publishing.” http://arxiv.org/abs/1101.5260.\n\n\nElse, Holly. 2018. “Dutch Publishing Giant Cuts Off Researchers in\nGermany and Sweden.” Nature 559 (7715): 454–55. https://doi.org/10.1038/d41586-018-05754-1.\n\n\nGeschuhn, Kai, and Graham Stone. 2017. “It’s the Workflows,\nStupid! What Is Required to Make ‘Offsetting’ Work for the\nOpen Access Transition.” Insights: The UKSG Journal 30\n(3): 103–14. https://doi.org/10.1629/uksg.391.\n\n\nHolzer, Angela. 2017. “Wozu\nOpen-Access-Transformationsverträge?” O-Bib. Das Offene\nBibliotheksjournal 4: 87–95. https://doi.org/10.5282/o-bib/2017H2S87-95.\n\n\nJahn, Najko, and Marco Tullney. 2016. “A Study of Institutional\nSpending on Open Access Publication Fees in Germany.”\nPeerJ 4 (August): e2323. https://doi.org/10.7717/peerj.2323.\n\n\nKeyes, Os, Jay Jacobs, Drew Schmidt, Mark Greenaway, Bob Rudis, Alex\nPinto, Maryam Khezrzadeh, et al. 2019. Urltools: Vectorised Tools\nfor URL Handling and Parsing. https://CRAN.R-project.org/package=urltools.\n\n\nLaakso, Mikael, and Bo-Christer Björk. 2016. “Hybrid Open Access—a\nLongitudinal Study.” Journal of Informetrics 10 (4):\n919–32. https://doi.org/10.1016/j.joi.2016.08.002.\n\n\nLeeper, Thomas J. 2018. Tabulizer: Bindings for Tabula PDF Table\nExtractor Library. https://cran.r-project.org/package=tabulizer.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).”\nPeerJ Preprints. https://doi.org/10.7287/peerj.preprints.3192v2.\n\n\nMittermaier, Bernhard. 2017. “From the DEAL Engine Room — an\nInterview with Bernhard Mittermaier.” LIBREAS.Library\nIdeas. https://libreas.eu/ausgabe32/mittermaier_en/.\n\n\nPieper, Dirk, and Christoph Broschinski. 2018. “OpenAPC: A\nContribution to a Transparent and Reproducible Monitoring of Fee-Based\nOpen Access Publishing Across Institutions and Nations.”\nInsights: The UKSG Journal 31. https://doi.org/10.1629/uksg.439.\n\n\nPinfield, Stephen, Jennifer Salter, and Peter A. Bath. 2015. “The\n\"Total Cost of Publication\" in a Hybrid Open-Access Environment:\nInstitutional Approaches to Funding Journal Article-Processing Charges\nin Combination with Subscriptions.” Journal of the\nAssociation for Information Science and Technology 67 (7): 1751–66.\nhttps://doi.org/10.1002/asi.23446.\n\n\nRam, Karthik. 2019. Holepunch: Configure Your r Project for\n’Binderhub’. https://github.com/karthik/holepunch.\n\n\nRudis, Bob. 2019. Ggeconodist: Create Diminutive Distribution\nCharts. https://github.com/hrbrmstr/ggeconodist.\n\n\nSolomon, David J., and Bo-Christer Björk. 2011. “Publication Fees\nin Open Access Publishing: Sources of Funding and Factors Influencing\nChoice of Journal.” Journal of the Association for\nInformation Science and Technology 63 (1): 98–107. https://doi.org/10.1002/asi.21660.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nMcGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome\nto the Tidyverse.” Journal of Open Source Software 4\n(43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization.\nO’Reilly. https://serialmentor.com/dataviz/.\n\n\n\n\n",
    "preview": "posts/elsevier_invoice/distill-preview.png",
    "last_modified": "2023-11-01T09:08:02+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/datacite_graph/",
    "title": "Interfacing the PID Graph with R",
    "description": "The PID Graph from DataCite interlinks persistent identifiers (PID) in research. In this blog post, I will present how to interface this graph using the DataCite GraphQL API with R. To illustrate it, I will visualise the research information network of a  person.",
    "author": [
      {
        "name": "Najko Jahn",
        "url": "https://twitter.com/najkoja"
      }
    ],
    "date": "2019-10-24",
    "categories": [],
    "contents": "\nIn 1965, Derek J. de Solla Price proposed to study the relationships between research articles using bibliographic references (Solla Price 1965). Ever since, scholars and librarians have been working on interrelating research activities and making such links discoverable.\nIn this context, the FREYA project, funded by the European Commission, connects and interlinks persistent identifier (PID) schemes. FREYA focuses, among others, on PIDs for persons (ORCID), organisations (ROR), publications, research data, and software (DOI). The project has created a PID Graph, which connects various resources using persistent identifiers. A GraphQL interface allows accessing these data.\nUpon invitation of Martin Fenner, Technical Director of DataCite and FREYA team member, I attended the half-day workshop Project FREYA: connecting knowledge in the European Open Science Cloud, co-located at 14th Plenary Meeting of the Research Data Alliance in Helsinki. Using data analytics as an outreach strategy, Martin prepared a large collection of Juypter notebooks showcasing how the PID Graph can be interfaced using R and Python (Fenner 2019c). During the workshop, we presented two interactive notebooks deployed on mybinder.org, and invited workshop participants to re-run them in the web browser. The first notebook (Fenner 2019b) presents the overall indexing coverage of the PID Graph, while the second notebook demonstrated how to obtain data about a personal researcher network (Fenner 2019a).\nIn this blog post, I want to expand on what I have learned during the FREYA workshop. Although most participants were able to run the interactive Jupyter notebooks, some articulated problems along the data transformation path. In the following, I will therefore present an complementary approach of how to transform and visualise data from the PID graph with R by using tools from the popular tidyverse package collection.\nAccessing the PID Graph using GraphQL\nA first version of the PID graph is accessible via the DataCite GraphQL API. GraphQL is a query language designed to request multiple connections across resources at once. As an example, a query for accessing publications, research data and software by a particular researcher using the DataCite GraphQL API looks like this:\n\ngraphql_query <- '{\n  person(id: \"https://orcid.org/0000-0003-1444-9135\") {\n    id\n    type\n    name\n    publications(first: 50) {\n      totalCount\n      nodes {\n        id\n        type\n        relatedIdentifiers {\n          relatedIdentifier\n        }\n      }\n    }\n    datasets(first: 50) {\n      totalCount\n      nodes {\n        id\n        type\n        relatedIdentifiers {\n          relatedIdentifier\n        }\n      }\n    }\n    softwareSourceCodes(first: 50) {\n      totalCount\n      nodes {\n        id\n        type\n        relatedIdentifiers {\n          relatedIdentifier\n        }\n      }\n    }\n  }\n}'\n\nHere, I query for publications, research data and software code authored by Scott Chamberlain, who is represented by his ORCID. I also retrieve relations between his research activities that are represented in the relatedIdentifier node. The query is stored in the R object graphql_query that will be used to interface the DataCite GraphQL API in the following.\nTo make GraphQL requests with R, Scott developed the R package ghql, which is maintained by rOpenSci. The package is not on CRAN, but can be installed from GitHub.\n\n# Not on CRAN.\n# Install from GitHub remotes::install_github(\"ropensci/ghql\")\nlibrary(ghql)\n\nTo initialize the client session, call\n\ncli <- GraphqlClient$new(\n  url = \"https://api.datacite.org/graphql\"\n)\nqry <- Query$new()\n\nNext, I can send the query stored in graphql_query to the API.\n\nqry$query(\"getdata\", graphql_query)\n\nThe data is represented in json. To parse the API response, I use the jsonlite package.\n\nlibrary(jsonlite)\nmy_data <- jsonlite::fromJSON(cli$exec(qry$queries$getdata))\n\nData Transformation\nThe data is represented as a nested list, which can be transformed to a data.frame using tidyverse tools tidyr and dplyr. Here, I want to obtain all DOIs representing scholarly articles, datasets and software including the relationships between them. Unlike the DOIs for research outputs, related identifiers of type DOI lack the DOI prefix. For consistency with the overall dataset, the prefix will be added.\n\nlibrary(dplyr)\nlibrary(tidyr)\nmy_df <- bind_rows(\n  # publications\n  my_data$data$person$publications$nodes,\n  # dataset\n  my_data$data$person$datasets$nodes,\n  # software\n  my_data$data$person$softwareSourceCodes$nodes\n) %>%\n  # get related identifiers\n  unnest(cols = c(relatedIdentifiers), keep_empty = TRUE) %>%\n  # unfortunately, related identifiers of type DOI lack DOI prefix\n  mutate(to = ifelse(\n    grepl(\"^10.\", relatedIdentifier),\n    paste0(\"https://doi.org/\", relatedIdentifier),\n    relatedIdentifier)\n  )\nhead(my_df)\n# A tibble: 6 x 4\n  id                  type      relatedIdentifier    to               \n  <chr>               <chr>     <chr>                <chr>            \n1 https://doi.org/10… Scholarl… <NA>                 <NA>             \n2 https://doi.org/10… Scholarl… 10.6084/m9.figshare… https://doi.org/…\n3 https://doi.org/10… Scholarl… <NA>                 <NA>             \n4 https://doi.org/10… Scholarl… <NA>                 <NA>             \n5 https://doi.org/10… Scholarl… 10.6084/m9.figshare… https://doi.org/…\n6 https://doi.org/10… Scholarl… 10.6084/m9.figshare… https://doi.org/…\n\nA network consists of nodes (vertices) and links (edges). Nodes represent an output, while links describes relationships between them (relatedIdentifier).\nLet’s create node data.frame\n\nmy_nodes <- my_df %>%\n  select(name = id, type) %>%\n  distinct() %>%\n  # person\n  add_row(name = my_data$data$person$id, type = \"Person\")\nhead(my_nodes)\n# A tibble: 6 x 2\n  name                                         type            \n  <chr>                                        <chr>           \n1 https://doi.org/10.6084/m9.figshare.97222    ScholarlyArticle\n2 https://doi.org/10.6084/m9.figshare.94217.v2 ScholarlyArticle\n3 https://doi.org/10.6084/m9.figshare.94296    ScholarlyArticle\n4 https://doi.org/10.6084/m9.figshare.94090    ScholarlyArticle\n5 https://doi.org/10.6084/m9.figshare.94295.v2 ScholarlyArticle\n6 https://doi.org/10.6084/m9.figshare.97215.v1 ScholarlyArticle\n\nand a data.frame with the relationships between these nodes, i.e. edges:\n\nmy_edges_pub <- my_df %>%\n  select(source = id, target = to) %>%\n  # we only observe links between them\n  filter(target %in% my_nodes$name)\n#' lets ad relationsships between person and outputs\nmy_edges <-\n  tibble(source = my_data$data$person$id, target = my_nodes$name) %>%\n  # no self loop\n  filter(target != my_data$data$person$id) %>%\n  bind_rows(my_edges_pub)\nhead(my_edges)\n# A tibble: 6 x 2\n  source                          target                              \n  <chr>                           <chr>                               \n1 https://orcid.org/0000-0003-14… https://doi.org/10.6084/m9.figshare…\n2 https://orcid.org/0000-0003-14… https://doi.org/10.6084/m9.figshare…\n3 https://orcid.org/0000-0003-14… https://doi.org/10.6084/m9.figshare…\n4 https://orcid.org/0000-0003-14… https://doi.org/10.6084/m9.figshare…\n5 https://orcid.org/0000-0003-14… https://doi.org/10.6084/m9.figshare…\n6 https://orcid.org/0000-0003-14… https://doi.org/10.6084/m9.figshare…\n\nNetwork visualisation\nFor the graph visualisation, I use the popular network analysis package igraph. First, the node and edge data are transformed to an igraph object. I also want to remove potential loops (“self-links”).\n\nlibrary(igraph)\ng <-\n  graph_from_data_frame(d = my_edges,\n                        vertices = my_nodes,\n                        directed = FALSE)\n#' remove potential loops\ng <- igraph::simplify(g)\n\nNext, some visualisation parameter are defined including node colours and labels. Here, node colours represent the person and the three different publication types.\n\n#' define node colours\nmy_palette <-\n  c(\"#6da7de\", \"#9e0059\", \"#dee000\", \"#d82222\")\nmy_color <- my_palette[as.numeric(as.factor(V(g)$type))]\n#' don't display label\nV(g)$label = NA\n\nFinally, let’s visualise Scott’s publication network according to DataCite metadata.\n\nplot(simplify(g), vertex.color = my_color, \n     vertex.frame.color = my_color,\n     arrow.mode = 0)\nlegend(\n  \"bottomleft\",\n  legend = levels(as.factor(V(g)$type)),\n  col = my_palette,\n  bty = \"n\",\n  pch = 20 ,\n  pt.cex = 2.5,\n  cex = 1,\n  horiz = FALSE,\n  inset = c(0.1,-0.1)\n)\n\n\nDiscussion and Outlook\nUsing data analytics is a great outreach activity to promote the PID Graph. During the workshop, participants were able to run the interactive notebooks with analytical code. This enabled a hands-on experience about how to interface the graph with GraphQL. It also led to great discussions about the PID Graph’s indexing coverage and potential use-cases. In particular, participants raised the issue of yet-incomplete PID metadata coverage. In our example, for instance, we likely miss a considerable amount of Scott’s software projects linked with a DOI, because the underlying metadata records lack his ORCID.\nBesides the fruitful discussion about PID coverage in the metadata, I had the feeling that many participants struggled with following the steps for data transformation. Therefore, I decided to dry out the code from the initial notebook using the packages tidyr and dplyr from the tidyverse. I hope that such an approach will make the examples clearer.\nIn the future, the FREYA team will continuously extend the indexing coverage of the PID Graph in collaboration with related research graph activities from OpenAIRE (Manghi et al. 2019), and the Wikibase community. On 25 October, there will be a joint meeting of large data providers for Open Science Graphs at the RDA 14th Plenary. Together with the Software Sustainability Institute, FREYA will hold a day-long hackathon on 4 December at the British Library so as to further improve data analytics using the PID graph.\nAcknowledgments\nI would like to thank Martin Fenner, Kristian Garza, Slava Tykhonov, and Maaike de Jong for having me at the workshop, and their valuable help with the analysis and the use of the PID Graph with Jupyter Notebooks.\n\n\n\nFenner, Martin. 2019a. “FREYA PID Graph for a Specific Researcher.” DataCite. https://doi.org/10.14454/628M-3882.\n\n\n———. 2019b. “FREYA PID Graph Key Performance Indicators (KPIs).” DataCite. https://doi.org/10.14454/3BPW-W381.\n\n\n———. 2019c. “Using Jupyter Notebooks with GraphQL and the PID Graph.” https://doi.org/10.5438/HWAW-XE52.\n\n\nManghi, Paolo, Alessia Bardi, Claudio Atzori, Miriam Baglioni, Natalia Manola, Jochen Schirrwagen, and Pedro Principe. 2019. “The OpenAIRE Research Graph Data Model.” https://doi.org/10.5281/ZENODO.2643199.\n\n\nSolla Price, D. J. de. 1965. “Networks of Scientific Papers.” Science 149 (3683): 510–15. https://doi.org/10.1126/science.149.3683.510.\n\n\n\n\n",
    "preview": "posts/datacite_graph/distill-preview.png",
    "last_modified": "2023-11-01T09:08:02+01:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/unpaywall_evidence/",
    "title": "Open Access Evidence in Unpaywall",
    "description": "We investigated more than 31 million scholarly journal articles published between 2008 and 2018 that are indexed in Unpaywall, a widely used open access discovery tool. Using Google BigQuery and R, we determined over 11.6 million journal articles with open access full-text links in Unpaywall, corresponding to an open access share of 37 %. Our data analysis revealed various open access location and evidence types, as well as large overlaps between them, raising important questions about how to responsibly re-use Unpaywall data in bibliometric research and open access monitoring.",
    "author": [
      {
        "name": "Najko Jahn",
        "url": "https://twitter.com/najkoja"
      },
      {
        "name": "Anne Hobert",
        "url": {}
      }
    ],
    "date": "2019-05-07",
    "categories": [],
    "contents": "\n\n\n\nUnpaywall, developed and maintained by the team of Impactstory, finds open access copies of scholarly literature (Piwowar et al. 2018). Providing DOIs to Unpaywall’s REST API not only returns open access full-text links, but also helpful metadata about the open access status of publications indexed in Crossref, a DOI registration agency. While the API allows to retrieve a limited amount of records, Unpaywall also offers database snapshots for large-scale analysis, which more and more bibliometric databases and open access monitoring services utilise. However, documentation about how database and service providers work with these dumps are hard to find.\nIn this blog post, we describe how we loaded Unpaywall’s data dump into Google BigQuery, a cloud-based service that allows fast analysis of large datasets, and how we interfaced BigQuery for our analysis with R. We wanted to know the extent of open access status information in Unpaywall, particularly, how this information can be utilised for bibliometric research. In our case, we intend to match open access status information from Unpaywall with the Web of Science in-house database from the German Competence Center for Bibliometrics to determine factors influencing open access publication activities among German Universities as part of our BMBF-funded research project OAUNI.\nStore and analyse large datasets with Google BigQuery\nThe Unpaywall data dump from February 2019 comprises more than 100 million records amounting to a file size of more than 100 GB. Working with datasets of such a large size is generally non-trivial. We therefore chose Google’s BigQuery as a cloud-based solution. From our perspective BigQuery has several advantages. Firstly, it is a highly performant tool enabling us to query and manipulate large datasets very fast. We would not be able to achieve a similarly satisfying performance with a local database deployed on our standard laptops. Secondly, using this cloud-based service gives us the possibility to share access to our database with colleagues and collaborators. Finally, the already existing interfaces to BigQuery from R allow us to incorporate this environment into our familiar data analytics workflow.\nAs a preparatory step, we loaded the entire dataset into a local Mongo DB database, exported relevant fields and rows for the study of the open access status of scholarly output as compressed JSON Lines files, and uploaded them to Google Cloud Storage. To import these files into BigQuery, we had to specify a schema, which we share in the source code repository of this blog. We used the BigQuery user interface, where the files are automatically decompressed and the corresponding tables are created.\n\nGoogle BigQuery is a paid service (with a large free contingent). If you would like to work with our access-restricted instance, please contact us.\nUnpaywall Overview\nIn R, we interface our Unpaywall dataset stored in Google BigQuery with the packages DBI and bigrquery.\n\n\n# connect to google bg where we imported the json lines Unpaywall dump\nlibrary(DBI)\nlibrary(bigrquery)\ncon <- dbConnect(\n  bigrquery::bigquery(),\n  project = \"api-project-764811344545\",\n  dataset = \"oadoi_full\"\n)\n\n\n\nOur BigQuery project has two tables, one containing all records between 2008 and 2012, and another for more recent works published since 2013. When connecting with tbl() from dplyr, Google asks us to login via a web browser or to supply a private access token to interface our access-restricted database.\n\n\nlibrary(dplyr)\nupw_08_12 <- tbl(con, \"feb_19_mongo_export_2008_2012_full_all_genres\")\nupw_13_19 <- tbl(con, \"feb_19_mongo_export_2013_Feb2019_full_all_genres\")\n\n\n\nbigrquery allows querying BigQuery tables using SQL or dplyr functions. The latter is convenient for us, because we have just started to learn SQL, but feel more experienced in the tidyverse, a popular collection of R packages following the Wickham-Grolemund approach to practise data science (Wickham and Grolemund 2017). Here’s an example where we call BigQuery with dplyr, which is part of the tidyverse, to obtain the first ten records from 2018. We restrict our search to journal articles, the most common genre in Unpaywall.\n\n\nlibrary(tidyverse)\nupw_13_19 %>%\n  filter(year == 2018, genre == \"journal-article\") %>%\n  head(10)\n\n\n#> # Source:   lazy query [?? x 13]\n#> # Database: BigQueryConnection\n#>     year genre     updated             published_date journal_is_in_d…\n#>    <int> <chr>     <dttm>              <date>         <lgl>           \n#>  1  2018 journal-… 2018-06-20 21:37:24 2018-01-01     FALSE           \n#>  2  2018 journal-… 2019-01-26 20:22:21 2018-05-07     TRUE            \n#>  3  2018 journal-… 2018-06-19 15:09:52 2018-03-23     TRUE            \n#>  4  2018 journal-… 2019-01-15 22:31:55 2018-01-12     FALSE           \n#>  5  2018 journal-… 2018-06-21 16:16:44 2018-04-01     FALSE           \n#>  6  2018 journal-… 2018-06-18 19:20:28 2018-05-10     TRUE            \n#>  7  2018 journal-… 2018-06-18 20:12:06 2018-01-01     FALSE           \n#>  8  2018 journal-… 2019-01-15 13:40:41 2018-01-04     FALSE           \n#>  9  2018 journal-… 2019-02-11 09:39:25 2018-12-14     TRUE            \n#> 10  2018 journal-… 2018-12-09 03:14:10 2018-12-03     TRUE            \n#> # … with 8 more variables: journal_is_oa <lgl>, journal_issns <chr>,\n#> #   oa_locations <list>, doi <chr>, is_oa <lgl>, publisher <chr>,\n#> #   journal_name <chr>, data_standard <int>\n\nNotice that our schema follows the Unpaywall data format. However, we excluded the large data field z-authors. Moreover, we did not consider the fields title, doi-url, which is redundant to the doi field, and best-oa-location, which is derived from the Open Access location object.\nIn this blog post, we will examine the following open access indicators:\nis_oa: A logical value indicating whether an open access version of the article was found or not.\njournal_is_in_doaj: A logical value indicating whether an article was published in a journal registered in the Directory of Open Access Journals (DOAJ).\nThe column oa_locations is a list-column that contains individual metadata about all open access full-text links found per article. By definition, open access provision is not limited to one route, but multiple copies of an article can be made freely available at the same time using various means (Suber 2012).\nHere are the three data variables from the oa_locations object that we will focus on:\nis_best: A logical value defined by Unpaywall’s algorithm that describes the most relevant open access location. The algorithm prioritises publisher-hosted content.\nhost_type: Is the open access full-text provided by a publisher or a repository?\nevidence: How did Unpaywall find the open access full-text?\nOpen Access availability (is_oa)\nTo start with, we retrieve the number and proportion of journal articles with open access full-text published between 2008 and 2018 using Unpaywall’s most basic open access indicator is_oa, a logical value, which is TRUE when at least one open access full-text was found. After matching and summarising the is_oa observations by year with dplyr, collect() from the dplyr database complement dbplyr loads the aggregated data from BigQuery into a local tibble. We use the lubridate package to transform the year variable to a date object.\n\n\nlibrary(lubridate)\noa_08_12 <- upw_08_12 %>%\n  # query and aggregate with dpylr \n  filter(genre == \"journal-article\") %>%\n  group_by(year, is_oa) %>%\n  summarise(n = n()) %>% \n  # load the data into a local tibble\n  collect()\noa_13_18 <- upw_13_19 %>%\n  # query and aggregate with dpylr\n  filter(genre == \"journal-article\", year < 2019) %>%\n  group_by(year, is_oa) %>%\n  summarise(n = n()) %>% \n  # load the data into a local tibble\n  collect()\nmy_df <- bind_rows(oa_08_12, oa_13_18) %>%\n  # calculate proportion per year\n  ungroup() %>%\n  mutate(year = lubridate::ymd(paste0(year, \"-01-01\"))) %>%\n  group_by(year, is_oa) %>%\n  summarise(n = sum(n)) %>%\n  mutate(prop = n / sum(n))\nmy_df\n\n\n#> # A tibble: 22 × 4\n#> # Groups:   year [11]\n#>    year       is_oa       n  prop\n#>    <date>     <lgl>   <int> <dbl>\n#>  1 2008-01-01 FALSE 1454745 0.711\n#>  2 2008-01-01 TRUE   590250 0.289\n#>  3 2009-01-01 FALSE 1562765 0.701\n#>  4 2009-01-01 TRUE   665951 0.299\n#>  5 2010-01-01 FALSE 1724760 0.697\n#>  6 2010-01-01 TRUE   749139 0.303\n#>  7 2011-01-01 FALSE 1585092 0.654\n#>  8 2011-01-01 TRUE   838014 0.346\n#>  9 2012-01-01 FALSE 1636130 0.630\n#> 10 2012-01-01 TRUE   962036 0.370\n#> # … with 12 more rows\n\nIn total, 31,159,960 journal articles published between 2008 and 2018 were included in Unpaywall. For 11,633,886 articles, Unpaywall was able to link a DOI to at least one freely available full-text (37 %). This means that around every third scholarly journal article published since 2008 is currently openly available.\nNext, let’s plot the prevalence of open access to journal articles over time using the data visualisation package ggplot2, which is also part of the tidyverse. To make our ggplot object interactive, we turn it into a plotly chart, a javascript library, using ggplotly(). The tooltip presents the total number and percentage for each category and year. We use the package scales to format the y-axis.\n\nTo learn more about plotly, we recommend the book “Interactive web-based data visualization with R, plotly, and shiny” from Carson Sievert. https://plotly-r.com/index.html\n\n\nlibrary(scales)\nplot_a <- my_df %>%\n  # prepare label that we want to present as tooltip\n  mutate(`Proportion in %` = round(prop * 100, 2)) %>%\n  ggplot(aes(year, n, label = `Proportion in %`)) +\n  geom_area(aes(fill = is_oa, group = is_oa),  alpha = 0.8) +\n  labs(x = \"Year published\", y = \"Journal Articles\",\n       title = \"Open Access to Journal Articles\") +\n  scale_fill_manual(\"Is OA?\",\n                    values = c(\"#b3b3b3a0\", \"#56B4E9\")) +\n  scale_x_date(date_labels = \"%y\") +\n  scale_y_continuous(labels = scales::number_format(big.mark = \" \")) +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(plot.margin = margin(30, 30, 30, 30)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.border = element_blank())\n# turn ggplot object into interactive plotly chart\nplotly::ggplotly(plot_a, tooltip = c(\"label\", \"y\")) \n\n\n\n\n\nFigure 1: Open access to journal articles according to Unpaywall. Blue area represents journal articles with at least one freely available full-text, grey area represents toll-access articles.\n\n\n\nWhile a general growth of journal articles and open access provision to them can be observed, there is a considerable decline in the number of journal articles published in 2018, presumably because of an indexing lag between Crossref and Unpaywall. The decline in open access full-text availability was even clearer, suggesting that some open access content is provided only after a certain period of time.\nUnpaywall Open Access Hosting Types (host_type)\nUsing Unpaywall’s open access location types allows for a more detailed analysis of open access provision. In the following, we explore the variable host_type, showing whether Unpaywall found the open access full-text on a publisher’s website or in a repository. Furthermore, we specifically highlight articles from fully open access journals that are indexed in the Directory of Open Access Journals (DOAJ) as indicated by the journal_is_in_doaj variable. As a start, we only examine the best open access location per DOI, is_best. As said before, this variable is defined by Unpaywall’s algorithm that prioritises publisher-hosted content.\nInstead of dplyr, we are now querying BigQuery with SQL. Before, we built and tested the SQL queries in the BigQuery user interface. The SQL code is stored in separate files (host_type_08_12.sql and host_type_13_18.sql), which we share in the source code repository of this blog.\n\n\nhost_type_08_12_query <- readLines(\"database/host_type_08_12.sql\") %>%\n  paste(collapse = \"\")\nhost_type_13_18_query <- readLines(\"database/host_type_13_18.sql\") %>%\n  paste(collapse = \"\")\n\n\n\nAfter calling BigQuery using SQL with the DBI interface, we bind the two resulting data frames into one. Using case_when() from dplyr, we create a host column distinguishing between “DOAJ-listed Journal,” “Other Journals” and “Repositories only” open access provision.\n\n\nhost_type_08_12_query_df <- dbGetQuery(con, host_type_08_12_query)\nhost_type_13_18_query_df <- dbGetQuery(con, host_type_13_18_query)\nhost_type_df <-\n  bind_rows(host_type_08_12_query_df, host_type_13_18_query_df) %>%\n  mutate(\n    host = case_when(\n      journal_is_in_doaj == TRUE ~ \"DOAJ-listed Journal\",\n      host_type == \"publisher\" ~ \"Other Journals\",\n      host_type == \"repository\" ~ \"Repositories only\"\n    )\n  ) %>%\n  mutate(year = lubridate::ymd(paste0(year, \"-01-01\")))\nhost_type_df\n\n\n#> # A tibble: 34 × 5\n#>    year       host_type  journal_is_in_doaj number_of_articles host   \n#>    <date>     <chr>      <lgl>                           <int> <chr>  \n#>  1 2011-01-01 publisher  TRUE                           170917 DOAJ-l…\n#>  2 2011-01-01 repository FALSE                          178323 Reposi…\n#>  3 2012-01-01 repository FALSE                          188662 Reposi…\n#>  4 2012-01-01 publisher  TRUE                           219293 DOAJ-l…\n#>  5 2012-01-01 publisher  FALSE                          554081 Other …\n#>  6 2008-01-01 publisher  FALSE                          358552 Other …\n#>  7 2008-01-01 publisher  TRUE                            81731 DOAJ-l…\n#>  8 2008-01-01 repository FALSE                          149967 Reposi…\n#>  9 2010-01-01 publisher  FALSE                          436375 Other …\n#> 10 2010-01-01 repository FALSE                          173419 Reposi…\n#> # … with 24 more rows\n\nTo explore our data, we follow Claus Wilke’s excellent book “Fundamentals of Data Visualization”(Wilke 2019) and visualise our proportions separately as parts of the total. Again, our final ggplot graphic is transformed to an interactive plotly chart.\n\n\n# calculate all oa articles per year\nall_articles <- host_type_df %>%\n  ungroup() %>%\n  group_by(year) %>%\n  summarise(number_of_articles = sum(number_of_articles))\n\nplot_b <-\n  ggplot(host_type_df, aes(x = year, y = number_of_articles, text = paste0(\"Publication year: \", lubridate::year(year)))) +\n  geom_bar(\n    data = all_articles,\n    aes(fill = \"All OA Articles\"),\n    color = \"transparent\",\n    stat = \"identity\"\n  ) +\n  geom_bar(aes(fill = \"by Host\"), color = \"transparent\", stat = \"identity\") +\n  facet_wrap( ~ host, nrow = 1) +\n  scale_fill_manual(values = c(\"#b3b3b3a0\", \"#56B4E9\"), name = \"\") +\n  labs(x = \"Year\", y = \"OA Articles (Total)\", title = \"Open Access to Journal Articles by Unpaywall host\") +\n  theme(legend.position = \"top\",\n        legend.justification = \"right\") +\n  scale_x_date(date_labels = \"%y\") +\n  scale_y_continuous(labels = scales::number_format(big.mark = \" \")) +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(plot.margin = margin(30, 30, 30, 30)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.border = element_blank())\n# turn ggplot object into interactive plotly chart\nplotly::ggplotly(plot_b, tooltip = c(\"y\", \"text\")) \n\n\n\n\n\nFigure 2: Open access to journal articles by open access hosting location. Colored bars represent the number of open access articles per host (“DOAJ-listed Journal,” “Other Journals,” “Repositories only”), grey bars the total number of journal articles indexed in Crossref, where Unpaywall was able to identify at least one openly available full-text.\n\n\n\nThe figure shows that most publisher-provided open access links were obtained from journals that were not indexed in the DOAJ: these are 6,531,822 articles, representing 56 % of all journal articles with openly available full-text identified by Unpaywall.\nWhile the number of publications in DOAJ-indexed journals is rising constantly, open access provided by other journal types and repositories declined from 2017 to 2018. Indeed, there is a considerable amount of journals that delay open access provision (Laakso and Björk 2013). A prominent example is the journal Cell where all articles are made freely available after an embargo period of twelve months. Also, self-archiving in repositories is often subject to embargo periods imposed by publishers, or researchers upload their publications later (Björk et al. 2013). A more detailed analysis of delayed open access, however, is challenging using Unpaywall data only, because Unpaywall has not tracked so far the point of time when articles were made open access.\nUnpaywall Open Access Evidence Types (evidence)\nThe evidence field of the oa_locations object contains more detailed open access status information. We use again SQL queries stored in separate files (evidence_08_12.sql and evidence_13_18.sql) to be found in the source code repository, and create a data.frame with the relevant fields.\n\n\nlibrary(tidyverse)\n# define queries\nevidence_08_12_query <- readLines(\"database/evidence_08_12.sql\") %>%\n  paste(collapse = \"\")\nevidence_13_18_query <- readLines(\"database/evidence_13_18.sql\") %>%\n  paste(collapse = \"\")\n# fetch records and bind them to one data frame\nevidence_08_12 <- dbGetQuery(con, evidence_08_12_query)\nevidence_13_18 <- dbGetQuery(con, evidence_13_18_query)\nevidence_df <- bind_rows(evidence_08_12, evidence_13_18) %>%\n  ungroup() %>%\n  mutate(year = lubridate::ymd(paste0(year, \"-01-01\")))\n\n\n\nThe evidence field indicates how Unpaywall found the article at a specific location and identified it as open access, for example via PubMed Central or via license information from Crossref.\nFor each evidence type we want to see how many articles were identified as open access in this way. To this end, we created the following table that shows the total number of articles per evidence type as well as their proportion and cumulative proportions with respect to the total number of all articles.\n\n\n# calculate numbers and proportion of articles per evidence type\nevidence_df %>%\n  group_by(evidence) %>%\n  summarize(N_records = sum(number_of_articles)) %>%\n  arrange(desc(N_records)) %>%\n  mutate(\n    prop = N_records / sum(N_records) * 100,\n    cum_prop = cumsum(prop)\n  ) -> articles_per_type_df\narticles_per_type_df %>%\n  knitr::kable(\n    col.names = c(\n      \"Evidence Types\",\n      \"Number of Articles\",\n      \"Proportion of all Articles in %\",\n      \"Cumulative Proportion in %\"\n    ),\n    big.mark = \",\",\n    caption = \"Number of articles per evidence type. Columns show the total number per evidence type, the proportion of articles of this type with respect to the number of all articles and the cumulative proportion of articles associated with any of the above evidence types.\"\n  )\n\n\nTable 1: Number of articles per evidence type. Columns show the total number per evidence type, the proportion of articles of this type with respect to the number of all articles and the cumulative proportion of articles associated with any of the above evidence types.\nEvidence Types\nNumber of Articles\nProportion of all Articles in %\nCumulative Proportion in %\nopen (via free pdf)\n4415372\n21.84\n22\nopen (via page says license)\n3404452\n16.84\n39\noa repository (via OAI-PMH doi match)\n3247290\n16.06\n55\noa journal (via doaj)\n2960898\n14.65\n69\noa repository (via OAI-PMH title and first author match)\n2704088\n13.38\n83\noa repository (via pmcid lookup)\n2103323\n10.41\n93\nopen (via crossref license)\n985336\n4.87\n98\nopen (via page says Open Access)\n126113\n0.62\n99\nopen (via crossref license, author manuscript)\n82079\n0.41\n99\noa journal (via publisher name)\n63234\n0.31\n99\noa repository (via OAI-PMH title and last author match)\n60535\n0.30\n100\noa repository (via OAI-PMH title match)\n57106\n0.28\n100\noa repository (via doi prefix)\n3583\n0.02\n100\noa journal (via issn in doaj)\n392\n0.00\n100\nmanual\n27\n0.00\n100\n\nIt can be seen that the long tail of the least frequent 8 categories at the bottom of the table with proportions smaller than 1 % of all articles only makes up 1.9 % of all articles in total, which is why we will aggregate these evidence types in the category Other in the following.\n\n\n# collate least frequent articles as 'Other'\narticles_per_type_df %>%\n  mutate(evidence = as_factor(evidence)) %>%\n  mutate(evidence_grouped = fct_relevel(fct_other(evidence, keep = evidence[.$prop > 1]), \"Other\")) %>%\n  group_by(evidence_grouped) %>%\n  summarize(number_of_articles = sum(N_records)) %>%\n  mutate(\n    prop = number_of_articles / sum(number_of_articles) * 100,\n    cum_prop = cumsum(number_of_articles) / sum(number_of_articles) *\n      100\n  ) -> articles_per_type_grouped_df\n# group according to categorization with \"Other\"\nevidence_grouped_df <- evidence_df %>%\n  mutate(evidence = as_factor(evidence)) %>%\n  mutate(\n    evidence_grouped = factor(\n      fct_other(evidence, keep = articles_per_type_grouped_df$evidence_grouped),\n      levels = articles_per_type_grouped_df$evidence_grouped\n    )\n  ) %>%\n  mutate(evidence_grouped = fct_relevel(fct_rev(evidence_grouped), \"Other\")) %>%\n  group_by(evidence_grouped, is_best, year) %>%\n  summarize(number_of_articles = sum(number_of_articles))\n\n\n\nSo far, we only examined the best open access location per DOI, indicated by is_best, a variable defined by Unpaywall algorithm that prioritises publisher-hosted content. However, evidence types in Unpaywall are not exclusive categories. On the contrary, many records are associated with several evidence types, because various ways to openly access full-texts have been found by Unpaywall. For this reason, the following figure distinguishes whether a given evidence type is classified as best open access location by Unpaywall or not. It is clearly visible that Unpaywall prioritises publisher hosted content (open, oa_journal) over repository depositions (oa_repository), as they state on their website. However, the figure also shows that an existing free pdf version on the publisher’s website (likely even without licensing information) is prioritised over journal level classifications as for example being indexed in DOAJ.\n\n\nevidence_grouped_df %>%\n  group_by(evidence_grouped, is_best) %>%\n  summarize(number_of_articles = sum(number_of_articles)) %>%\n#create plot\n  ggplot(aes(x = evidence_grouped, y = number_of_articles, fill = is_best)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#b3b3b3a0\", \"#56B4E9\"), name = \"Is best?\") +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(plot.margin = margin(30, 30, 30, 30)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  theme(panel.grid.major.y = element_blank()) +\n  theme(panel.border = element_blank()) +\n  coord_flip() +\n  labs(y = \"Number of Open Access Articles\", x = \"Evidence Type\",\n       title = \"Number of Open Access Articles per Unpaywall Evidence Type\") -> plot_ev_types_is_best\n#create interactive plot\nplotly::ggplotly(plot_ev_types_is_best, tooltip = c(\"y\"))\n\n\n\n\n\nFigure 3: Number of articles per evidence type. Least frequent evidence types are collated as category Other. In blue, the amount of articles where the corresponding evidence type is classified as best_oa_location by Unpaywall is shown.\n\n\n\nTo investigate the development of the most prevalent evidence types over time, we use a faceted graph. We again observe declines in the proportion of repository-based evidences which are chosen as best location.\n\n\n# use collated data frame\nevidence_grouped_df %>%\n  ggplot(aes(year, number_of_articles, fill = is_best, text = paste0(\"Publication year: \", lubridate::year(year)))) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap( ~ fct_rev(evidence_grouped), ncol = 2) +\n  scale_fill_manual(values = c(\"#b3b3b3a0\", \"#56B4E9\"), name = \"Is best?\") +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.border = element_blank()) +\n  scale_x_date(date_labels = \"%y\") +\n  labs(x = \"Publication Year\", y = \"Number of Open Access Articles\",\n       title = \"Unpaywall Open Access Evidence Categories per Year\") -> plot_ev_types_per_year\n#create interactive plot\nplotly::ggplotly(plot_ev_types_per_year, tooltip = c(\"y\", \"text\")) -> plotlyfacetfig\n# move y-axes label to the left to ensure readability\nplotlyfacetfig[['x']][['layout']][['annotations']][[2]][['x']] <- -0.08\nplotlyfacetfig\n\n\n\n\n\nFigure 4: Development of the number of articles per evidence type over time. Least frequent evidence types are collated as category Other. For each type the total number of articles per year is shown for publication years from 2008 to 2018. In blue, the amount of articles where the corresponding evidence type is classified as best_oa_location by Unpaywall is highlighted.\n\n\n\nOverlap of Open Access Provision and Evidence Types\nMany open access articles are accessible through a number of locations, including the publisher’s website and also one or more open access repositories. Unpaywall not only describes one, but all open access full-texts it discovers with useful metadata. In the following, we will analyse if and to which extent the various open access indicators intersect. We start with an analysis of the overlap between host types, followed by determining set intersections between Unpaywall’s evidence types.\nOverlap between Host Types\nTo present articles that are both provided by publishers and repositories, we use the BigQuery SQL function STRING_AGG to create a new variable where we concatenate the different host_types per open access article (for more details, see the full SQL queries host_type_intersect_08_12.sql and host_type_intersect_13_18.sql.\n\n\nhost_type_intersect_08_12_query <- readLines(\"database/host_type_intersect_08_12.sql\") %>%\n  paste(collapse = \"\")\nhost_type_intersect_13_18_query <- readLines(\"database/host_type_intersect_13_18.sql\") %>%\n  paste(collapse = \"\")\n\n\n\nAgain, we call BigQuery and load the aggregated data into our local R session. This time, we do not want to present the total number of open access publications, but its relative share. We already obtained the total number of articles, which are stored in the my_df data.frame.\n\n\nhost_type_08_12_intersect_df <-\n  dbGetQuery(con, host_type_intersect_08_12_query)\nhost_type_13_18_intersect_df <-\n  dbGetQuery(con, host_type_intersect_13_18_query)\nhost_type_intersect <-\n  bind_rows(host_type_08_12_intersect_df, host_type_13_18_intersect_df) %>%\n  mutate(year = lubridate::ymd(paste0(year, \"-01-01\"))) %>%\n  mutate(\n    host = case_when(\n      host_type_count == \"publisher\" ~ \"Publisher only\",\n      host_type_count == \"publisher,repository\" ~ \"Publisher & Repository\",\n      host_type_count == \"repository\" ~ \"Repositories only\"\n    )\n  ) %>%\n  mutate(host = factor(\n    host,\n    levels = c(\"Publisher only\", \"Publisher & Repository\", \"Repositories only\")\n  ))\n# obtain yearly publication volumes\nhost_type_intersect <- my_df %>%\n  group_by(year) %>%\n  summarise(all_articles = sum(n)) %>%\n  # join with host type figures\n  right_join(host_type_intersect, by = \"year\") %>%\n  # calculate proportion\n  mutate(prop = number_of_articles / all_articles)\nhost_type_intersect\n\n\n#> # A tibble: 33 × 6\n#>    year       all_articles host_type_count      number_of_artic… host \n#>    <date>            <int> <chr>                           <int> <fct>\n#>  1 2008-01-01      2044995 publisher                      312392 Publ…\n#>  2 2008-01-01      2044995 repository                     149967 Repo…\n#>  3 2008-01-01      2044995 publisher,repository           127891 Publ…\n#>  4 2009-01-01      2228716 publisher                      347502 Publ…\n#>  5 2009-01-01      2228716 repository                     165372 Repo…\n#>  6 2009-01-01      2228716 publisher,repository           153077 Publ…\n#>  7 2010-01-01      2473899 publisher                      396449 Publ…\n#>  8 2010-01-01      2473899 publisher,repository           179271 Publ…\n#>  9 2010-01-01      2473899 repository                     173419 Repo…\n#> 10 2011-01-01      2423106 publisher                      444220 Publ…\n#> # … with 23 more rows, and 1 more variable: prop <dbl>\n\nLet’s visualise the host type distribution including the overlap between publisher and repository-provided open access:\n\n\n# get overall oa share\nhost_type_all <- host_type_intersect %>%\n  group_by(year) %>%\n  summarise(prop = sum(prop))\n# make a ggplot graphic\nplot_host_intersect <-\n  ggplot(host_type_intersect, aes(x = year, y = prop, text = paste0(\"Publication year: \", year(year)))) +\n  geom_bar(\n    data = host_type_all,\n    aes(fill = \"All OA Articles\"),\n    color = \"transparent\",\n    stat = \"identity\"\n  ) +\n  geom_bar(aes(fill = \"by Host\"), color = \"transparent\", stat = \"identity\") +\n  facet_wrap(~ host, nrow = 1) +\n  scale_fill_manual(values = c(\"#b3b3b3a0\", \"#56B4E9\"), name = \"\") +\n  labs(x = \"Year\", y = \"OA Share\",\n       title = \"Overlap between Open Access Host Types in Unpaywall\") +\n  scale_x_date(date_labels = \"%y\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 5L)) +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(plot.margin = margin(30, 30, 30, 30)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.border = element_blank())\n# turn ggplot object into interactive plotly chart\nplotly::ggplotly(plot_host_intersect, tooltip = c(\"y\", \"text\")) \n\n\n\n\n\nFigure 5: Open access to journal articles by open access hosting location. Coloured bars represent the number of open access articles per Unpaywall host category: “publisher” and “repository,” grey bars the percentage of open access to journal articles indexed in Crossref from Unpaywall. Because open access provision is not mutually exclusive, the overlap between “publisher” and “repository” hosted open access full-texts is also shown.\n\n\n\nThe figure shows that Unpaywall found most open access full-text on publishers’ websites (82%). 56 % of all open access full-texts was not archived in a repository. The overlap of open access provided by both routes also deserves attention: a proportion of around 26 % of all open access articles was accessible from both publishers’ websites and repositories.\nOverlaps between Evidence types\nThe categorisation of evidence types is not exclusive, either. Therefore, many records will be associated with more than one evidence type.\nTo better understand this overlap, we generate a new column specifying all found combinations of evidence types using concatenation with the SQL function STRING_AGG. The SQL queries we use (evidence_single_cat_08_12.sql and evidence_single_cat_13_18.sql) can be found in the source code repository.\n\n\nlibrary(tidyverse)\n# define queries\nevidence_single_cat_08_12_query <- readLines(\"database/evidence_single_cat_08_12.sql\") %>%\n  paste(collapse = \"\")\nevidence_single_cat_13_18_query <- readLines(\"database/evidence_single_cat_13_18.sql\") %>%\n  paste(collapse = \"\")\n# fetch records and bind them to one data frame\nevidence_categories_08_12 <- dbGetQuery(con, evidence_single_cat_08_12_query)\nevidence_categories_13_18 <- dbGetQuery(con, evidence_single_cat_13_18_query)\nevidence_categories_df <- bind_rows(evidence_categories_08_12, evidence_categories_13_18) %>%\n  group_by(ev_cat) %>%\n  summarize(number_of_articles = sum(number_of_articles)) %>%\n  arrange(desc(number_of_articles))\nevidence_categories_df\n\n\n#> # A tibble: 411 × 2\n#>    ev_cat                                             number_of_artic…\n#>    <chr>                                                         <int>\n#>  1 open (via free pdf)                                         3290683\n#>  2 oa repository (via OAI-PMH title and first author…           945958\n#>  3 open (via page says license)                                 910028\n#>  4 oa journal (via doaj)&open (via page says license)           687962\n#>  5 open (via crossref license)                                  546154\n#>  6 oa journal (via doaj)                                        515453\n#>  7 oa journal (via doaj)&oa repository (via OAI-PMH …           513620\n#>  8 oa repository (via OAI-PMH doi match)&oa reposito…           462086\n#>  9 oa repository (via OAI-PMH doi match)                        462071\n#> 10 oa repository (via OAI-PMH doi match)&oa reposito…           260696\n#> # … with 401 more rows\n\nWe first illustrate for each evidence type - collating again the least frequent types in the category Other - the amount of articles which corresponds exclusively to this type and no others.\n\n\n#determine number of articles corresponding only to one evidence type\nevidence_single_cat_df <- evidence_df %>%\n  group_by(evidence) %>%\n  summarize(number_of_articles = sum(number_of_articles)) %>%\n  left_join(evidence_categories_df, by = c(\"evidence\" = \"ev_cat\")) %>%\n  rename(number_of_articles = number_of_articles.x, number_of_single_cat = number_of_articles.y) %>%\n  mutate(number_of_articles = replace_na(number_of_articles, 0),\n         number_of_single_cat = replace_na(number_of_single_cat, 0))\n#aggregate least frequent types as category `Other`\nevidence_single_cat_grouped_df <- evidence_single_cat_df %>%\n  ungroup() %>%\n  mutate(evidence = as_factor(evidence)) %>%\n  mutate(\n    evidence_grouped = factor(\n      fct_other(evidence, keep = articles_per_type_grouped_df$evidence_grouped),\n      levels = articles_per_type_grouped_df$evidence_grouped\n    )\n  ) %>%\n  mutate(evidence_grouped = fct_relevel(fct_rev(evidence_grouped), \"Other\")) %>%\n  group_by(evidence_grouped) %>%\n  summarize(\n    number_of_articles = sum(number_of_articles),\n    number_of_single_cat = sum(number_of_single_cat)\n  ) %>%\n  #arrange data in order to enable stacked barplots\n  mutate(multiple = number_of_articles-number_of_single_cat, single = number_of_single_cat) %>%\n  select(evidence_grouped, single, multiple) %>%\n  gather(is_single, number_of_articles, -evidence_grouped) %>%\n  mutate(is_single = case_when(\n    is_single == \"single\" ~ TRUE,\n    is_single == \"multiple\" ~ FALSE\n    )) %>%\n    #rename column for correct display in plot\n  rename(proportion = number_of_articles)\n#create aggregated proportions barplot\nevidence_single_cat_grouped_df %>%\n  ggplot(aes(x = evidence_grouped, y = proportion, fill = is_single)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  scale_fill_manual(values = c(\"#b3b3b3a0\", \"#56B4E9\"), name = \"Is unique?\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal(base_family = \"Roboto\") +\n  theme(plot.margin = margin(30, 30, 30, 30)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  theme(panel.grid.major.y = element_blank()) +\n  theme(panel.border = element_blank()) +\n  coord_flip() +\n  theme(legend.position = \"top\",\n        legend.justification = \"right\") +\n  labs(y = \"Proportion of Articles\", x = \"Evidence Type\",\n       title = \"Proportion of Articles per Evidence Type\") -> plot_ev_types_is_single_prop\n#create interactive plot\nplotly::ggplotly(plot_ev_types_is_single_prop, tooltip = c(\"y\"))\n\n\n\n\n\nFigure 6: Proportion of articles per evidence type. In blue, the amount of articles uniquely associated with the corresponding evidence type is shown.\n\n\n\nIt is interesting to see, that the evidence type which appears most often as a unique form of open access provision is via an openly available pdf on the publisher’s website, meaning that no other evidence, like license information from Crossref, was found. This confirms that Unpaywall’s open access detection has benefited from scraping publishers’ websites.\nMoreover, a critical amount of articles is found only through repository-based evidence sources and hence, is available only via the green route. Still, the figure shows a phenomenon that we observed also for the host_type, namely that repository-based evidence types often overlap with other evidence types.\nBecause of Unpaywall’s prioritisation of publisher-provided open access, caution is in order, when only the best_oa_location is used for categorisation: there are publisher-based evidence types that may not comply with how funders define open access journals, particularly with regard to license statements. On the other hand, a large number of articles seems to be identified as open access only through license information from Crossref without an associated free pdf having been found. However, we are unsure whether Unpaywall performs all open access identification procedures for every single record indexed in Crossref, which would allow for such a comparison.\nVisualising the occurring intersections of multiple evidence types is difficult. Following (Alexander Lex and Gehlenborg 2014) and (A. Lex et al. 2014), we created an UpSet figure using the UpSetR package described in (Conway, Lex, and Gehlenborg 2017) in order to examine in more detail, which of the 411 occurring combinations of evidence types - including singletons - shows up most frequently and how large these groups are. In theory, for the 15 evidence types up to 32,768 combinations would be possible.\n\nA very good introduction into the usage of UpSetR is given by the Basic Usage Vignette.\nTo start with, we interface BigQuery and retrieve how often the combinations of evidence types occur. We store the corresponding SQL queries (evidence_overlap_08_12.sql and evidence_overlap_13_18.sql) and share the resulting dataset as .csv files (results_evidence_overlap_08_12.csv and results_evidence_overlap_13_18.csv). Next, we transform the data to an upsetr-compatible expression format, resulting in a named character vector. Lastly, the upset() function generates the graph. To keep the resulting figure readable, we only display the 15 combinations of the 7 most frequent types with the highest number of articles each.\n\n\n\nlibrary(UpSetR)\n# fetch data for upset graph\nevidence_categories_upset_08_12_query <- readLines(\"database/evidence_overlap_08_12.sql\") %>%\n  paste(collapse = \"\")\nevidence_categories_upset_13_18_query <- readLines(\"database/evidence_overlap_13_18.sql\") %>%\n  paste(collapse = \"\")\nevidence_categories_upset_08_12 <- dbGetQuery(con, evidence_categories_upset_08_12_query)\n# export to csv\nwrite_csv(evidence_categories_upset_08_12, \"data/results_evidence_overlap_08_12.csv\")\nevidence_categories_upset_13_18 <- dbGetQuery(con, evidence_categories_upset_13_18_query)\n# export to csv\nwrite_csv(evidence_categories_upset_13_18, \"data/results_evidence_overlap_13_18.csv\")\nevidence_categories_upset_df <- bind_rows(evidence_categories_upset_08_12, evidence_categories_upset_13_18) %>%\n  group_by(ev_cat) %>%\n  summarise(n = sum(number_of_articles))\n# list with counts\nevidence_categories_upset_list <- as.list(evidence_categories_upset_df$n)\n# categories as list names\nnames(evidence_categories_upset_list) <- evidence_categories_upset_df$ev_cat\n# convert to vector\nevidence_categories_upset_expr <- unlist(evidence_categories_upset_list)\nupset(fromExpression(evidence_categories_upset_expr), nsets = 7, nintersects = 15, order.by = \"freq\", show.numbers = FALSE, set_size.angles = 25)\n\n\n\n\nFigure 7: Most frequent combinations of evidence types. The barplot on the left displays the total number of articles per evidence type (“Set Size”). The central barplot shows the number of articles per overlap category (“Intersection Size”). Which evidence types contribute to each intersection is given by the black dots in the chart below.\n\n\n\nDiscussion and Conclusion\nIn this blog post, we demonstrated how to analyse the Unpaywall data dump with Google BigQuery and R. Interfacing BigQuery with R has allowed us to integrate a high-performance and user-friendly database environment into our R data analytics workflow. Using this data management environment, we found 11,6 million journal articles published between 2008 and 2018 with open access full-texts, representing around one third of all articles indexed in Crossref for this period. Moreover, we found Unpaywall to be a suitable data source for open access analytics, because Unpaywall does not only tag if a publication is freely available, but also provides metadata describing how and where the open access full-text links were discovered.\nOur Unpaywall data analysis revealed various open access location and evidence types, as well as large overlaps between them. Along with the likely influence of embargoed or delayed open access provision on some of these types, our analysis raises important questions about how to responsibly use Unpaywall data in bibliometric research and open access monitoring. Examining Unpaywall’s best open access location only, favours publisher-provided open access, which, in turn, means that open access provided by repositories would be underestimated. Likewise, large overlaps between evidence categories can be observed. To allow for careful consideration, bibliometric research and open access monitoring must therefore be clear about how open access indicators were derived from Unpaywall.\nIn future, we will use these insights from our data analysis to work on a matching procedure between Unpaywall and the Web of Science in-house database from the German Competence Center for Bibliometrics in our OAUNI project. In doing so, we want to represent Unpaywall’s open access evidence as comprehensive as possible to allow for a pluralist view on open access to journal articles from researchers affiliated with German Universities.\nAcknowledgments\nWe are very grateful for helpful feedback on this post and ongoing discussions on the usage of Unpaywall data for bibliometric analyses with our project partners in Bielefeld, Niels Taubert and Elham Iravani, the related project OASE, in particular Nicholas Fraser and Philipp Mayr-Schlegel, Stephan Stahlschmidt and Aliakbar Akbaritabar from the DZHW, as well as Daniel Bangert and Birgit Schmidt from the SUB Göttingen.\nWe acknowledge financial support from the the Federal Ministry of Education and Research of Germany (BMBF) in the framework Quantitative research on the science sector (Project: “OAUNI Entwicklung und Einflussfaktoren des Open-Access-Publizierens an Universitäten in Deutschland,” Förderkennzeichen: 01PU17023A).\n\n\n\nBjörk, Bo-Christer, Mikael Laakso, Patrik Welling, and Patrik Paetau. 2013. “Anatomy of Green Open Access.” Journal of the Association for Information Science and Technology 65 (2): 237–50. https://doi.org/10.1002/asi.22963.\n\n\nConway, Jake R., Alexander Lex, and Nils Gehlenborg. 2017. “UpSetR: An r Package for the Visualization of Intersecting Sets and Their Properties.” Bioinformatics 33 (18): 2938–40. https://doi.org/10.1093/bioinformatics/btx364.\n\n\nLaakso, Mikael, and Bo-Christer Björk. 2013. “Delayed Open Access: An Overlooked High-Impact Category of Openly Available Scientific Literature.” Journal of the American Society for Information Science and Technology 64 (7): 1323–29. https://doi.org/10.1002/asi.22856.\n\n\nLex, A., N. Gehlenborg, H. Strobelt, R. Vuillemot, and H. Pfister. 2014. “UpSet: Visualization of Intersecting Sets.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nLex, Alexander, and Nils Gehlenborg. 2014. “Points of View: Sets and Intersections.” Nature Methods 11 (July): 779. https://doi.org/10.1038/nmeth.3033.\n\n\nPiwowar, Heather, Jason Priem, Vincent Larivière, Juan Pablo Alperin, Lisa Matthias, Bree Norlander, Ashley Farley, Jevin West, and Stefanie Haustein. 2018. “The State of OA: A Large-Scale Analysis of the Prevalence and Impact of Open Access Articles.” PeerJ 6: e4375. https://doi.org/10.7717/peerj.4375.\n\n\nSuber, Peter. 2012. Open Access. MIT Press. https://mitpress.mit.edu/books/open-access.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc. https://r4ds.had.co.nz/.\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media, Inc. https://serialmentor.com/dataviz/.\n\n\n\n\n",
    "preview": "posts/unpaywall_evidence/distill-preview.png",
    "last_modified": "2023-11-01T09:08:02+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
