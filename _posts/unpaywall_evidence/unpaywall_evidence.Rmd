---
title: "Open Access Evidence in Unpaywall"
description: |
  We investigated more than 31 million scholarly journal articles published between 2008-2018 that are indexed in Unpaywall, a widely used open access discovery tool. Using Google BigQuery and R, we determined over 11,6 million journal articles with open access full-text links in Unpaywall, corresponding to an open access share of 37 %. Our data analysis revealed various open access location and evidence types, as well as large overlaps between them, raising important questions about how to responsibly re-use Unpaywall data in bibliometric research and open access monitoring.
author:
  - name: Najko Jahn 
    url: https://twitter.com/najkoja
    affiliation: State and University Library Göttingen
    affiliation_url: https://www.sub.uni-goettingen.de/
  - name: Anne Hobert
    affiliation: State and University Library Göttingen
    affiliation_url: https://www.sub.uni-goettingen.de/
date: "`r Sys.Date()`"
creative_commons: CC BY
output: distill::distill_article
bibliography: pubs.bib
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE
)
options(scipen = 999, digits = 2)
knitr::knit_hooks$set(
  inline = function(x) {
    if (is.numeric(x)) {
      return(prettyNum(x, big.mark = ","))
    } else{
      return(x)
    }
  }
)
```

[Unpaywall](https://unpaywall.org/), developed and maintained by the [team of Impactstory](http://impactstory.org/team), finds open access copies of scholarly literature [@Piwowar_2018]. Providing DOIs to [Unpaywall's REST API](https://unpaywall.org/products/api) not only returns open access full-text links, but also helpful metadata about the open access status of publications indexed in [Crossref](https://www.crossref.org/), a DOI registration agency. While the API allows to retrieve a limited amount of records, Unpaywall also offers [database snapshots](https://unpaywall.org/products/snapshot) for large-scale analysis, which more and more bibliometric databases and open access monitoring services utilise. However, documentation about how database and service providers work with these dumps are hard to find.

In this blog post, we describe how we loaded Unpaywall's data dump into [Google BigQuery](https://cloud.google.com/bigquery/), a cloud-based service that allows fast analysis of large datasets, and how we interfaced BigQuery for our analysis with R. We wanted to know the extent of open access status information in Unpaywall, particularly, how this information can be utilised for bibliometric research. In our case, we intend to match open access status information from Unpaywall with the Web of Science in-house database from the [German Competence Center for Bibliometrics](http://www.bibliometrie.info/) to determine factors influencing open access publication activities among German Universities as part of our [BMBF-funded research project OAUNI](https://www.wihoforschung.de/de/oauni-2182.php).

## Store and analyse large datasets with Google BigQuery 

The most recent Unpaywall dataset from February 2019 comprises more than 100 million records amounting to a file size of more than 100 GB. Working with datasets of such a large size is generally non-trivial. We therefore chose Google's BigQuery as a cloud-based solution. From our perspective it has several advantages. Firstly, it is a highly performant tool enabling us to obtain query results of large databases very fast. We would not be able to achieve similarly satisfying performance with a local database deployed on our standard laptops. Secondly, using this cloud-based service gives us the possibility to share access to our database with colleagues and collaborators. Finally, the already existing interfaces to BigQuery from R allow us to incorporate this environment into our familiar data analytics workflow.

As a preparatory step, we loaded the entire dataset into a local [Mongo DB database](https://www.mongodb.com), exported relevant fields and rows for the study of the open access status of scholarly output as compressed JSON Lines files, and uploaded them to [Google Cloud Storage](https://cloud.google.com/storage/). To import these files into [BigQuery](https://cloud.google.com/bigquery/), we had to specify a [schema](database/bq_schema.json), which we share in the source code repository of this blog. We used the BigQuery user interface, where the files are automatically decompressed and the corresponding tables are created.

<aside>
  Google BigQuery is a paid service (with a large free contingent, however). If you would like to work with our access-restricted instance, please contact us.
</aside>

## Unpaywall Overview

In R, we interface our Unpaywall dataset stored in Google BigQuery with the packages [DBI](https://www.r-dbi.org/) and [bigrquery](https://github.com/r-dbi/bigrquery).


```{r fetch_bq}
# connect to google bg where we imported the json lines Unpaywall dump
library(DBI)
library(bigrquery)
con <- dbConnect(
  bigrquery::bigquery(),
  project = "api-project-764811344545",
  dataset = "oadoi_full"
)
```

Our BigQuery project has two tables, one containing all records between 2008 and 2012, and another for more recent works published since 2013. When connecting with `tbl()` from [dplyr](https://dplyr.tidyverse.org/), Google asks us to login via a web browser or to supply a private access token to interface our access-restricted database. 

```{r}
library(dplyr)
upw_08_12 <- tbl(con, "feb_19_mongo_export_2008_2012_full_all_genres")
upw_13_19 <- tbl(con, "feb_19_mongo_export_2013_Feb2019_full_all_genres")
```

[bigrquery](https://github.com/r-dbi/bigrquery) allows querying BigQuery tables using SQL or [dplyr](https://dplyr.tidyverse.org/) functions. The latter is convenient for us, because we have just started to learn SQL, but feel more experienced in the [tidyverse](https://www.tidyverse.org/), a popular collection of R packages following the Wickham-Grolemund approach to practise data science [@Wickham_2017]. Here's an example where we call BigQuery with dplyr, which is part of the tidyverse, to obtain the first ten records from 2018. We restrict our search to journal articles, the most common genre in Unpaywall.

```{r}
library(tidyverse)
upw_13_19 %>%
  filter(year == 2018, genre == "journal-article") %>%
  head(10)
```

Notice that [our schema](database/bq_schema.json) follows the [Unpaywall data format](https://unpaywall.org/data-format). However, we excluded the large data field `z-authors`. Moreover, we did not consider the fields `title`, `doi-url`, which is redundant to the `doi` field, and `best-oa-location`, which is derived from the Open Access location object.  

In this blog post, we will examine the following open access indicators: 

- `is_oa`: A logical value indicating whether an open access version of the article was found or not.

- `journal_is_in_doaj`: A logical value indicating whether an article was published in a journal registered in the [Directory of Open Access Journals (DOAJ)](https://doaj.org/).

The column  `oa_locations` is a [list-column](https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html) that contains individual metadata about all open access full-text links found per article. By definition, open access provision is not limited to one route, but multiple copies of an article can be made freely available at the same time using various means [@Suber_2012].

Here are the three data variables from the `oa_locations` object that we will focus on:

- `is_best`: A logical value defined by Unpaywall's algorithm that describes the most relevant open access location. The algorithm prioritises publisher-hosted content.

- `host_type`: Is the open access full-text provided by a publisher or a repository? 

- `evidence`: How did Unpaywall find the open access full-text?

### Open Access availability (`is_oa`)

To start with, we retrieve the number and proportion of journal articles with open access full-text published between 2008 and 2018 using Unpaywall's most basic open access indicator `is_oa`, a logical value, which is `TRUE` when at least one open access full-text was found. After matching and summarising the `is_oa` observations by year with dplyr, `collect()` from the dplyr database complement [dbplyr](https://cran.r-project.org/web/packages/dbplyr/index.html) loads the aggregated data from BigQuery into a local tibble. We use the [lubridate package](https://lubridate.tidyverse.org/) to transform the year variable to a date object.

```{r cache=TRUE}
library(lubridate)
oa_08_12 <- upw_08_12 %>%
  # query and aggregate with dpylr 
  filter(genre == "journal-article") %>%
  group_by(year, is_oa) %>%
  summarise(n = n()) %>% 
  # load the data into a local tibble
  collect()
oa_13_18 <- upw_13_19 %>%
  # query and aggregate with dpylr
  filter(genre == "journal-article", year < 2019) %>%
  group_by(year, is_oa) %>%
  summarise(n = n()) %>% 
  # load the data into a local tibble
  collect()
my_df <- bind_rows(oa_08_12, oa_13_18) %>%
  # calculate proportion per year
  ungroup() %>%
  mutate(year = lubridate::ymd(paste0(year, "-01-01"))) %>%
  group_by(year, is_oa) %>%
  summarise(n = sum(n)) %>%
  mutate(prop = n / sum(n))
my_df
```

In total, `r sum(my_df$n)` journal articles published between 2008 and 2018 were included in Unpaywall. For `r  my_df %>% filter(is_oa == TRUE) %>% .$n %>% sum()` articles, Unpaywall was able to link a DOI to at least one freely available full-text (`r  my_df %>% filter(is_oa == TRUE) %>% .$n %>% sum() / sum(my_df$n) * 100` %). This means that around every third scholarly journal article published since 2008 is currently openly available. 

Next, let's plot the prevalence of open access to journal articles over time using the data visualisation package [ggplot2](https://ggplot2.tidyverse.org/index.html), which is also part of the tidyverse. To make our ggplot object interactive, we turn it into a [plotly](https://plot.ly/) chart, a javascript library, using `ggplotly()`. The tooltip presents the total number and percentage for each category and year. We use the package [scales](https://cran.r-project.org/web/packages/scales/index.html) to format the y-axis.

<aside>
  To learn more about plotly, we recommend the book "Interactive web-based data visualization with R, plotly, and shiny" from Carson Sievert. <https://plotly-r.com/index.html>
</aside>

```{r, layout="l-page", fig.cap="Open access to journal articles according to Unpaywall. Blue area represents journal articles with at least one freely available full-text, grey area represents toll-access articles."}
library(scales)
plot_a <- my_df %>%
  # prepare label that we want to present as tooltip
  mutate(`Proportion in %` = round(prop * 100, 2)) %>%
  ggplot(aes(year, n, label = `Proportion in %`)) +
  geom_area(aes(fill = is_oa, group = is_oa),  alpha = 0.8) +
  labs(x = "Year published", y = "Journal Articles",
       title = "Open Access to Journal Articles") +
  scale_fill_manual("Is OA?",
                    values = c("#b3b3b3a0", "#56B4E9")) +
  scale_x_date(date_labels = "%y") +
  scale_y_continuous(labels = scales::number_format(big.mark = " ")) +
  theme_minimal(base_family = "Roboto") +
  theme(plot.margin = margin(30, 30, 30, 30)) +
  theme(panel.grid.minor = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(panel.grid.major.x = element_blank()) +
  theme(panel.border = element_blank())
# turn ggplot object into interactive plotly chart
plotly::ggplotly(plot_a, tooltip = c("label", "y")) 
```

While a general growth of journal articles and open access provision to them can be observed, there is a considerable decline in the number of journal articles published in 2018, presumably because of an indexing lag between Crossref and Unpaywall. The decline in open access full-text availability was even clearer, suggesting that some open access content is provided only after a certain period of time. 

### Unpaywall Open Access Hosting Types (`host_type`)

Using Unpaywall's open access location types allows for a more detailed analysis of open access provision. In the following, we explore the variable `host_type`, showing whether Unpaywall found the open access full-text on a publisher website or in a repository. Furthermore, we specifically highlight articles from fully open access journals that are indexed in the [Directory of Open Access Journals (DOAJ)](https://doaj.org/) as indicated by the `journal_is_in_doaj` variable. As a start, we only examine the best open access location per DOI, `is_best`. As said before, this variable is defined by Unpaywall's algorithm that prioritises publisher-hosted content.

Instead of dplyr, we are now querying BigQuery with SQL. Before, we built and tested the SQL queries in the BigQuery user interface. The SQL code is stored in separate files ([host_type_08_12.sql](database/host_type_08_12.sql) and [host_type_13_18.sql](database/host_type_13_18.sql)), which we share in the source code repository of this blog.

```{r}
host_type_08_12_query <- readLines("database/host_type_08_12.sql") %>%
  paste(collapse = "")
host_type_13_18_query <- readLines("database/host_type_13_18.sql") %>%
  paste(collapse = "")
```

After calling BigQuery using SQL with the [DBI](https://www.r-dbi.org/) interface, we bind the two resulting data frames into one. Using `case_when()`from dplyr, we create a `host` column distinguishing between "DOAJ-listed Journal", "Other Journals" and "Repositories only" open access provision.

```{r}
host_type_08_12_query_df <- dbGetQuery(con, host_type_08_12_query)
host_type_13_18_query_df <- dbGetQuery(con, host_type_13_18_query)
host_type_df <-
  bind_rows(host_type_08_12_query_df, host_type_13_18_query_df) %>%
  mutate(
    host = case_when(
      journal_is_in_doaj == TRUE ~ "DOAJ-listed Journal",
      host_type == "publisher" ~ "Other Journals",
      host_type == "repository" ~ "Repositories only"
    )
  ) %>%
  mutate(year = lubridate::ymd(paste0(year, "-01-01")))
host_type_df
```

To explore our data, we follow [Claus Wilke's excellent book "Fundamentals of Data Visualization"](https://serialmentor.com/dataviz/)[@Wilke_2019] and [visualise our proportions separately as parts of the total](https://serialmentor.com/dataviz/visualizing-proportions.html#visualizing-proportions-separately-as-parts-of-the-total). Again, our final ggplot graphic is transformed to an interactive plotly chart.

```{r, layout="l-page", fig.cap='Open access to journal articles by open access hosting location. Colored bars represent the number of open access articles per host ("DOAJ-listed Journal", "Other Journals", "Repositories only"), grey bars the total number of journal articles indexed in Crossref, where Unpaywall was able to identify at least one openly available full-text.'}
# calculate all oa articles per year
all_articles <- host_type_df %>%
  ungroup() %>%
  group_by(year) %>%
  summarise(number_of_articles = sum(number_of_articles))

plot_b <-
  ggplot(host_type_df, aes(x = year, y = number_of_articles)) +
  geom_bar(
    data = all_articles,
    aes(fill = "All OA Articles"),
    color = "transparent",
    stat = "identity"
  ) +
  geom_bar(aes(fill = "by Host"), color = "transparent", stat = "identity") +
  facet_wrap( ~ host, nrow = 1) +
  scale_fill_manual(values = c("#b3b3b3a0", "#56B4E9"), name = "") +
  labs(x = "Year", y = "OA Articles (Total)", title = "Open Access to Journal Articles by Unpaywall host") +
  theme(legend.position = "top",
        legend.justification = "right") +
  scale_x_date(date_labels = "%y") +
  scale_y_continuous(labels = scales::number_format(big.mark = " ")) +
  theme_minimal(base_family = "Roboto") +
  theme(plot.margin = margin(30, 30, 30, 30)) +
  theme(panel.grid.minor = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(panel.grid.major.x = element_blank()) +
  theme(panel.border = element_blank())
# turn ggplot object into interactive plotly chart
plotly::ggplotly(plot_b, tooltip = c("y")) 
```

The figure shows that most publisher-provided open access links were obtained from journals that were not indexed in the DOAJ: these are`r host_type_df %>% filter(host == "Other Journals") %>% .$number_of_articles %>% sum()` articles, representing `r host_type_df %>% filter(host == "Other Journals") %>% .$number_of_articles %>% sum() / host_type_df %>% .$number_of_articles %>% sum() * 100` % of all journal articles with openly available full-text identified by Unpaywall. 

While the number of publications in DOAJ-indexed journals is rising constantly, open access provided by other journal types and repositories declined from 2017 to 2018. Indeed, there is a considerable amount of journals that delay open access provision [@Laakso_2013]. A prominent example is the journal [Cell](https://www.cell.com/cell/archive) where all articles are made freely available after an embargo period of twelve months. Also, self-archiving in repositories is often subject to embargo periods imposed by publishers, or researchers upload their publications later [@Bj_rk_2013]. A more detailed analysis of delayed open access, however, is challenging using Unpaywall data only, because Unpaywall has not tracked so far the point of time when articles were made open access.

### Unpaywall Open Access Evidence Types (`evidence`)

The `evidence` field of the `oa_locations` object contains more detailed open access status information. We use again SQL queries stored in separate files ([evidence_08_12.sql](database/evidence_08_12.sql) and [evidence_13_18.sql](database/evidence_13_18.sql)) to be found in the source code repository, and create a data.frame with the relevant fields.

```{r}
library(tidyverse)
# define queries
evidence_08_12_query <- readLines("database/evidence_08_12.sql") %>%
  paste(collapse = "")
evidence_13_18_query <- readLines("database/evidence_13_18.sql") %>%
  paste(collapse = "")
# fetch records and bind them to one data frame
evidence_08_12 <- dbGetQuery(con, evidence_08_12_query)
evidence_13_18 <- dbGetQuery(con, evidence_13_18_query)
evidence_df <- bind_rows(evidence_08_12, evidence_13_18) %>%
  ungroup() %>%
  mutate(year = lubridate::ymd(paste0(year, "-01-01")))
```

The `evidence` field indicates how Unpaywall found the article at a specific location and identified it as OA, for example via PubMed Central or via license information from Crossref.

For each evidence type we want to see how many records were identified as open access in this way. To this end, we created the following table that shows the total number of records per evidence type. The columns also present the total number of associated records, the proportion with respect to the number of all articles and the cumulative proportion of articles associated with all evidence types below.

```{r, layout="l-page", fig.cap = "Number of articles per evidence type."}
# calculate numbers and proportion of articles per evidence type
evidence_df %>%
  group_by(evidence) %>%
  summarize(N_records = sum(number_of_articles)) %>%
  arrange(N_records) %>%
  mutate(
    prop = N_records / sum(N_records) * 100,
    cum_prop = cumsum(N_records) / sum(N_records) * 100
  ) -> articles_per_type_df
articles_per_type_df %>%
  arrange(desc(N_records)) %>%
  knitr::kable(
    col.names = c(
      "Evidence Types",
      "Number of Articles",
      "Proportion of all Articles in %",
      "Cumulative Proportion in %"
    )
  )
```

It can be seen that the long tail of the least frequent 8 categories at the bottom of the table only makes up `r articles_per_type_df$cum_prop[sum(articles_per_type_df$cum_prop <= 5)]` % of all articles, which is why we will aggregate these evidence types in the category `Other` in the following.

So far, we only examined the best open access location per DOI, indicated by `is_best`, a variable defined by Unpaywall algorithm that prioritises publisher-hosted content. However,  evidence types in Unpaywall are not exclusive categories. On the contrary, many records are associated with several evidence types, because various ways to openly access full-texts have been found by Unpaywall. For this reason, the following figure distinguishes whether a given evidence type is classified as best open access location by Unpaywall or not. It is clearly visible that Unpaywall prioritises publisher hosted content (`open`, `oa_journal`) over repository depositions (`oa_repository`), as they state on their website. However, the figure also shows that an existing free pdf version on the publisher's website (likely even without licensing information) is prioritised over journal level classifications as for example being indexed in DOAJ.

```{r, layout = "l-page", fig.cap="Number of articles per evidence type. Least frequent evidence types are collated as category `Other`. In blue, the amount of articles where the corresponding evidence type is classified as `best_oa_location` by Unpaywall is shown."}
# collate least frequent articles as 'Other'
articles_per_type_df %>%
  mutate(evidence = as_factor(evidence)) %>%
  mutate(evidence_grouped = fct_relevel(fct_other(evidence, keep = evidence[.$cum_prop >
                                                                              5]), "Other")) %>%
  group_by(evidence_grouped) %>%
  summarize(number_of_articles = sum(N_records)) %>%
  mutate(
    prop = number_of_articles / sum(number_of_articles) * 100,
    cum_prop = cumsum(number_of_articles) / sum(number_of_articles) *
      100
  ) -> articles_per_type_grouped_df
# group according to categorization with "Other"
evidence_df %>%
  mutate(evidence = as_factor(evidence)) %>%
  mutate(
    evidence_grouped = factor(
      fct_other(evidence, keep = articles_per_type_grouped_df$evidence_grouped),
      levels = articles_per_type_grouped_df$evidence_grouped
    )
  ) %>%
  group_by(evidence_grouped, is_best) %>%
  summarize(number_of_articles = sum(number_of_articles)) %>%
  #create plot
  ggplot(aes(x = evidence_grouped, y = number_of_articles, fill = is_best)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("#b3b3b3a0", "#56B4E9"), name = "Is best?") +
  theme_minimal(base_family = "Roboto") +
  theme(plot.margin = margin(30, 30, 30, 30)) +
  theme(panel.grid.minor = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(panel.grid.major.y = element_blank()) +
  theme(panel.border = element_blank()) +
  coord_flip() +
  scale_y_continuous(
    labels = function(x)
      format(x, scientific = TRUE)
  ) +
  labs(y = "Number of Open Access Articles", x = "Evidence Type",
       title = "Number of Open Access Articles per Unpaywall Evidence Type") -> plot_ev_types_is_best
#create interactive plot
plotly::ggplotly(plot_ev_types_is_best, tooltip = c("y"))
```

To investigate the development of the most prevalent evidence types over time, we use a faceted graph. We again observe declines in the proportion repository-based evidences which are chosen as best location.

```{r, layout = "l-page", fig.cap="Development of the number of articles per evidence type over time. Least frequent evidence types are collated as category `Other`. For each type the total number of articles per year is shown for publication years from 2008 to 2018. In blue, the amount of articles where the corresponding evidence type is classified as `best_oa_location` by Unpaywall is highlighted.", fig.height=6}
#collate least frequent categories as 'Other'
evidence_df %>%
  mutate(evidence = as_factor(evidence)) %>%
  mutate(
    evidence_grouped = factor(
      fct_other(evidence, keep = articles_per_type_grouped_df$evidence_grouped),
      levels = articles_per_type_grouped_df$evidence_grouped
    )
  ) %>%
  #create plot
  ggplot(aes(year, number_of_articles, fill = is_best)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ fct_rev(evidence_grouped), ncol = 2) +
  scale_fill_manual(values = c("#b3b3b3a0", "#56B4E9"), name = "Is best?") +
  theme_minimal(base_family = "Roboto") +
  theme(panel.grid.minor = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(panel.grid.major.x = element_blank()) +
  theme(panel.border = element_blank()) +
  scale_y_continuous(
    labels = function(x)
      format(x, scientific = TRUE)
  ) +
  scale_x_date(date_labels = "%y") +
  labs(x = "Publication Year", y = "Number of Open Access Articles",
       title = "Unpaywall Open Access Evidence Categories per Year") -> plot_ev_types_per_year
#create interactive plot
plotly::ggplotly(plot_ev_types_per_year, tooltip = c("y"))
```

## Overlap of Open Access Provision and Evidence Types

Many open access articles are accessible through a number of locations, including the publisher's webpage and also one or more open access repositories. Unpaywall not only describes one, but all open access full-texts it discovers with useful metadata. In the following, we will analyse if and to which extent the various open access indicators intersect. We start with an analysis of the overlap between host types, followed by determining set intersections between Unpaywall's evidence types.

### Overlap between Host Types

To present articles that are both provided by publishers and repositories, we use the BigQuery SQL function [STRING_AGG](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#string_agg) to create a new variable where we concatenate the different host_types per open access article (for more details, see the full SQL queries [`host_type_intersect_08_12.sql`](database/host_type_intersect_08_12.sql) and [`host_type_intersect_13_18.sql`](database/host_type_intersect_13_18.sql)].

```{r}
host_type_intersect_08_12_query <- readLines("database/host_type_intersect_08_12.sql") %>%
  paste(collapse = "")
host_type_intersect_13_18_query <- readLines("database/host_type_intersect_13_18.sql") %>%
  paste(collapse = "")
```

Again, we call BigQuery and load the aggregated data into our local R session. This time, we do not want to present the total number of open access publications, but its relative share. We already obtained the total number of articles, which are stored in the `my_df` data.frame.

```{r}
host_type_08_12_intersect_df <-
  dbGetQuery(con, host_type_intersect_08_12_query)
host_type_13_18_intersect_df <-
  dbGetQuery(con, host_type_intersect_13_18_query)
host_type_intersect <-
  bind_rows(host_type_08_12_intersect_df, host_type_13_18_intersect_df) %>%
  mutate(year = lubridate::ymd(paste0(year, "-01-01"))) %>%
  mutate(
    host = case_when(
      host_type_count == "publisher" ~ "Publisher only",
      host_type_count == "publisher,repository" ~ "Publisher & Repository",
      host_type_count == "repository" ~ "Repositories only"
    )
  ) %>%
  mutate(host = factor(
    host,
    levels = c("Publisher only", "Publisher & Repository", "Repositories only")
  ))
# obtain yearly publication volumes
host_type_intersect <- my_df %>%
  group_by(year) %>%
  summarise(all_articles = sum(n)) %>%
  # join with host type figures
  right_join(host_type_intersect, by = "year") %>%
  # calculate proportion
  mutate(prop = number_of_articles / all_articles)
host_type_intersect
```

Let's visualise the host type distribution including the overlap between publisher and repository-provided open access:

```{r, layout="l-page", fig.cap='Open access to journal articles by open access hosting location. Coloured bars represent the number of open access articles per Unpaywall host category: "publisher" and "repository", grey bars the percentage of open access to journal articles indexed in Crossref from Unpaywall. Because open access provision is not mutually exclusive, the overlap between "publisher" and "repository" hosted open access full-texts is also shown.'}
# get overall oa share
host_type_all <- host_type_intersect %>%
  group_by(year) %>%
  summarise(prop = sum(prop))
# make a ggplot graphic
plot_host_intersect <-
  ggplot(host_type_intersect, aes(x = year, y = prop)) +
  geom_bar(
    data = host_type_all,
    aes(fill = "All OA Articles"),
    color = "transparent",
    stat = "identity"
  ) +
  geom_bar(aes(fill = "by Host"), color = "transparent", stat = "identity") +
  facet_wrap(~ host, nrow = 1) +
  scale_fill_manual(values = c("#b3b3b3a0", "#56B4E9"), name = "") +
  labs(x = "Year", y = "OA Share",
       title = "Overlap between Open Access Host Types in Unpaywall") +
  scale_x_date(date_labels = "%y") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 5L)) +
  theme_minimal(base_family = "Roboto") +
  theme(plot.margin = margin(30, 30, 30, 30)) +
  theme(panel.grid.minor = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(panel.grid.major.x = element_blank()) +
  theme(panel.border = element_blank())
# turn ggplot object into interactive plotly chart
plotly::ggplotly(plot_host_intersect, tooltip = c("y")) 
```

The figure shows that Unpaywall found most open access full-text on publisher websites, of which a large proportion was not archived in a repository. The overlap of open access provided by both routes deserves attention: around `r filter(host_type_intersect, host != "Publisher only") %>% .$number_of_articles %>% sum() / host_type_intersect %>% .$number_of_articles %>% sum() * 100` % of all open access articles was provided in open access using both publisher web sites and repositories.

### Overlaps between Evidence types

The categorization of evidence types is not exclusive, either. Therefore, many records will be associated with more than one evidence type.

To better understand this overlap, we generate a new column specifying all found combinations of evidence types using concatenation with the SQL function [STRING_AGG](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#string_agg). The SQL queries we use ([evidence_single_cat_08_12.sql](database/evidence_single_cat_08_12.sql) and [evidence_single_cat_08_12.sql](database/evidence_single_cat_08_12.sql)) can be found in the source code repository.

```{r}
library(tidyverse)
# define queries
evidence_single_cat_08_12_query <- readLines("database/evidence_single_cat_08_12.sql") %>%
  paste(collapse = "")
evidence_single_cat_13_18_query <- readLines("database/evidence_single_cat_13_18.sql") %>%
  paste(collapse = "")
# fetch records and bind them to one data frame
evidence_categories_08_12 <- dbGetQuery(con, evidence_single_cat_08_12_query)
evidence_categories_13_18 <- dbGetQuery(con, evidence_single_cat_13_18_query)
evidence_categories_df <- bind_rows(evidence_categories_08_12, evidence_categories_13_18)
evidence_categories_df %>%
  group_by(ev_cat) %>%
  summarize(number_of_articles = sum(number_of_articles)) %>%
  arrange(desc(number_of_articles))
```

We first illustrate for each evidence type - collating again the least frequent types in the category `Other` - the amount of articles which corresponds exclusively to this type and no others.

```{r, layout = "l-page", fig.cap="Proportion of articles per evidence type. In blue, the amount of articles uniquely associated with the corresponding evidence type is shown."}
#determine number of articles corresponding only to one evidence type
evidence_single_cat_df <- evidence_df %>%
  group_by(evidence) %>%
  summarize(number_of_articles = sum(number_of_articles)) %>%
  left_join(evidence_categories_df, by = c("evidence" = "ev_cat")) %>%
  rename(number_of_articles = number_of_articles.x, number_of_single_cat = number_of_articles.y) %>%
  mutate(number_of_articles = replace_na(number_of_articles, 0),
         number_of_single_cat = replace_na(number_of_single_cat, 0))
#aggregate least frequent types as category `Other`
evidence_single_cat_grouped_df <- evidence_single_cat_df %>%
  ungroup() %>%
  mutate(evidence = as_factor(evidence)) %>%
  mutate(
    evidence_grouped = factor(
      fct_other(evidence, keep = articles_per_type_grouped_df$evidence_grouped),
      levels = articles_per_type_grouped_df$evidence_grouped
    )
  ) %>%
  group_by(evidence_grouped) %>%
  summarize(
    number_of_articles = sum(number_of_articles),
    number_of_single_cat = sum(number_of_single_cat)
  ) %>%
  #arrange data in order to enable stacked barplots
  mutate(multiple = number_of_articles-number_of_single_cat, single = number_of_single_cat) %>%
  select(evidence_grouped, single, multiple) %>%
  gather(is_single, number_of_articles, -evidence_grouped)
#create aggregated proportions barplot
evidence_single_cat_grouped_df %>%
  ggplot(aes(x = evidence_grouped, y = number_of_articles, fill = is_single)) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = c("#b3b3b3a0", "#56B4E9"), name = "Is unique?") +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_family = "Roboto") +
  theme(plot.margin = margin(30, 30, 30, 30)) +
  theme(panel.grid.minor = element_blank()) +
  theme(axis.ticks = element_blank()) +
  theme(panel.grid.major.y = element_blank()) +
  theme(panel.border = element_blank()) +
  coord_flip() +
  theme(legend.position = "top",
        legend.justification = "right") +
  labs(y = "Proportion of Articles", x = "Evidence Type",
       title = "Proportion of Articles per Evidence Type") -> plot_ev_types_is_single_prop
#create interactive plot
plotly::ggplotly(plot_ev_types_is_single_prop, tooltip = c("y"))
```

It is interesting to see, that the evidence type which appears most often as a unique form of open access provision is via an openly available pdf on the publisher's page, meaning that no other evidence, like license information from Crossref, was found. This confirms that Unpaywall's open access detection has benefited from scraping publishers' websites.

Moreover, a critical amount of articles is found only through repository-based evidence sources and hence, is available only via the green route. Still, the figure shows a phenomenon that we observed also for the `host_type`, namely that repository-based evidence types often overlap with other evidence types.

Because of Unpaywall's prioritisation of publisher-provided open access, caution is in order, when only the `best_oa_location` is used for categorisation: there are publisher-based evidence types that may not comply with how funders define open access journals, particularly with regard to license statements. On the other hand, a large number of articles seems to be identified as open access only through license information from Crossref without an associated free pdf having been found. However, we are unsure whether Unpaywall performs all open access identification procedures for every single record indexed in Crossref, which would allow for  such a comparison. 

Visualising the occurring intersections of multiple evidence types is difficult. Following [@Lex_2014] and [@Lex_2014b], we created an [UpSet](http://vcg.github.io/upset/) figure using the [UpSetR package](https://cran.r-project.org/web/packages/UpSetR/) described in [@Conway_2017] in order to examine in more detail, which of the `r nrow(evidence_categories_df %>% group_by(ev_cat) %>% summarize(n = n()))` occurring combinations of evidence types - including singletons - shows up most frequently and how large these groups are. In theory, for the `r length(unique(evidence_df$evidence))` evidence types up to `r 2^length(unique(evidence_df$evidence))` combinations would be possible.

<aside>
A very good introduction into the usage of UpSetR gives the [Basic Usage Vignette](https://cran.r-project.org/package=UpSetR/vignettes/basic.usage.html).
</aside>

To start with, we interface BigQuery and retrieve how often the combinations of evidence types occur. We store the corresponding SQL queries ([evidence_overlap_08_12.sql](database/evidence_overlap_08_12.sql) and [evidence_overlap_13_18.sql](database/evidence_overlap_13_18.sql)) and share the resulting dataset as .csv files ([results_evidence_overlap_08_12.csv](data/results_evidence_overlap_08_12.csv) and [results_evidence_overlap_13_18.csv](data/results_evidence_overlap_13_18.csv)). Next, we transform the data to an upsetr-compatible expression format, resulting in a named character vector. Lastly, the `upset()` function generates the graph. To keep the resulting figure readable, we only display the 15 combinations of the 7 most frequent types with the highest number of articles each.

<!-- The total numbers for the displayed overlapping categories include also the amount of articles associated with intersections with sets, which are exlcuded here because of their low frequency. For example, the set `open (via free pdf)` includes articles from the sets `open (via free pdf)` and `oa repository (via OAI-PMH title match)` because the latter is not considered seperately. -->


```{r, layout = "l-page", fig.cap='Most frequent combinations of evidence types. The barplot on the left displays the total number of articles per evidence type ("Set Size"). The central barplot shows the number of articles per overlap category ("Intersection Size"). Which evidence types contribute to each intersection is given by the black dots in the chart below.'}
library(UpSetR)
# fetch data for upset graph
evidence_categories_upset_08_12_query <- readLines("database/evidence_overlap_08_12.sql") %>%
  paste(collapse = "")
evidence_categories_upset_13_18_query <- readLines("database/evidence_overlap_08_12.sql") %>%
  paste(collapse = "")
evidence_categories_upset_08_12 <- dbGetQuery(con, evidence_categories_upset_08_12_query)
evidence_categories_upset_13_18 <- dbGetQuery(con, evidence_categories_upset_13_18_query)
evidence_categories_upset_df <- bind_rows(evidence_categories_upset_08_12, evidence_categories_upset_13_18) %>%
  group_by(ev_cat) %>% 
  summarise(n = sum(number_of_articles))
# list with counts
evidence_categories_upset_list <- as.list(evidence_categories_upset_df$n)
# categories as list names
names(evidence_categories_upset_list) <- evidence_categories_upset_df$ev_cat
# convert to vector
evidence_categories_upset_expr <- unlist(evidence_categories_upset_list)
upset(fromExpression(evidence_categories_upset_expr), nsets = 7, nintersects = 15, order.by = "freq", show.numbers = FALSE)
```


## Discussion and Conclusion

In this blog post, we demonstrated how to analyse the Unpaywall data dump with Google BigQuery and R. Interfacing BigQuery with R has allowed us to integrate a high-performance and user-friendly database environment into our R data analytics workflow. Using this data management environment, we found 11,6 million journal articles published between 2008 and 2018 with open access full-texts, representing around one third of all articles indexed in Crossref for this period. Moreover, we found Unpaywall to be a suitable data source for open access analytics, because Unpaywall does not only tag if a publication is freely available, but also provides metadata describing how and where the open access full-text links were discovered. 

Our Unpaywall data analysis revealed various open access location and evidence types, as well as large overlaps between them. Along with the likely influence of embargoed or delayed open access provision on some of these types, our analysis raises important questions about how to responsibly use Unpaywall data in bibliometric research and open access monitoring. Examining Unpaywall's best open access location only, favours publisher-provided open access, which, in turn, means that open access provided by repositories would be underestimated. Likewise, large overlaps between evidence categories can be observed. To allow for careful consideration, bibliometric research and open access monitoring must therefore be clear about how open access indicators were derived from Unpaywall.

In future, we will use these insights from our data analysis to work on a matching procedure between Unpaywall and the Web of Science in-house database from the [German Competence Center for Bibliometrics](http://www.bibliometrie.info/) in our [OAUNI project](https://www.wihoforschung.de/de/oauni-2182.php). In doing so, we want to represent Unpaywall's open access evidence as comprehensive as possible to allow for a pluralist view on open access to journal articles from researchers affiliated with German Universities.

## Acknowledgments {.appendix}

We acknowledge financial support from the the Federal Ministry of Education and Research of Germany (BMBF) in the framework [Quantitative research on the science sector](https://www.wihoforschung.de/en/quantitative-research-on-the-science-sector-1573.php) (Project: "OAUNI Entwicklung und Einflussfaktoren des Open-Access-Publizierens an Universitäten in Deutschland", Förderkennzeichen: 01PU17023A).

